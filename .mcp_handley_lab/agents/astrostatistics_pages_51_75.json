{
  "name": "astrostatistics_pages_51_75",
  "system_prompt": null,
  "created_at": "2025-11-05T14:22:50.943497",
  "messages": [
    {
      "role": "user",
      "content": "Please transcribe pages 51-75 of this astrostatistics lecture notes PDF into markdown format. \n\nFollow this exact style and formatting:\n- Use page markers like `---\\n(Page N)` to indicate page numbers\n- Preserve ALL mathematical notation using LaTeX syntax (inline: $...$ and display: $$...$$)\n- Include detailed descriptions of any diagrams/images in [brackets]\n- Use markdown headers (##, ###) for lecture titles and sections\n- Preserve lecture headers with dates in format: ## Lecture N\\n_DD.M.YY_\n- Use blockquotes (>) for annotations, notes, and side comments\n- Use bullet points (*) and numbered lists as appropriate\n- Preserve all arrows (→, ←, ↑, ↓) and special symbols\n- Keep abbreviations and shorthand exactly as written\n- Maintain indentation for nested content\n- Use horizontal rules (---) between pages\n\nBe extremely thorough and accurate. Transcribe ALL text, equations, diagrams, and annotations exactly as they appear in the handwritten notes.",
      "timestamp": "2025-11-05T14:25:12.474450",
      "tokens": 43044,
      "cost": 0.0853275
    },
    {
      "role": "assistant",
      "content": "---\n(Page 51)\n## Bayes Advantages (slides)\n* Ability to include prior information P(θ)\n    - Incorporate info. from external datasets. P(θ) is the posterior from some other data P(θ|D_ext). E.g. cosmology. diff datasets give diff ways to probe cosmological parameters -> encode external posterior from one analysis as prior in another. E.g. CMB analysis -> constraints -> to incorporate information since one parameter often has multiple observables in cosmology.\n* Regularisation: penalises overfitting with complex model. e.g. gaussian process prior\n    > punishment term for complex model\n    - e.g. linear regression, ridge regression is linear regression with a penalty term in likelihood. Can be interpreted with priors. e.g. can rule out. Think large slopes will occur and can incorporate that into prior. Can also penalise complex models -> Occam's razor.\n    - It's not flat. > e.g. weights using L_1 norm, L_2 norm etc.\n* \"Noninformative\"/weakly informative/default priors when you don't have much prior information.\n    - e.g. might only be able to guess param is between 0 and 1 but still information! -> cannot have prior without some information, but reminder: definition of likelihood is the probability of data given param. P(D|θ)\n* Likelihood (you're plugged in observed dataset and are viewing that as a fn. of the parameters) is not a probability density in the parameters. But multiply by a prior (even flat), and the (normalised) posterior is a probability density, conditional/marginal probabilities can be computed.\n    (e.g., in multi-parameter cases, useful to derive conditional densities 'example later').\n* Ability to deal with high dimensional parameter space e.g. latent variables or nuisance parameters and marginalise them out (analytically/numerically) e.g. posterior mode or MLE is called maximum a posteriori -> can find peak.\n* Note: Bayesian inference not necessarily completely opposed to frequentist statistics. Estimators derived from Bayesian arguments can still be evaluated in a frequentist basis (e.g. James-Stein estimators)\n    > (a technique to generate an estimator)\n    > (could call this as intervals that contain some positive prob. of containing θ).\n\n---\n(Page 52)\n> (Last time: gaussian example of bayesian inference (1D, flat prior -> like \"hello world\" of bayesian inference)\n> recap: now slightly more involved example.\n\n### Simple Gaussian Example w/ conjugate prior (board)\n$Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)$ ($ \\mu $ unknown, $ \\sigma^2 $ known).\n\nLikelihood: $ P(\\vec{y}|\\mu, \\sigma^2) = \\prod_{i=1}^{N} N(y_i|\\mu, \\sigma^2) = \\dots $\n$= (2\\pi \\sigma^2)^{-N/2} e^{-\\frac{(N-1)S^2}{2\\sigma^2}} e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} $\n\nDefined sufficient statistics:\n> all info in data is contained in the likelihood & likelihood depends on sufficient statistics. meaning if two diff. y vectors with same N & $\\bar{y}$, S yield same info about parameters. even if y vector is different.\n\n$\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i$ \"sample mean\"\n\n$S^2 = \\frac{1}{N-1} \\sum_{i=1}^N (y_i - \\bar{y})^2$ \"sample variance\"\n\nConjugate Prior: $ P(\\mu) = N(\\mu|\\mu_0, \\tau_0^2) $\n(equivalently can write $ \\mu \\sim N(\\mu_0, \\tau_0^2) $) $ \\leftarrow \\text{prior mean} \\leftarrow \\text{prior variance} $\n$ P(\\mu) = \\frac{1}{\\tau_0 \\sqrt{2\\pi}} e^{-\\frac{1}{2\\tau_0^2}(\\mu - \\mu_0)^2} $\n\nPosterior: $ P(\\mu|\\vec{y}) \\propto P(\\vec{y}|\\mu) P(\\mu) $\n> Only stuff w. $\\mu$ in will matter.\n$ \\propto e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} e^{-\\frac{1}{2\\tau_0^2}(\\mu-\\mu_0)^2} $\n> Prior: relates to our degree of belief in parameters (a conceptual point)\n> will be normally const, so don't need to do integration.\n> let's get robust result. will actually be a gauss.\n\n[warm up ex.] - product of gaussian densities\n$ P(\\mu|\\vec{y}) = N(\\mu|\\mu_N, \\tau_N^2) $\n$ \\qquad \\uparrow \\qquad\\qquad \\uparrow $\nposterior mean posterior variance\n\n---\n(Page 53)\n> (from up sheet 1)\nreminder: when you have product of gaussian densities, the precision = $\\frac{1}{\\text{variance}}$ of the resulting gaussian = sum of precision of individual gaussians.\n\n$$ \\left[ \\frac{1}{\\tau_N^2} = \\frac{1}{\\tau_0^2} + \\frac{N}{\\sigma^2} \\right] \\qquad \\text{POSTERIOR PRECISION} $$\n> e.g. have $N(\\mu_1, \\sigma_1^2), N(\\mu_2, \\sigma_2^2)$\n> add them: $N(\\mu, \\sigma^2)$\n> multiply them: $N(\\mu_{12}, \\sigma_{12}^2), \\sigma_{12}^{-2} = \\sigma_1^{-2} + \\sigma_2^{-2}$\n\nPosterior precision = sum of precisions from likelihood & prior.\n\nreminder:\n$$ \\left[ \\mu_N = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{N}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2} + \\frac{N}{\\sigma^2}} \\right] \\qquad \\text{POSTERIOR MEAN} $$\n> (divide by weights to get mean)\n\nPosterior mean = precision weighted average of means from likelihood & prior.\n> These formulas explicitly tell us how prior information & likelihood are combined to give us the bayesian inference.\n> (mean & variance)\n\nFor fixed prior $\\tau_0$, as $N \\rightarrow \\text{large}$\n> send $N \\rightarrow \\infty$, second term in $\\tau_N^2$ goes to $\\infty$. $\\tau_N^2 \\rightarrow 0$?\n$\\mu_N = \\bar{y}$\n$\\tau_N^2 \\rightarrow \\sigma^2/N$\n> ($P(\\mu|\\vec{y}) = N(\\mu|\\bar{y}, \\frac{\\sigma^2}{N} + \\frac{\\sigma^2}{\\tau_0^2 \\dots})$\n(Data $N \\rightarrow$ large, likelihood dominates over prior.)\n> basically after a lot of data, we get the same result as flat prior.\n\nSide note: What is conjugate prior?\nfor a given likelihood, if you combine it with a conjugate prior coming from \"nice\" distribution then the posterior is guaranteed to come from the same class of \"nice\" distributions. (e.g. gaussian, wishart, laplace)\n\n---\n(Page 54)\n### Astrophysics Example (slide)\n#### BAYESIAN INFERENCE FOR PARALLAX\nPAPER: C. Bailer Jones\nEstimating distances from parallaxes\n- parallax is a way to measure distances\n$ \\varpi = \\frac{\\text{parsec}}{r} \\qquad $ (apparent position)\n$ \\text{arcsec} $\n$ \\varpi_{\\text{true}} = 1/r \\qquad $ (in units of parsec)\n[Diagram showing the Earth at two points in its orbit around the Sun, observing a distant star against a background of even more distant stars. The angle subtended by the Earth's orbit as seen from the star is the parallax angle.]\n> earth\n> star\n> path of late-time\n\n- Gaia satellite makes parallax measurement to measure distances & map stars in our galaxy.\n- measurement error in parallax angle. measured parallax is different to true parallax.\n$ P(\\tilde\\omega|r) = \\frac{1}{\\sigma_{\\tilde\\omega}\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2\\sigma_{\\tilde\\omega}^2}(\\tilde\\omega - \\frac{1}{r})^2\\right] \\qquad \\sigma_{\\tilde\\omega} > 0 $\n- measured parallax has some distribution: assume gaussian distribution for simplicity\n- fractional measurement error $f = \\sigma_{\\tilde\\omega}/\\tilde\\omega$\n> just another way to express error, no more info than $\\sigma$\n$ r=10 \\text{pc}: $\n[Plot of a probability distribution P(ω|r). The x-axis is ω, centered at 0.1. There are two Gaussian curves shown. The wider, shorter one is labeled \"ω_observed = 0.2, σ_ω = 0.05, f=0.5, less precise instrument\". The narrower, taller one is labeled \"σ_ω = 0.02, f=0.2, more precise instrument\".]\n> true value of actual parallax could be anywhere in distribution.\n- possible to get negative parallax due to measurement error.\n\nLikelihood\n$ P(\\tilde\\omega|r) $\n[Plot of likelihood P(ῶ|r) vs r. The x-axis is r (true distance). The y-axis is Likelihood. Three curves are shown, corresponding to different observed parallaxes ῶ. A dashed blue curve peaks at a low r and is labeled \"upper likelihood\". A solid blue curve peaks at a higher r and is labeled \"true likelihood\". A solid purple curve is shown to the right of the blue ones, labeled \"lower likelihood\".]\n> get skewed distribution of likelihoods.\n> true value\n\n---\n(Page 55)\n> what if ῶ < 0? large error? then peak is -ve\n- Laplace approximation gives gaussian, incidentally this is mathematically equivalent to doing propagation of error (assume gaussian measurement error propagated to $r=1/\\tilde\\omega$...)\n- this is an approximation. (dotted vs solid line).\n> no peak when ῶ < 0\n[Plot of likelihood P(ῶ|r) vs r. x-axis from -30 to 30. y-axis is likelihood. Several curves are shown for different values of ῶ. For ῶ=0.1, the curve is a peak in the positive r region. For ῶ=0.05, the peak is further out. For ῶ=0.01, the peak is even further out. For ῶ=-0.01, there is no peak in the positive r region, the curve rises as r approaches 0 from the positive side. A dashed curve shows the symmetric negative part for ῶ=0.05. A label \"Futility function\" is near the peak of ῶ=0.05. A note says \"Likelihood for seeing return a positive parallax given that no prior is used. Just reality.\"]\n- likelihood is positive on negative values of distance (unphysical) -> see all values of mapping $r \\rightarrow 1/r$\n- negative measurements have no mode (MLE).\n> We peak - most likely peak for the but not for -ve distance.\n\nthey try to fix this:\n- impose our knowledge that we know distances should be positive\n$ P(r|\\tilde\\omega) \\propto P(\\tilde\\omega|r) P(r) $\n$ P(r) = \\begin{cases} 1 & r>0 \\\\ 0 & \\text{otherwise} \\end{cases} $\n> note: improper distribution, not normalisable\n- this cuts off -ve values of r in our graph\n[Two plots are shown. Left plot: P(r|ῶ) vs r. Curves for ῶ=0.01, δ=0.2, δ=0.5 are shown, all peaking at positive r and going to zero at r=0. The y-axis is labeled P(r|ῶ) (unnormalized posterior). Right plot: P(r|ῶ) vs r. Two curves for ῶ=-0.01 and σ=0.02 are shown. One is the likelihood which is non-zero for r<0. The other is the posterior P(r|ῶ) which is a cumulative-like curve for r>0. Note says \"for -ve parallaxes, not getting a peak at all -> no mode\".]\n- improper posterior: not-normalisable no mode, variance etc.\n- mode ($r=1/\\tilde\\omega$) exists for +ve $\\tilde\\omega$ but undefined for -ve $\\tilde\\omega$. ($r>0$)\n> still have some problems\n\n---\n(Page 56)\n- impose a proper distance prior (impose limit to max distance of a star in our survey)\n$ P(r) = \\begin{cases} 1/r_{lim} & 0 < r \\le r_{lim} \\\\ 0 & \\text{otherwise} \\end{cases} $\n\n$ P(r|\\tilde\\omega) \\propto P(\\tilde\\omega|r)P(r) $\n$ P^*_{\\mu}(r|\\tilde\\omega, \\sigma_{\\tilde\\omega}) = \\begin{cases} (1/r_{lim}) P(\\tilde\\omega|r, \\sigma_{\\tilde\\omega}) & 0 < r \\le r_{lim} \\\\ 0 & \\text{otherwise} \\end{cases} $ > unnormalized posterior\n\n[Plot of P(r|ῶ) vs r. The x-axis goes up to r_lim=100. Three curves are shown, for ῶ=0.01, ῶ=0.025, and ῶ=0.05. All curves are cut off at r=100. A dashed line shows the continuation of the ῶ=0.01 curve.]\n> note: cut off at r_lim\n- implies unrealistic stellar density ~ $1/r^2$\n- cut off of mode at edge $r_{lim}$ for small or negative measured parallax ($\\tilde\\omega < 0$) (but better than going to infinity)\n$ \\text{rest} = \\begin{cases} 1/\\tilde\\omega & 0 < 1/\\tilde\\omega \\le r_{lim} \\\\ r_{lim} & 1/\\tilde\\omega > r_{lim} \\\\ \\text{undefined} & \\tilde\\omega \\le 0 \\end{cases} \\quad \\text{(extreme mode)} $\n\n- still have some problems -> can go towards more & more astrophysically motivated priors.\n- problem w/ previous prior: volume density $p(r)$.\n$ P_r(\\text{distance} \\in [r, r+dr]) \\propto p(r) 4\\pi r^2 dr $\n> unphysical ->\n$ p(r) \\propto 1/r^2, r < r_{lim} \\Rightarrow p(r) \\propto \\text{const}. \\quad r < r_{lim} $\n$ \\qquad 0 \\text{ otherwise} \\qquad\\qquad\\qquad 0 \\text{ otherwise} $\n- more physical: copernican principle $p(r) \\propto \\text{const}. \\Rightarrow p(r) \\propto \\text{const} \\cdot 4\\pi r^2 $\n> sounds reasonable.\n> see slides CBJ, some principle as previous priors\n\n---\n(Page 57)\n## Lecture 13\n_21.2.25_\n> see slides for reading refs\n- astrophysics example\nrecap: understanding how priors impact inference\n- keeps going as before: introduce more & more astrophysically motivated priors. (see slides)\n\n### Conclude: Priors in Bayesian Inference\n* Priors can be used to encode background info./external knowledge about parameters.\n    - weak mathematical constraints on physical parameters e.g. positivity of distances\n    - astrophysical info. e.g. distributions of stars\n* Test sensitivity of your inference to the priors (& likelihood). Under various assumptions of the model.\n> Note: also depends on your measurement error & precision. not just prior.\n$ \\underset{\\text{likelihood}}{P(\\tilde\\omega|r)} - \\underset{\\text{measured}}{P(r|\\tilde\\omega)} \\propto P(\\tilde\\omega|r) \\underset{\\text{prior}}{P(r)} - \\text{posterior} $\n\n---\n(Page 58)\n### MULTIPARAMETER BAYESIAN MODELS (slides)\nExample: Gelman BDA 3.2 sheet 2\n> [Analytic posterior for gaussian model with non-informative prior]\ndata generating process: $ y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2) \\quad i=1,...,n $\nlikelihood: $ p(\\vec{y}|\\mu, \\sigma^2) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\\right) $\n\nsufficient statistics\n(non informative) $ p(\\mu) \\propto 1 $\n> independent of ordering of meas.\nimproper prior: $ p(\\log \\sigma^2) \\propto 1 $ or $ p(\\sigma^2) \\propto \\sigma^{-2} \\quad (\\sigma^2 > 0) $ JEFFREYS' PRIOR\n> even if prior improper, need proper posterior (corresponds to real observations)\njoint posterior: $ p(\\mu, \\sigma^2 | \\vec{y}) \\propto p(\\vec{y}|\\mu, \\sigma^2) \\times p(\\mu, \\sigma^2) $\n$ p(\\mu, \\sigma^2|\\vec{y}) \\propto (\\sigma^2)^{-n/2-1} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\\right) $\n> equivalent, can say: $ P(\\sigma^2|\\vec{y}) \\frac{P(\\mu|\\sigma^2, \\vec{y})}{P(\\sigma^2)} \\dots $\n> joint is useful to find conditional/marginals:\nconditional posterior: $ p(\\mu|\\sigma^2, \\vec{y}) = N(\\mu|\\bar{y}, \\sigma^2/n) $\n> ex. sheet 2\nmarginal posteriors: $ p(\\sigma^2|\\vec{y}) = \\int p(\\mu, \\sigma^2|\\vec{y}) d\\mu = \\text{Inv-}\\chi^2(\\sigma^2|n-1, s^2) $\n$ p(\\mu|\\vec{y}) = \\int p(\\mu, \\sigma^2|\\vec{y}) d\\sigma^2 $\n> can derive these analytically so useful\n$ \\propto \\left[ 1 + \\frac{n(\\mu-\\bar{y})^2}{(n-1)s^2} \\right]^{-n/2} $\n$ = t_{n-1}(\\mu|\\bar{y}, s^2/n) \\qquad t \\text{ distribution} $\n\nNote: (1) inverse chi-squared distribution\n$ \\text{Inv-}\\chi^2(\\theta|\\nu, s^2) \\propto \\theta^{-(\\nu/2+1)} \\exp\\left(-\\frac{\\nu s^2}{2\\theta}\\right), \\quad \\theta>0 $\ndeg. of freedom $ \\nu > 0 $\nscale parameter $ s > 0 $\n[A sketch of a right-skewed distribution, peaking near zero and having a long tail to the right.]\n\n---\n(Page 59)\n(2) Same as inverse-gamma\n$ \\text{Inv-gamma}(\\theta|\\alpha, \\beta) \\propto \\theta^{-(\\alpha+1)} \\exp(-\\beta/\\theta), \\quad \\theta > 0 $\nshape parameter $ \\alpha = \\nu_0/2 $\nscale parameter $ \\beta = \\nu_0\\sigma_0^2/2 $\n> useful named distributions have random number generators in common python packages\n\n(3) Student's t distribution\n$ t_{\\nu}(\\theta|\\mu, \\sigma^2) \\propto \\left[1 + \\frac{1}{\\nu} \\left(\\frac{\\theta-\\mu}{\\sigma}\\right)^2\\right]^{-(\\nu+1)/2} $\ndeg. of freedom $ \\nu > 0 $\nlocation parameter $ \\mu $\nscale parameter $ \\sigma > 0 $\n> see these distributions in book i.e. Gelman BDA appendix or just wikipedia for your example sheet\n[A sketch of a bell-shaped curve, similar to a Gaussian but with potentially heavier tails.]\n\nWhat if you can't compute marginals/expectations analytically? -> Bayesian Computation\n* Bayesian answer: full posterior $P(\\theta|D)$, numerical estimates are attempts to (imperfectly) summarise the posterior e.g. mean, mode.\n* Often these are posterior expectations e.g. $E[f(\\theta)|D] = \\int f(\\theta) P(\\theta|D) d\\theta$ (computationally difficult)\n* Bayesian computation: algorithms to \"map out\"/sample the posterior $P(\\theta|D)$ and compute expectations $E[f(\\theta)|D]$.\n* e.g. MCMC, nested sampling, importance sampling.\n> \"all models are wrong, some are useful. Even if your computation is 'right'!\"\n\n---\n(Page 60)\n> object of computation is to calculate some integral (posterior expectations) -> can do with monte carlo integration\n### MONTE CARLO INTEGRATION (slides)\nTypically we want to summarize the posterior & compute expectations of the form\n$ I = E[f(\\theta)|D] = \\int f(\\theta) P(\\theta|D) d\\theta $\n\nUsing m samples from the posterior\n$ \\theta_i \\sim P(\\theta|D) $\n$ \\hat{I} = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) \\rightarrow I $ (LLN for large N) CLT\n\nMonte Carlo Error (derive later on next page)\n$ \\text{Var}[\\hat{I}] = \\frac{1}{m^2} \\sum_{i=1}^m \\text{Var}[f(\\theta_i)] = \\frac{1}{m} \\text{Var}[f(\\theta)] \\propto \\frac{1}{\\sqrt{m}} \\text{Var}[E_f(\\theta_i)] $\n'Monte carlo error ~ $1/\\sqrt{m}$ indep of dim[$\\theta$] - is convenient'\n\nFundamental Theorem of Monte Carlo -> Bayesian computation \"using sampling\"\n$ \\underset{\\text{posterior expectation}}{E[f(\\theta)|D]} = \\int f(\\theta) P(\\theta|D) d\\theta \\approx \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) $\n$ \\qquad \\uparrow \\qquad\\qquad \\text{sample average} $\n\nE.g.\nPosterior mean $\\mu \\qquad f(\\theta) = \\theta $\n> reporting this as if it is Maximum?\nPosterior variance $ \\qquad f(\\theta) = (\\theta-\\mu)^2 $\nProbability in an interval [a,b] $ \\qquad f(\\theta) = I_{[a,b]}(\\theta) $\n(indicator function)\n= one when value between a & b.\n\n---\n(Page 61)\n### Monte Carlo Error (board)\nsuppose: $\\theta$ is some invertible fn. of $\\theta$\n$ \\theta_i \\stackrel{iid}{\\sim} P(\\theta) \\qquad i=1,...,m $\n> monte carlo sample size $m$ is not same as data sample size $n$.\n$ \\rightarrow f(\\theta_i) $ are iid\nDef: $ \\hat{I} = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) $\n$ \\text{Var}(\\hat{I}) = \\text{Cov}(\\hat{I}, \\hat{I}) = \\text{Cov}\\left[\\frac{1}{m}\\sum_i f(\\theta_i), \\frac{1}{m}\\sum_j f(\\theta_j)\\right] $\n$ = \\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m \\text{Cov}[f(\\theta_i), f(\\theta_j)] $\n> $\\theta_i, \\theta_j$ are iid -> only non zero contributions when $i=j$\n$ = \\frac{1}{m^2} \\sum_{i=1}^m \\text{Var}(f(\\theta_i)) $\n$ = \\frac{1}{m} \\text{Var}(f(\\theta_i)) $\n> standard deviation of monte carlo error\n> std MC error $\\propto \\frac{1}{\\sqrt{m}}$\nGLT: $ m \\rightarrow \\text{large} \\quad \\hat{I} \\sim N(E[f(\\theta)], \\text{Var}(f(\\theta))/m) $\n\napproximate variance\n$ \\text{Var}(\\hat{I}) \\approx \\frac{1}{m} \\widehat{\\text{Var}}(\\{f(\\theta_i)\\}) $\n> slow convergence (accuracy of MC error)\n> but! -> nice: indep. of dim($\\theta$)\n> so MC error rate is indep of dimension. oooft!\nMC sample variance\n$ \\widehat{\\text{Var}}[\\{f(\\theta_i)\\}] = \\frac{1}{m-1} \\sum_{i=1}^m (f(\\theta_i) - \\hat{I})^2 $\n> MC error scales as $m^{-1/2}$\n> slow but indep of dim($\\theta$)!\n> an approx.\n\n---\n(Page 62)\n## Lecture 14\n_24.2.25_\n> note on useful papers of gaussians on moodle, & for ex sheet 2.\nToday: Bayesian Computation, Direct sampling, Importance Sampling\n\n### Monte carlo direct sampling (slides)\n> e.g. previous example is gaussian-gaussian model with non-informative prior.\nHow does one sample from joint? -> it factorises.\nFactorise posterior: $ P(\\mu, \\sigma|y) = P(\\mu|\\sigma^2, y) P(\\sigma^2|y) \\leftarrow \\text{Factorises!} $\n1. $ \\sigma_i^2 \\sim P(\\sigma^2|y) \\qquad \\text{Inv-}\\chi^2 $\n2. $ \\mu_i|\\sigma_i^2 \\sim P(\\mu|\\sigma_i^2, y) \\qquad \\text{Normal} \\rightarrow (\\mu_i, \\sigma_i^2) \\sim P(\\mu, \\sigma^2|y) $\n> from posterior\n> Joint-draw from posterior!\n\n[A diagram with two panels. The left panel shows a 2D scatter plot labeled \"Joint\". The x-axis is μ, and the y-axis is σ². A cloud of blue dots represents the joint samples. Below this is a histogram labeled \"Marginal\" for μ. To the right is a histogram labeled \"Marginal\" for σ². Notes are added to the diagram.]\n> e.g. $\\bar{y}=0$, $s^2=1$, $n=10$. Arbitrary choice of sufficient statistics.\n> reminder, summarizing data into sufficient statistics\n> Compute posterior summaries from monte carlo.\n> $E[\\mu|y]=1.10$.\n> $Std[\\sigma|y]=0.25$\n> $E[\\sigma|y]=0.00$.\n> $Std[\\mu|y]=0.14$.\n\n(did not need to solve analytically to get marginals)\n\n### Kernel density estimate = estimate a smooth density from samples.\n[A diagram shows a histogram made of many thin vertical blue lines (representing samples), with a smooth, bell-shaped curve drawn over it. The curve is thicker and more pronounced in the center.]\n> can think of it as \"a smoothed histogram\".\n\n---\n(Page 63)\n### KERNEL DENSITY ESTIMATION (board)\n$ \\theta_i \\stackrel{iid}{\\sim} P(\\theta|D) \\qquad i=1,...,m $\nApprox. $ P(\\theta|D) $ using $ \\hat{P}(\\theta|D) \\approx \\frac{1}{m} \\sum_{i=1}^m N(\\theta|\\theta_i, bw^2) $\n$ \\int \\hat{P}(\\theta|D) d\\theta = 1 \\quad \\checkmark $\n> nice rule of thumb.\n> select bandwidth\nLots of ways to choose bandwidth, nice one is\nSilverman's Rule of Thumb:\n$ bw = \\left(\\frac{4\\hat\\sigma^5}{3m}\\right)^{1/5} \\quad \\hat\\sigma^2 = \\text{Var}(\\theta|D) \\approx \\widehat{\\text{Var}}(\\{\\theta_i\\}) $\n> sample variance $ \\frac{1}{m-1}\\sum(\\theta_i - \\bar{\\theta})^2?? $\n\n### SUMMARISING POSTERIOR UNCERTAINTIES\n* Can compute posterior mean (or median) $\\pm$ posterior standard deviation.\n* If posterior $P(\\theta|D)$ is Gaussian, then this contains 68% posterior probability\n* But not necessarily for non-gaussian posterior!\n\n### CREDIBLE INTERVALS\n> on Bayesian land, talk about \"credible intervals\"\n> prob that $\\theta$ true is in interval $L(\\vec{x}), U(\\vec{x})$ is $1-\\alpha$ given that we know data D.\n[Two plots of a skewed posterior distribution P(θ|D) vs θ are shown.\nLeft plot: A \"68% central credible interval\" is marked. The area under the curve between the 16% quantile and the 84% quantile is shaded. The tails, each containing 16% of the probability, are also indicated. A note says \"NOT UNIQUE!\".\nRight plot: Another interval is shaded, covering 68% of the probability mass, but it is shifted to the right, starting from a low percentile and ending at a higher one. The area is labeled \"68%\".]\n\n---\n(Page 64)\n### (HPD) HIGHEST POSTERIOR DENSITY CREDIBLE INTERVAL (board)\n- Unique\n- Narrowest interval that contains X% posterior probability\n\n[A plot of a unimodal posterior distribution P(θ|D) vs θ. A horizontal dashed line cuts through the peak. The interval on the θ-axis where the curve is above this line, `[l(ρ), u(ρ)]`, is marked. Two lower horizontal lines for ρ₁ and ρ₂ are also shown with their corresponding intervals.]\n\nFind $\\rho$ s.t. `[l(ρ), u(ρ)]` contains X% posterior probability.\n\nHPD works for multi-modal posteriors!\n[A plot of a bimodal posterior distribution P(θ|D) vs θ. A horizontal dashed line at height ρ cuts through both peaks. The regions under the curve and above the line are shaded. The total shaded area corresponds to the HPD credible interval.]\n\nX% HPD-C.I. = set $\\Theta$ (capital $\\theta$) (possibly disconnected) with highest $\\rho$ s.t.\n$ \\forall \\theta \\in \\Theta, P(\\theta|D) > \\rho \\text{ and } \\int_{\\Theta} P(\\theta|D) = X $\n\n---\n(Page 65)\n[ What if you can't directly sample the posterior $\\theta_i \\sim P(\\theta|D)$? (board) ]\n$ E[f(\\theta)|D] = \\int f(\\theta) P(\\theta|D) d\\theta \\approx \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i) $\n\ne.g. likelihood might be output of some computer program\n* Importance sampling:\ndraw from an easier \"tractable\" distribution (importance function) $\\theta_i \\sim Q(\\theta)$ and weight the samples by $w_i = P(\\theta_i|D)/Q(\\theta_i)$ to compute expectations\n* Posterior simulation:\nMCMC, Nested sampling etc. generates draws from the posterior density iteratively in long-run\n$ p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} $\n\n---\n(Page 66)\n### IMPORTANCE SAMPLING (board)\nGiven a probability density $P(\\theta)$ [e.g. the posterior]\nWant to estimate\n$ I = E_P[f(\\theta)] = \\int f(\\theta)P(\\theta)d\\theta $\n\nHowever, $P(\\theta)$ is intractable i.e. difficult to directly sample, e.g. $\\theta_i \\sim P(\\theta)$, but can evaluate $P(\\theta_i)$ for a parameter value $\\theta_i$.\n\nSelect an instrumental distribution (importance function) from which it is easy to draw samples!\n$ \\theta_i \\sim Q(\\theta) \\rightarrow \\text{tractable, can easily draw/sample} \\rightarrow \\text{evaluate} $\nand can evaluate $Q(\\theta_i)$, and $Q(\\theta)>0$ wherever $P(\\theta)>0$.\n\nRewrite $ I = E_P[f(\\theta)] = \\int f(\\theta) \\frac{P(\\theta)}{Q(\\theta)} Q(\\theta) d\\theta $\n$ = E_Q\\left[f(\\theta) \\frac{P(\\theta)}{Q(\\theta)}\\right] $\n\nDraw samples $ \\theta_i \\stackrel{iid}{\\sim} Q(\\theta) \\quad (i=1,...,m) $\nApproximate I with importance sampling estimate:\n$ \\hat{I}^* = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) \\frac{P(\\theta_i)}{Q(\\theta_i)} = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) w_i^* $\nwhere $ w_i^* = w^*(\\theta_i) = \\text{importance weights} = \\frac{P(\\theta_i)}{Q(\\theta_i)} $\n\n---\n(Page 67)\n$ E_Q[\\hat{I}^*] = \\frac{1}{m} \\sum_{i=1}^m E_Q[f(\\theta_i)w^*(\\theta_i)] $\n$ = \\frac{1}{m} \\sum_{i=1}^m E_Q\\left[ f(\\theta_i) \\frac{P(\\theta_i)}{Q(\\theta_i)} \\right] $\n$ = \\frac{1}{m} \\sum_{i=1}^m \\int f(\\theta) \\frac{P(\\theta)}{Q(\\theta)} Q(\\theta) d\\theta $\n$ = \\frac{1}{m} \\sum_{i=1}^m \\int f(\\theta) P(\\theta) d\\theta $\n$ = \\frac{1}{m} \\sum_{i=1}^m E_P[f(\\theta)] = E_P[f(\\theta)] $\n$ = I \\quad (\\text{unbiased}) $\n\n> reminder: Var(aX+bY) = a²VarX + b²VarY\n$ \\text{Var}_Q[\\hat{I}^*] = \\frac{1}{m^2} \\sum_{i=1}^m \\text{Var}(f(\\theta)w^*(\\theta)) $\n$ = \\frac{1}{m} \\text{Var}(f(\\theta)w^*(\\theta)) $\n$ \\approx \\frac{1}{m} \\widehat{\\text{Var}}(\\{f(\\theta_i)w^*(\\theta_i)\\}) $\n$ \\widehat{\\text{Var}}[\\{ \\cdot \\}] = \\frac{1}{m-1} \\sum \\left(f(\\theta_i)w^*(\\theta_i) - \\hat{I}^*\\right)^2 $\n\n---\n(Page 68)\n## Lecture 15\n_26.2.25_\n> useful papers & gaussian's identities on moodle for sheet 2\nToday: case study \"Importance sampling the mass of the milky way galaxy\"\nContinuing: Bayesian computation, importance sampling, MCMC. (Gelman BDA ch10-12)\n\n> Recap: MC integration, want to summarize posterior, estimate w/ samples from the posterior. Replace integral w/ MC sum, which converges to desired expectation for large N. Error on estimate $\\propto 1/\\sqrt{N}$ and is indep of dim of parameter space!\n\n### Importance sampling recap (slides)\nObjective: compute expectation w.r.t. distribution $P(\\theta)$\n$ I = E_f[\\theta] = \\int f(\\theta)p(\\theta)d\\theta $\n\nExample: Posterior (suppress \"|D\" here.)\n$ P(\\theta) = \\frac{L(\\theta)\\pi(\\theta)}{\\int L(\\theta)\\pi(\\theta)d\\theta} \\qquad \\frac{\\text{likelihood}}{\\text{prior}} $\nCan evaluate $p(\\theta)$ but not sample from it directly. Choose importance function $Q(\\theta)$ you can evaluate and sample!\n> named distribution\n> e.g. normal, student's t\n\nImportance sampling estimate\n$ \\theta_1, ..., \\theta_m \\stackrel{iid}{\\sim} Q(\\theta) $\n$$ \\hat{I}^* = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) w^*(\\theta_i) $$\n$$ w_i^* = w^*(\\theta_i) = P(\\theta_i)/Q(\\theta_i) $$\n\n---\n(Page 69)\n### SELF-NORMALISED IMPORTANCE SAMPLING (slides)\nObjective: compute expectation\n$ I = E_f[\\theta] = \\int f(\\theta)P(\\theta)d\\theta $\nbut can only evaluate unnormalised $\\tilde{P}(\\theta)$.\n\nExample: unnormalised posterior\n$ \\tilde{P}(\\theta) = L(\\theta)\\pi(\\theta) $.\n> (evidence, hard to evaluate)\nNormalised posterior is $ P(\\theta) = \\frac{\\tilde{P}(\\theta)}{Z_p} = \\frac{\\tilde{P}(\\theta)}{\\int \\tilde{P}(\\theta)d\\theta} $,\nbut, cannot easily calculate\n$ Z_p = \\int L(\\theta)\\pi(\\theta)d\\theta $ so cannot evaluate $P(\\theta)$.\n\n---\n(Page 70)\n### Self-Normalised Importance Weighting (board)\nOften in Bayesian Analysis, you can only compute/evaluate posterior $P(\\theta)$ up to a constant.\n$ P(\\theta) = \\frac{L(\\theta)\\pi(\\theta)}{Z_p} = \\frac{\\tilde{P}(\\theta)}{Z_p} \\leftarrow \\text{unnormalised} $\n$ \\qquad \\uparrow \\qquad Z_p = \\int \\tilde{P}(\\theta)d\\theta $\nnormalised\n\nGOAL: Estimate $I = E_p[f(\\theta)] = \\int f(\\theta) P(\\theta)d\\theta $\n$ = \\int f(\\theta) \\frac{\\tilde{P}(\\theta)}{Z_p} d\\theta $\n$ = \\frac{\\int f(\\theta) \\frac{\\tilde{P}(\\theta)}{Q(\\theta)} Q(\\theta)d\\theta}{\\int \\frac{\\tilde{P}(\\theta)}{Q(\\theta)} Q(\\theta)d\\theta} $\n> if you can't solve a hard problem, solve an easier problem.\n\ninstrumental distribution -> probably named distribution\nChoose Q s.t. drawing samples $\\theta_i \\stackrel{iid}{\\sim} Q(\\theta)$ is easy $(i=1,...,m)$.\n> don't need to know normalization of Q either!\nApproximate I with\n$ I \\approx \\hat{I} = \\frac{\\sum_{i=1}^m f(\\theta_i) \\tilde{w}(\\theta_i)}{\\sum_{i=1}^m \\tilde{w}(\\theta_i)} = \\sum_{i=1}^m f(\\theta_i) w(\\theta_i) $ (FTMC)\n(Estimate normalisation using same samples drawn from Q)\n\nwhere the self-normalised weights are,\n$ w(\\theta_i) = \\frac{\\tilde{w}(\\theta_i)}{\\sum_{j=1}^m \\tilde{w}(\\theta_j)} = \\frac{\\tilde{P}(\\theta_i)/Q(\\theta_i)}{\\sum_{j=1}^m \\tilde{P}(\\theta_j)/Q(\\theta_j)} $\n> nice, works. when $P$ is unnormalised... we have done the bottom integral using importance sampling itself.\n\n---\n(Page 71)\n(Also works if Q is unnormalised, since const. factor in Q cancel above.)\n> whereas importance sampling we looked at before required Q to be proper i.e. normalised.\n\nCOST: $\\hat{I}$ is slightly biased\n$ E[\\hat{I}] = I + O(1/m) $\nb/c of estimating denominator (walk out yourself)\n> biased but consistent? -> as $m \\to \\infty$, $P(\\hat{I}=I) \\to 1$\n\n> 'bias arises from non-domination' process\nreminder: $E[AB] = E[A]E[B] + \\text{Cov}(A,B)$\n> since cov(A,B) = E[AB] - E[A]E[B]\n'cheat GET attempt':\n$ E_Q[\\hat{I}] = \\sum_i E_Q[f(\\theta_i) w(\\theta_i)] = \\sum_i (E_Q[f(\\theta_i)]E_Q[w(\\theta_i)] + \\text{Cov}(f_i, w_i)) $\n> unclear steps\n> since $\\sum w_i = 1$?\n> and $\\text{Cov}(f_i, w_i)$ is ignored? O(1/m²)?\n$ E_Q[w(\\theta_i)] = 1/m + O(1/m^2) $\n$ \\rightarrow E_Q[\\hat{I}] = \\sum_i(E_Q[f_i]\\theta^i(1/m + O(1/m^2))) $\n$ = E_Q[f_i(\\theta)] + O(1/m) $\n\n> some kind of perturbation analogy/taylor expansion?\n\n$ \\text{Var}[\\hat{I}] = \\sum_i^m \\text{Var}[f(\\theta_i)w(\\theta_i)] = \\frac{1}{m} \\text{Var}[f(\\theta_i)w(\\theta_i)] \\qquad ? \\text{.ie} $\n$ \\approx \\frac{1}{m} \\text{Var}(\\{f_i(\\theta_i)w_i(\\theta_i)\\}) \\quad ? $\n\n---\n(Page 72)\n### Contrived example: Gaussian Mixture, normalised PDF (slides)\n#### IMPORTANCE SAMPLING EXAMPLE (not self normalised)\n[A plot showing two curves on an x-axis. A blue, bimodal curve is labeled \"target p(x)\". A wider, purple, unimodal curve is labeled \"impt. fn. Q(x) - we choose gaussian\".]\n> mixture of 2 gaussians\n> target mean = 0.20\n> (have analytic soln but suppose we didn't have this & try to estimate)\n\nGenerate 1000 samples from impt. fn.\n[A plot of the importance weight function vs x. It shows a dashed line that is low and then rises to a peak where the target PDF has its main peak.]\n[A histogram of m=1000 draws from the importance function is shown, roughly matching the shape of Q(x).]\nMC estimate of mean\n$ \\sum w_i^* x_i = 0.215 \\pm 0.028 $\n> within $1\\sigma$ of target\n\n> could also do this for posterior variance etc.\nm=10,000 draws: mc estimate of mean is\n$ \\sum x_i w_i^* = 0.199 \\pm 0.009 $\n> error does down as $1/\\sqrt{m}$\n\n---\n(Page 73)\n### Parallax Example (slides)\n> gaussian measurement error in ῶ?\nLikelihood $ \\tilde\\omega \\sim N(1/r, \\sigma_{\\tilde\\omega}^2) $\n$ P(\\tilde\\omega|r) = \\frac{1}{\\sigma_{\\tilde\\omega}\\sqrt{2\\pi}} \\exp \\left[-\\frac{1}{2\\sigma_{\\tilde\\omega}^2}(\\tilde\\omega - 1/r)^2\\right], \\quad \\sigma_{\\tilde\\omega} > 0 $\n> parallax prop.\nTrue relation (no errors): $ \\omega = \\frac{\\text{parsec}}{r} \\leftarrow \\text{distance} $\n> parallax angle\n[Diagram showing the Earth orbiting the sun, observing a star. The parallax angle is defined. The distance to the background stars is effectively infinite.]\n> since $\\tilde\\omega \\ll 1$,\n> $\\tan \\tilde\\omega \\approx \\tilde\\omega$ to v. good approx.\n> parsec is 'parsec-second',\n> r is defined as the parsec.\n\nIntroduce physically motivated prior\n$ P(r) = \\begin{cases} \\frac{2}{L^3}r^2 e^{-r/L} & \\text{if } r>0 \\\\ 0 & \\text{otherwise} \\end{cases} $\n> as seen before, just some kind of it.\nExponential decrease in density of stars with galactic length scale L.\n$ P(r|\\tilde\\omega) \\propto P(\\tilde\\omega|r)P(r) $\nUnnormalised posterior\n$ P^*_{post}(r|\\tilde\\omega, \\sigma_{\\tilde\\omega}) = \\begin{cases} \\frac{r^2 e^{-r/L}}{\\sigma_{\\tilde\\omega}} \\exp\\left[-\\frac{1}{2\\sigma_{\\tilde\\omega}^2}(\\tilde\\omega - 1/r)^2\\right] & r>0 \\\\ 0 & \\text{otherwise} \\end{cases} $\nCan't compute normalisation analytically.\n> (in reality this is a 1D example so we could actually evaluate this on a grid and do integral numerically, but let's pretend we can't do that)\n\n---\n(Page 74)\nWant to compute posterior mean. consider posterior of distance\n[Plot showing an unnormalized prior (a skewed curve peaking at low r) and two posteriors. One posterior for ῶ=-0.01 is a rising curve. The other posterior for ῶ=0.33 is a peaked curve labeled \"unminimalised posterior\". A note points to the ῶ=0.33 curve saying it is bimodal, higher signal. A note points to the ῶ=-0.01 curve saying it has a short-tail at low r, low s/n ratio.]\n> what is the value of ῶ? we need to test\n> e.g. two mock values for ῶ=-0.01. with prior ῶ=0.33, need to use self-normalised importance sampling, or need to choose impt. fn.\n> ῶ=-0.33\n\n[Plot showing the unnormalized posterior for ῶ=0.33 and ῶ=0.01. The y-axis is labeled unnormalised PDF. The curve for ῶ=0.33 is the \"target posterior p(r|w)\". A wider, smoother curve is the \"impt. fn. Q(r)\".]\nChoose Q(r) that covers (is >) (target)\n(is should be +ve anywhere the target is +ve.)\n> choose, say, student's t-dist with v=3 (low deg of freedom)\n> choose because tail of student-t is thicker than gaussian. (gaussian drops as $e^{-x^2}$ but student t as $x^{-(\\nu+1)}$). have to scale it up more than the target if you have but we want a named distribution.\n\nM=10⁴ draws from Q(r):\n[Plot of the importance weight vs r. It is noisy and has large spikes.]\nMC estimate of mean\n$ \\sum x_i w_i = 2391.263 \\pm 3.918 $\n\nM=10000 draws from impt. fn.\n[Histogram of the number of draws vs distance r.]\n> think of weights like adjusting histogram of Q(r) to match P(r|D).\n> from wikipedia, a conceptual introduction to MCMC.\n> Imagine approximately integral we want to do over a grid of points, say 10⁵ cycles in low-prob areas, the algorithm would terminate a lot to the true mean. It is reject-dominated so small no. of points is in high prob areas. The probability to do this is high so, yes we are in regions that don't add much -> equivalent to 'concept' of 'bores' sample weights everywhere. So small sample size.\n\n> weights of medium variance -> not bad\n> large variance in weights\n> optimal goal is no variance in weights.\n> idea of ESS: how many draws from posterior do we get that are effective from original grid of non-uniform samples, we want to get more effective samples from each grid. good is has low square-root of err, low confidence.\n\n---\n(Page 75)\n### CHOOSING A GOOD IMPORTANCE FN. (slides)\n> (most important part of importance sampling)\nWithin chsq named distributions.\n* Can be shown (sheet 2) that theoretically optimal (minimum variance) importance fn. is\n$ Q^*(\\theta) = \\frac{|f(\\theta)|P(\\theta)}{\\int |f(\\theta)|P(\\theta) d\\theta} $\n> high variance in importance weights -> company, estimates -> small effective sample size.\n* However, if we can't directly sample from $P(\\theta)$, then we probably can't sample from $|f(\\theta)|P(\\theta)$.\n* Want to keep the importance weights roughly const, otherwise large variations in $P(\\theta)/Q(\\theta)$ will lead to high variance of estimate, smaller ESS.\n* Effective sample size\n> 'importance fn similar shape to dist we are trying to sample'\n$$ ESS = \\frac{m}{1+\\text{Var}[\\{w^*(\\theta_i)\\}]} $$\n> e.g. as in parallax example\n* In practice, find thick-tailed distribution $Q(\\theta)$ that is positive everywhere and similar in shape to $|f(\\theta)|P(\\theta)$.\n* Don't want $Q(\\theta)$ small when $|f(\\theta)|P(\\theta)$ large!\n> problem: small sample size, high variance.\n\n> -> ideal importance fn. is proportional to $|f(\\theta)|P(\\theta)$\n> want fn that resembles target $P(\\theta) \\times f(\\theta)$ (i.e. importance weights const -> small variance)\n> weights -> large ESS.\n> -> in practice want positive, everywhere, covers the tail (thicker tail), and... (misc)\n> weight roughly skewed distribution.",
      "timestamp": "2025-11-05T14:25:12.475467",
      "tokens": 11685,
      "cost": 0.0853275
    }
  ],
  "total_tokens": 54729,
  "total_cost": 0.170655
}