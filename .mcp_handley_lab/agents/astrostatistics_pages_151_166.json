{
  "name": "astrostatistics_pages_151_166",
  "system_prompt": null,
  "created_at": "2025-11-05T14:34:55.441768",
  "messages": [
    {
      "role": "user",
      "content": "Please transcribe pages 151-166 (the final pages) of this astrostatistics lecture notes PDF into markdown format. \n\nFollow this exact style and formatting:\n- Use page markers like `---\\n(Page N)` to indicate page numbers\n- Preserve ALL mathematical notation using LaTeX syntax (inline: $...$ and display: $$...$$)\n- Include detailed descriptions of any diagrams/images in [brackets]\n- Use markdown headers (##, ###) for lecture titles and sections\n- Preserve lecture headers with dates in format: ## Lecture N\\n_DD.M.YY_\n- Use blockquotes (>) for annotations, notes, and side comments\n- Use bullet points (*) and numbered lists as appropriate\n- Preserve all arrows (→, ←, ↑, ↓) and special symbols\n- Keep abbreviations and shorthand exactly as written\n- Maintain indentation for nested content\n- Use horizontal rules (---) between pages\n\nBe extremely thorough and accurate. Transcribe ALL text, equations, diagrams, and annotations exactly as they appear in the handwritten notes.",
      "timestamp": "2025-11-05T14:36:33.538860",
      "tokens": 43051,
      "cost": 0.040731875
    },
    {
      "role": "assistant",
      "content": "---\n(Page 151)\n\nCalc. evidence via Laplace approx:\n\n(Recall: The Laplace Approximation.)\n\nEvidence $$P(D) = \\int P^*(\\theta|D) d\\theta$$\nUnnormalised posterior $$P^*(\\theta|D) = P(D|\\theta)P(\\theta)$$\nFind MAP estimate: $$\\theta_0 = \\underset{\\theta}{\\operatorname{argmax}} \\ln P^*(\\theta|D)$$\nTaylor expansion:\n$$ \\ln P^*(\\theta|D) \\approx \\ln P^*(\\theta_0|D) - \\frac{1}{2}(\\theta-\\theta_0)^T A (\\theta-\\theta_0) + \\dots $$\nHessian at mode: $$ A_{ij} = - \\frac{\\partial^2}{\\partial\\theta_i \\partial\\theta_j} \\ln P^*(\\theta|D)|_{\\theta=\\theta_0} $$\n\n$$ P^*(\\theta|D) \\approx P^*(\\theta_0|D) \\times \\exp(-\\frac{1}{2}(\\theta-\\theta_0)^T A (\\theta-\\theta_0)) $$\n\nNot always accurate:\ne.g.\n\n[Two graphs are shown side-by-side. The left graph plots -ln P*(Θ|D) against Θ. A dark blue line shows the actual function, which is a curve. A light blue parabola is overlaid, representing the Taylor expansion approximation. The right graph plots P*(Θ|D) against Θ. The dark blue line shows the actual posterior, which is a skewed bell curve. A light blue symmetric bell curve represents the Laplace/Gaussian approximation. The area under the dark blue curve is shaded.]\n\n> e.g. laplace unnormalised posterior...\n> laplace approx is good\n\n> but if you trust your approximation on an integral we 'praus' & get this?\n\n\\* Evidence from Laplace Approximation: \\*\n$$ P(D) = \\int P^*(\\theta|D) d\\theta \\approx P^*(\\theta_0|D) \\times |2\\pi A^{-1}|^{1/2} \\int N(\\theta|\\theta_0, A^{-1}) d\\theta $$\n\n> w.r.t P(D|Θ) x det(2πA)⁻¹/² ?\n\n$$ P(D) \\propto P^*(\\theta_0|D) \\times \\det(A/2\\pi)^{-1/2} $$\n$$ P(D) \\approx P(D|\\theta_0) \\times P(\\theta_0) \\times \\det(A/2\\pi)^{-1/2} $$\n\n> having wide prior?\n> unnormalised?\n> need to be able to evaluate posterior at least 2x to do this?\n> in many cases just use (unstable posterior)\n\n$$ A = - \\nabla \\nabla \\ln P(\\theta|D) $$\n\n---\n(Page 152)\n\nthis implements Occam's razor\n\n### Evidence implements Occam's Razor\n> samples to estimate better\n\n$$ \\underbrace{P(D)}_{\\text{Evidence}} \\approx \\underbrace{P(D|\\theta_0)}_{\\text{best fit likelihood}} \\times \\underbrace{P(\\theta_0) \\times \\det(A/2\\pi)^{-1/2}}_{\\text{Occam factor}} $$\n> (?? or just A⁻¹/²)\n\n$$ \\Sigma_{post} = A^{-1} \\quad \\text{roughly} $$\n\nhow can we see this?\n### Suppose:\n$$ P(\\theta) = N(\\theta|\\theta_{prior}, \\Sigma_{prior}) $$\n> Suppose prior is Gaussian\n\n### Occam Factor:\n> assume prior centered near MAP to do so or use 'generalised' version? (shan't get)\n$$ \\propto |2\\pi \\Sigma_{prior}|^{-1/2} \\times |2\\pi \\Sigma_{post}|^{1/2} $$\n$$ \\propto \\frac{|\\Sigma_{post}|^{1/2}}{|\\Sigma_{prior}|^{1/2}} \\quad \\text{(ratio of posterior to prior width)} $$\n\n[A diagram shows two bell curves on the same axis. One is wide and labeled \"prior\". The other is much narrower, taller, and centered at the same point, labeled \"post\".]\n\n> From inference book:\n> * factor by which our model's hypothesis space collapses when the data arrive\n> * magnitude of Occam factor is a measure of complexity of the model\n> * depends on both\n>     * no. of free params in model\n>     * prior prob model assigns to those params\n>\n> more params -> multiply factor by N -> more complex model? less simple model\n\n> makes sense by itself?\n> ($P(D|M_1) / P(D|M_2)$)\n> volume of params space for each model. Balance between maximising data fit so we minimise complexity comparisons.\n\n> Simple models concentrate probability around a limited no of datasets. Complex models must make range of datasets - best probability of a given dataset will be smaller i.e. even if both simple & complex model can predict data, complex will have much smaller prob of\n\n---\n(Page 153)\n\n> for nested models?\n> useful method that simple model $M_0$ is sub-model of complex model.\n\n### Savage-Dickey Ratio\n\nSuppose the parameters are $\\phi, \\psi$ and complex model $M_1$ reduces to simpler model $M_0$ when $\\psi=0$. (Nested)\n> e.g. cosmological params if some set to zero, then we get simple params (ΛCDM?)\n> or e.g. how many terms for polynomial fit? constants=0\n\nAnd the prior is separable: $P(\\phi, \\psi | M_1) = P(\\psi | M_1) P(\\phi | M_1)$\n> nuisance parameters? (dependent?)\n> assume independence (for this formulation)\n> see parameters in common between model and submodel (unknown)\n\nAnd the prior on $\\phi$ is the same for each model:\n$$ P(\\phi|M_1) = P(\\phi|M_0) $$\n> prior on nuisance parameters\n\nThen the Bayes Factor reduces to:\n$$ B_{01} = \\frac{P(D|M_0)}{P(D|M_1)} = \\frac{P(\\psi|D,M_1)}{P(\\psi|M_1)} |_{\\psi=0} $$\n> Show this:\n> * consider numerator: $P(D|M_0) = \\int P(D|\\phi, M_0) P(\\phi|M_0) d\\phi = \\int P(D|\\phi, \\psi=0, M_1) P(\\phi|M_1) d\\phi$.\n> * $P(\\psi=0|D, M_1) = P(\\psi=0|D,M_1) P(D|M_1) / (P(D|M_1))$ (wrong)\n> * num part becomes $P(\\psi=0|D, M_1) P(D|M_1)$\n\n[A complex diagram with several plots. On the left, a plot with vertical axis $\\psi$ and horizontal axis $P(D|\\psi,M_i)$ shows two overlapping vertical distributions for $P(D|M_1)$ and $P(D|M_2)$. On the right, a horizontal axis labeled $\\theta$ has four sets of distributions above it for models $M_0, M_1, M_2, M_3$. Each set shows a wider prior distribution (e.g., $P(\\theta|M_1)$ prior) and a narrower posterior distribution (e.g., $P(\\theta|D,M_1)$ posterior). The posteriors are shifted relative to the priors. A dashed line connects the peaks of the posterior distributions. Annotations are scattered around the plots.]\n\n> FROM INFERENCE BOOK. How does... without... relate to model complexity?\n\n> For this data, evidence of model $M_0$ is greater than for $M_3$ because area under curve for $M_3$ is greater.\n> i.e. $M_0$ has Occam factor $\\psi$? simple model but has the worse fit. $M_1$ has the best fit but posterior is still poor in comparison to prior.\n\n> if range of $\\psi$ for $M_3$ is small we use wider prior for complex model\n\n---\n(Page 154)\n\n### NESTED SAMPLING\n> for evidence calculation but also for posterior samples with byproduct!\n\n* An algorithm designed to compute Bayesian evidence (John Skilling, 2004)\n* Can be better than MCMC for multi-modal distributions\n* Get weighted samples from posterior for free\n* Evolve an ensemble of \"live points\" successively sampling prior volume above a likelihood-level constraint\n* Perform evidence integral over contours of equal likelihood\n* Uses statistical estimate of prior mass within likelihood-level\n\n---\n(Page 155)\n\nWant to calculate multi-dim. evidence integral\n$$ Z = \\int L(\\Theta)\\pi(\\Theta)d\\Theta $$\n\nDefine \"prior mass enclosed above likelihood $L^*$\"\n$$ X(L^*) = \\int_{L(\\Theta)>L^*} \\pi(\\Theta) d\\Theta $$\n\nInverse: Likelihood at enclosed prior mass: $L(X)$\n\n1D evidence integral: $$Z = \\int L(X) dX$$\n> (= likelihood value (from X=0) start high initially?)\n\n[Two diagrams are shown side-by-side. The left diagram is labeled \"PARAMETER SPACE $\\Theta$\". It shows a square with three nested, irregular, concentric contour lines labeled $\\Theta_1, \\Theta_2, \\Theta_3$ from outermost to innermost. An arrow points from outside the contours to just inside the $\\Theta_3$ contour, labeled $X(L_3)$. The right diagram has a vertical axis labeled \"ENCLOSING LIKELIHOOD L\" and a horizontal axis labeled \"ENCLOSED PRIOR MASS X\", with X ranging from 0 to 1. A monotonically decreasing curve labeled $L(X)$ is drawn. The area under the curve is divided into vertical colored bands corresponding to likelihood levels $L_1, L_2, L_3$ and prior mass values $X_1, X_2, X_3$.]\n\nNested likelihood contours sort to enclosed prior mass X.\n* e.g. 2D example take 3 random points from prior $\\Theta_1, \\Theta_2, \\Theta_3$ and calc. their likelihood, each will be associated with some likelihood contour. each contour is associated with some prior value & the prior volume X(L?).\n* So can associate with each point a likelihood contour and also a prior mass enclosed above that likelihood.\n* If we can map from points in param. space to L(X), I can turn a previously multi-dim integral in param space to a 1-Dim integral in X from 0 to 1.\n\n---\n(Page 156)\n\n### Nested Sampling: How?\n> how do we achieve this?\n\n[NB: For the rest of this lecture assume prior is uniform box in parameter space (w/ total integral 1)]\n\n1. Begin by sampling $N_{live}$ points $\\{\\Theta_i\\}$ from the prior ($X_0=1$), evaluate likelihood for each live point, initialise evidence $Z=0$.\n\n2. Find the live point with smallest likelihood $L^*$.\n\n3. Estimate compression factor $t_i = X_i/X_{i-1}$.\n> volume is estimated statistically. $t_i \\in (0, 1)^{N_{live}}$\n\n4. Accumulate evidence as a 1D integral\n$$ \\Delta Z = L^* \\times (X_{i-1} - X_i) = L^* \\times (1-t_i)X_i $$\n\n5. Sample a new live point from prior constrained to $\\{\\Theta | L > L^*\\}$, compute its likelihood value.\n> (e.g. with MCMC or otherwise) (easy to draw points?)\n> reject anything that does not satisfy constraint\n> simple reject & get?\n\n6. Repeat steps 2-5 until convergence (i.e. evidence $Z$ stops changing within some tolerance).\n\n7. Add final evidence estimate of remaining live points $\\Delta Z = \\bar{L} \\times X_{end}$.\n\n8. List of dead points $\\{\\Theta_i\\}$ give weighted posterior samples. $$w_i \\propto (1-t_i)X_i$$",
      "timestamp": "2025-11-05T14:36:33.539526",
      "tokens": 2765,
      "cost": 0.040731875
    }
  ],
  "total_tokens": 45816,
  "total_cost": 0.08146375
}