{
  "name": "astrostatistics_transcription",
  "system_prompt": null,
  "created_at": "2025-10-31T15:12:09.977597",
  "messages": [
    {
      "role": "user",
      "content": "You are transcribing handwritten lecture notes from an astrostatistics course. I need you to transcribe pages 26-166 of this PDF in the exact same style as the existing transcription.\n\n**TRANSCRIPTION STYLE RULES:**\n\n1. **Page markers:** End each page with `---` followed by `(Page N)` on the next line\n2. **Lecture headers:** Use `## Lecture N` and `_date_` in italics\n3. **Section headers:** Use `###` for major sections (all caps for emphasis)\n4. **Math notation:** Use LaTeX in inline `$...$` or display `$$...$$` format\n5. **Diagrams/Images:** Describe in square brackets like `[Image of ...]` with detailed description\n6. **Comments/notes:** Use `>` for marginal notes or annotations\n7. **Emphasis:** Use `**bold**` for important terms/concepts, `*italics*` for emphasis\n8. **Lists:** Use `-` for bullet points when appropriate\n9. **Preserve structure:** Maintain the hierarchical structure of definitions, examples, theorems\n\n**EXAMPLE FROM EXISTING TRANSCRIPTION:**\n\n```\n## Lecture 3\n_29.1.25_\n\n### FUNDAMENTALS AND NOTATION\n\nData are realisations of **random variables** (outcomes of probabilistic experiments/observations)\n\nA **random variable** $X$ can take on values in some domain (measurable space)\ne.g. (discrete) $\\mathbb{Z}$, (continuous) $\\mathbb{R}$, (multivariate/vector combinations) $\\mathbb{R}^N$\n\n**Discrete case:**\n\nDefine the probability distribution over possible outcomes with a **probability mass function** (PMF)\n$$ \\mathrm{Pr}(X=k) = P_X(k) $$\n> prob r.v. takes some value k\n\n---\n(Page 3)\n```\n\n**YOUR TASK:**\nPlease transcribe pages 26-50 of the PDF (we'll do the rest in batches). Start exactly where page 26 begins. Maintain the same careful attention to:\n- Mathematical notation accuracy\n- Diagram descriptions\n- Marginal notes and annotations\n- Page structure and flow\n\nStart with page 26 and output the transcription in the same markdown format.",
      "timestamp": "2025-10-31T15:17:19.780150",
      "tokens": 43313,
      "cost": 0.087565625
    },
    {
      "role": "assistant",
      "content": "> (evaluate Fisher Matrix)\n\n### Quick Example:\n\n$X_i \\stackrel{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)$ , $i=1, ..., N$\n$\\hat{\\theta} = (\\mu, \\sigma^2)$\n$$ L(\\vec{\\theta}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(x_i - \\mu)^2 / \\sigma^2} $$\n$$ \\rightarrow \\ell(\\vec{\\theta}) = \\sum_{i=1}^N -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2} $$\n> 1st deriv.\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\mu)}{\\sigma^2} = 0 $$\n$$ \\rightarrow \\hat{\\mu}_{MLE} = \\frac{1}{N}\\sum_{i=1}^N x_i = \\bar{X} \\rightarrow \\mathrm{Var}(\\hat{\\mu}_{MLE}) = \\sigma^2/N $$\n> 2nd deriv.\n$$ \\frac{\\partial^2 \\ell}{\\partial \\mu^2} = \\sum_{i=1}^N -1/\\sigma^2 = -N/\\sigma^2 $$\n> deriv w.r.t. $\\sigma^2$\n$$ \\frac{\\partial \\ell}{\\partial \\sigma^2} = \\sum_{i=1}^N - \\frac{1}{2}\\frac{1}{\\sigma^2} + \\frac{1}{2}\\frac{(x_i - \\mu)^2}{(\\sigma^2)^2} = 0 $$\n$$ \\rightarrow \\hat{\\sigma}^2_{MLE} = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^2 $$\n> symmetrical, unbiased but biased variance\n> see e.g. wiki \"Variance\"\n(N.B. this is **biased**! $E[\\hat{\\sigma}^2_{MLE}] = \\frac{N-1}{N}\\sigma^2$)\nBut **sample variance** $S^2 = \\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\bar{x})^2$ is **unbiased** for $\\sigma^2$.\n\n> 2nd deriv w.r.t $\\sigma^2$\n> don't actually need\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\sum_{i=1}^N \\frac{1}{2\\sigma^4} - \\frac{(x_i - \\mu)^2}{\\sigma^6} $$\n$$ E\\left[\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2}\\right] = - \\frac{N}{2\\sigma^4} \\quad (\\text{using } E[(x_i-\\mu)^2] = \\sigma^2) $$\n> Bessel's correction, biases/expectations etc, some $(x_i-\\mu)^2$ start as random sum, $(x_i-\\bar{x})^2$ depends on data $(x_i, \\bar{x})$ are correlated, number of d.f. split\n\n---\n(Page 26)\n\n> convince yourself\n\nCross term $E\\left[\\frac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\mu}\\right] = 0$ (using $E[x_i-\\mu]=0$)\n> (actually $\\frac{\\partial^2\\ell}{\\partial \\sigma^2 \\partial \\mu}$ etc?)\n\n$$ \\mathcal{I} = \\begin{pmatrix} -\\frac{\\partial^2 \\ell}{\\partial \\mu^2} & -\\frac{\\partial^2 \\ell}{\\partial \\mu \\partial \\sigma^2} \\\\ -\\frac{\\partial^2 \\ell}{\\partial \\sigma^2 \\partial \\mu} & -\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\end{pmatrix} = \\begin{pmatrix} N/\\sigma^2 & 0 \\\\ 0 & N/2\\sigma^4 \\end{pmatrix} $$\n$$ \\rightarrow \\mathcal{I}^{-1} = \\begin{pmatrix} \\sigma^2/N & 0 \\\\ 0 & 2\\sigma^4/N \\end{pmatrix} $$\n$$ \\left. \\begin{aligned} \\mathrm{Var}(\\hat{\\mu}_{MLE}) &= \\sigma^2/N \\geq (\\mathcal{I}^{-1})_{11} \\\\ \\mathrm{Var}(\\hat{\\sigma}^2_{MLE}) &\\geq (\\mathcal{I}^{-1})_{22} \\end{aligned} \\right\\} $$\n> check this yourself. is variance greater than CRLB? should follow from MLE theory?\n\n$$ \\mathrm{Var}(\\hat{\\mu}_{MLE}) = (\\mathcal{I}^{-1})_{11} \\text{ as } N \\rightarrow \\infty \\quad \\checkmark \\text{ as expected} $$\n\n---\n(Page 27)\n\n### Laplace Approximation\n> remember how's \"likelihood\" your \"map\"\n\nrecall $\\ell(\\theta) = \\ln L(\\theta)$\n> want to be near the parameters for MLE\nTaylor expansion around MLE:\n$$ \\ell(\\theta) \\approx \\ell(\\hat{\\theta}_{MLE}) + \\underbrace{\\left.\\frac{\\partial \\ell}{\\partial \\theta}\\right|_{\\hat{\\theta}_{MLE}}}_{0}(\\theta - \\hat{\\theta}_{MLE}) + \\frac{1}{2}\\underbrace{\\left.\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right|_{\\hat{\\theta}_{MLE}}}_{-\\hat{\\mathcal{I}}} (\\theta - \\hat{\\theta}_{MLE})^2 + \\dots $$\n\nAsymptotically, $N \\rightarrow$ large\n$$ \\ell(\\theta) \\approx \\ell(\\hat{\\theta}_{MLE}) - \\frac{1}{2}\\hat{\\mathcal{I}}(\\theta - \\hat{\\theta}_{MLE})^2 $$\n$$ L(\\theta) \\approx L(\\hat{\\theta}_{MLE}) e^{-\\frac{1}{2}(\\theta - \\hat{\\theta}_{MLE})^2 / \\hat{\\mathcal{I}}^{-1}} \\quad \\text{(Gaussian shape)} $$\n> useful bit for homework, in terms of MLE result and Hess\n\nMultiparameter case\n$$ L(\\vec{\\theta}) \\approx L(\\hat{\\vec{\\theta}}_{MLE}) \\times e^{-\\frac{1}{2}(\\vec{\\theta} - \\hat{\\vec{\\theta}}_{MLE})^T \\hat{\\mathcal{I}} (\\vec{\\theta} - \\hat{\\vec{\\theta}}_{MLE})} $$\n\nReminder: multidim Taylor expansion\n$$ f(x) = f(a) + \\underbrace{\\nabla f(a)}_{\\text{grad at a}}(x-a) + \\frac{1}{2}(x-a)^T \\underbrace{H(a)}_{\\text{Hessian of f at a}}(x-a) + \\dots $$\ne.g. $f(x,y) = f(a,b) + \\frac{\\partial f}{\\partial x}(x-a) + \\frac{\\partial f}{\\partial y}(y-b) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(x-a)^2 + \\frac{\\partial^2 f}{\\partial y^2}(y-b)^2 + \\frac{\\partial^2 f}{\\partial x \\partial y}(x-a)(y-b) + \\dots$\n\n---\n(Page 28)\n\n### Example (slides): Calibrating type Ia supernova absolute magnitudes with intrinsic dispersion and heteroskedastic measurement error.\n> Heteroskedastic\n\n### Determining Astronomical Distances using Standard Candles\n> key step in determining hubble constant.\n\n1. **Estimate** or **model luminosity L** of a class of astronomical objects\n2. **Measure apparent brightness** or **flux F**\n3. **Derive distance D** to object using inverse square law\n   $$ F = L/4\\pi D^2 $$\n4. Optical astronomers units $\\mu = m-M$\n   - $m$ = **apparent magnitude** (log apparent brightness, flux)\n   - $M$ = **absolute magnitude** (log luminosity)\n   - $\\mu$ = **distance modulus** (log distance)\n\n> can use hubble to get distances\nHubble: $V = H_0 \\times D$ distance $\\propto$ velocity (redshift)\n> Hubble's law $z \\approx H_0 d$\n\nNow use **type Ia Supernovae** as **standard candles**\n\nWe want to **measure the hubble constant**\n$\\rightarrow$ can use **local distance ladder**\n> ladder because one method is bootstrapped into the method of another one.\n\n> 2nd tier of distance ladder\nCan use **cepheid stars**: period of rotation linked to luminosity\n> (pulsating)\n> 1st tier: geometry, 2nd tier: cepheids, 3rd tier: supernovae. use links between tiers to calibrate. e.g. same distance, cepheids & supernovae in the same galaxy\n> i.e. 3 rungs of the distance ladder.\n> hubble flow things that are further away tend to move away from us faster\n\n---\n(Page 29)\n\n## Lecture 8\n_10.2.25_\n\n### Recap example (slides)\n- Measure **apparent brightness** or **flux F**\n- Derive **distance** to object using **inverse square law**\n  $F = L/4\\pi D^2$\n- Key step in determining **hubble constant**. > (redshift)\n- 3 rungs to distance ladder: geometry, cepheids, supernovae (can calibrate between rungs).\n\n> we will focus on cepheids\n> both on board!\n\n### Calibrating SNIa Abs Mags (board)\n\nCalibrator sample we observe: N SNe, $s=1,...,N$\n- $M_s$ = true apparent magnitude\n- $\\hat{M}_s$ = measured apparent magnitude\n- $\\hat{M}_s \\sim N(M_s, \\sigma_{ms}^2) \\leftarrow$ given, public\n\nequivalently, $\\hat{M}_s = M_s + \\mathcal{E}_{ms} \\quad \\mathcal{E}_{ms} \\sim N(0, \\sigma_{ms}^2) \\leftarrow$ data\n\n**Cepheid distance estimates** to SN galaxy:\n- $\\mu_s$ = true distance modulus\n  $= 25 + 5\\log_{10}(\\frac{\\text{distance}}{\\text{Mpc}})$\n> again with measurement error on distance\n- $\\hat{\\mu}_s \\sim N(\\mu_s, \\sigma_{\\mu s}^2) \\leftarrow$ given in data table\n\nequiv. $\\hat{\\mu}_s = \\mu_s + \\mathcal{E}_{\\mu s} \\quad \\mathcal{E}_{\\mu s} \\sim N(0, \\sigma_{\\mu s}^2) \\leftarrow$ \"data\"\n\n---\n(Page 30)\n\n### Abs Mag Distribution (Population)\n> e.g. want to... intrinsic scatter looks like random variations in absolute mag due to dust, properties etc.\n$M_s \\sim N(M_0, \\sigma_{int}^2)$ $\\leftarrow$ unknown\nequiv. $M_s = M_0 + \\mathcal{E}_{int,s} \\rightarrow N(0, \\sigma_{int}^2)$\n\n> We want to estimate $\\theta$, so form likelihood eq:\n> latent variable without noise $\\rightarrow$ relate to observed variables\n$M_s = M_s + \\mu_s$ (**latent variable equation**)\n\nDefine: $\\hat{M}_s = \\hat{M}_s - \\hat{\\mu}_s$ (Estimated Abs Mag)\n$= M_s + \\mathcal{E}_{ms} - \\mu_s - \\mathcal{E}_{\\mu s}$\n$= M_s + \\mathcal{E}_{ms} - \\mathcal{E}_{\\mu s} \\rightarrow \\sigma^2_{err,s}$\n> use: variance of sum of gaussian RVs is sum of variance\n$= M_s + \\mathcal{E}_{err,s} \\quad N(0, \\sigma^2_{ms} + \\sigma^2_{\\mu s})$\n> combine errors\n> abs mag define\n$\\hat{M}_s = M_0 + \\mathcal{E}_{int,s} + \\mathcal{E}_{err,s}$\n> w/o measurement errors\n$\\hat{M}_s \\sim N(M_0, \\sigma^2_{int} + \\sigma^2_{err,s})$\n> combine variances (as independent)\n\n### Likelihood:\n> note: $\\hat{M}_s$ not i.i.d! How? (different $\\sigma^2_{err,s}$)\n> aka heteroskedastic\n$P(\\hat{M}_s|M_0, \\sigma^2_{int}) = N(\\hat{M}_s|M_0, \\sigma^2_{int} + \\sigma^2_{err,s})$\n> (independence)\n$L(\\theta) = P(\\hat{\\mathbf{M}}|M_0, \\sigma^2_{int}) = \\prod_{s=1}^N N(\\hat{M}_s|M_0, \\sigma^2_{int} + \\sigma^2_{err,s})$\n\n**heteroskedastic** measurement error\n> typical in astronomy\n> example of heteroskedastic data is multiple sources of instruments, this one cannot be solved analytically. e.g. need to solve numerically.\n**cannot solve MLE analytically!**\n> (see slides for code example)\n\n---\n(Page 31)\n\n### Supernova Absolute Magnitude Distribution: Selection Effects (board)\n> Begin to think about nuisance, see how things begin to become tricky\nSame eq. but now\nAssume **measurement error = 0** = $\\sigma_{ms} \\geq \\sigma_{\\mu s}$.\nPopulation variability $\\sigma_{int} \\rightarrow \\sigma$.\n**Only intrinsic population** is source of variability $(\\sigma^2 = \\sigma^2_{int})$.\n> e.g. sampling q. of quasars?\n\nSuppose **distance** is the **same** for an entire sample of SN $\\mu_s = \\mu$.\n\npopulation dist. iid.\n$M_s \\sim N(M_0, \\sigma^2) \\quad$ (latent variable eq. $M_s = S + \\mu_s$)\ncombine.\n$\\rightarrow \\hat{M}_s \\sim N(M_0 + \\mu, \\sigma^2) \\quad s=1,...,N$\n\n[Image of a plot with x-axis 'm (dimmer)' and y-axis '$\\mu$ (fainter)'. A diagonal dashed line represents $\\mu = m - M_0$. Two Gaussian distributions are shown. The top one, at $\\mu_2$, has some of its right tail cut off by a vertical dashed line at $m_{lim}$ labeled \"survey limit\". The observed SN are shown as 'x's to the left of the limit, and unobserved SN are 'x's to the right. The peak of this Gaussian is at $m_2 = M_0 + \\mu_2$. The bottom Gaussian, at $\\mu_1$, is fully to the left of the limit. Its peak is at $m_1 = M_0 + \\mu_1$. A region to the right of $m_{lim}$ is labeled \"to faint to be observable\".]\n> observed SN\n> unobserved SN\n\n> e.g. scope is only going to detect up to certain magnitude limit, can't see anything dimmer than this. Surveys looking for SNe at any magnitude look similar, but in the end we don't see the one above some limit, what do we do?\n\n---\n(Page 32)\n\n**Data**: $\\{\\hat{M}_s\\} = \\hat{\\mathbf{m}}$\nfixed $\\mu$ for sample\n\n### Naive Likelihood:\n$$ P(M_s|M_0, \\sigma^2) = N(M_s|\\mu+M_0, \\sigma^2) $$\n$$ L(M_0, \\sigma^2) = \\prod_{s=1}^N P(M_s|M_0, \\sigma^2) $$\n\n> naively apply naive likelihood\nIf $M_0+\\mu$ close to $m_{lim}$, MLE will be **biased**:\n$$ \\left\\{ \\begin{array}{l} \\hat{M}_0 \\text{ too bright} \\\\ \\hat{\\sigma} \\text{ too small} \\end{array} \\right. $$\n> (we didn't see m,dim everywhere)\n> (b/c tail of dist. is cut off. different width, smaller than true criteria)\n\n> how do we resolve this?\n### Accounting for selection Effects (sheet 1, Q2)\n\nLet $I_s = \\begin{cases} 1, & \\text{if SN observed} \\\\ 0, & \\text{if SN NOT observed} \\end{cases}$\n> indicator variable\n\nDefine **Selection Function**:\n> prob. indicator given data\n$$ P(I_s|M_s) = \\begin{cases} 1, & M_s < M_{lim} \\\\ 0, & M_s \\ge M_{lim} \\end{cases} $$\n$$ = 1 - H(M_s - M_{lim}) $$\n> heaviside step fn\n\n[Image of a step function plot. X-axis is $m_{lim}$, Y-axis is unlabeled. The function is 1 for values less than $m_{lim}$ and 0 for values greater than or equal to $m_{lim}$.]\n\n---\n(Page 33)\n\n## Lecture 9\n_12.2.25_\n\n> (beg recap SN problem setup on board - useful for sheet!)\n\n### SELECTION EFFECTS cont.\nrecap:\n$M_s = N(M_0, \\sigma^2)$\n$s=1,...,N$ (known)\n$\\mu_s = \\mu$ (known)\n> known, effectively no measurement error\n\n[Image of a Gaussian distribution on an x-axis labeled 'm'. The peak is at M. The right tail of the Gaussian is cut off by a vertical line at $M_{lim}$, and observed points ('x's) are to the left of the line.]\n\nFormulate likelihood to account for selection effects:\nLet $I_s = \\begin{cases} 1, & \\text{if SN observed} \\\\ 0, & \\text{if SN NOT observed} \\end{cases}$\n\n**Selection function**: $P(I_s=1|m_s)$:\n$$ S(m_s) = P(I_s|m_s) = \\begin{cases} 1, & m_s < M_{lim} \\\\ 0, & m_s \\ge M_{lim} \\end{cases} $$\n\n[Image of a step function plot. X-axis is 'm', y-axis is $S(m_s)$. The function is 1 for values up to $M_{lim}$ and 0 after. Labeled $1-H(m_s-M_{lim})$.]\n\n---\n(Page 34)\n\n### Observed Data Likelihood:\n$$ P(m_s|I_s=1, \\theta) = \\frac{P(I_s=1, m_s|\\theta)}{P(I_s=1|\\theta)} $$\n$$ = \\frac{P(I_s=1|m_s, \\theta)P(m_s|\\theta)}{\\int P(I_s=1|m_s, \\theta)P(m_s|\\theta) dm_s} $$\n$$ = \\frac{S(m_s)N(m_s|M_0+\\mu, \\sigma^2)}{\\int S(m_s)N(m_s|M_0+\\mu, \\sigma^2) dm_s} $$\n> cut off above $M_{lim}$\n$$ = \\frac{[1-H(m_s-M_{lim})]N(m_s|M_0+\\mu, \\sigma^2)}{\\int_{-\\infty}^{M_{lim}} N(m_s|M_0+\\mu, \\sigma^2) dm_s \\quad \\rightarrow \\Phi \\text{ Gaussian CDF}} $$\n> to normalise\n> scale this upon denominator\n$$ (\\text{TRUNCATED}) = \\text{TN}(m_s|M_0+\\mu, \\sigma^2, -\\infty, M_{lim}) $$\n**NORMAL**\n$P(m_s|I_s=1, \\theta)$\n[Image of a truncated normal distribution. A full Gaussian is shown dashed, and the part to the left of a vertical line at $M_{lim}$ is solid. The peak is at $M_0+\\mu$. Labels: untruncated mean & variance, lower truncation limit, upper truncation limit.]\n\n### Challenge: what if $S(m_s) = P(I_s=1|m_s) = \\Phi\\left(\\frac{M_{lim}-m_s}{\\sigma_{lim}}\\right)$?\n> limit but a broad boundary, to take into account e.g. observer bias / seeing conditions / cloudy days?\n[Image of a sigmoid function (CDF of a Gaussian). X-axis is 'm', y-axis is not labeled. The function decreases from 1 to 0 around $M_{lim}$. A Gaussian centered at $M_{lim}$ with width $\\sigma_{lim}$ is shown below the curve. It is labelled $S(m_s)$ and 'A' on the y-axis, and $m_{lim}$ on the x-axis.]\n\n---\n(Page 35)\n\n> (Slides)\n> Ex sheet 1 q 2: star formation in Perseus\n> 3 in densest cloud regions, stars are hidden\n> 3 in densest regions will be where target stars are -> selection effect\n> Pareto or power law distribution $P(m) \\propto m^{-\\alpha}$ (for $m>m_0$)\n> [A sketch of two likelihood curves on an axis labeled 'power law exponent'. One curve, labeled 'true likelihood', has a peak. The other, labeled 'naive MLE -> biased', is shifted, indicating it \"does not account for selection effects\".]\n\n### QUANTIFYING UNCERTAINTY USING BOOTSTRAP\n\n### Frequentist interpretation:\n\nConsider **variability** of your estimator $g(\\vec{x})$ for $\\theta$ under (imaginary) repetitions of your experiment. (Random realisations of the potential data).\n\nHow does $g(\\vec{x})$ behave under the potential datasets you **did not** observe?\ne.g. $\\mathrm{Var}[g(\\vec{x})] = E[(g(\\vec{x}) - E[g(\\vec{x})])^2]$\n> under $P(x|\\theta)$\n\nIf $g(\\vec{x})$ is approximately Gaussian distributed.\n$\\Rightarrow$ 68% **confidence interval** $g(\\vec{x}) \\pm \\sqrt{\\mathrm{Var}(g(\\vec{x}))}$\n> standard deviation interval\n> $[g(\\vec{x}) - \\sqrt{\\mathrm{Var}(g(\\vec{x}))}, g(\\vec{x}) + \\sqrt{\\mathrm{Var}(g(\\vec{x}))}]$\n\n$(1-\\alpha)\\%$ confidence interval $[L(\\vec{x}), U(\\vec{x})]$ contains the true value $\\theta_{true}$ in at least $(1-\\alpha)\\%$ of the realisations.\n> across repeated experiments, fraction of intervals that contain the parameter will approach $(1-\\alpha)\\%$\n> NOT this -> probability interval contains the true parameter (common misconception)\n\n---\n(Page 36)\n\n> get diff interval for each experiment -> CI encapsulates interval -> 68% of those realisations contain $\\theta$.\n[Image of three confidence intervals as horizontal lines. A vertical line represents the true value $\\theta$. Two of the intervals cross the line, one does not.]\n\n$[L(x), U(x)]$ is a **random interval** vs. $[L(x_{obs}), U(x_{obs})]$ evaluated on observed numerical values, only one dataset $x_{obs}$! (either contains $\\theta_{true}$ or doesn't).\n\n### Bootstrap:\nUse the **observed dataset** to **simulate** the variability of the **unobserved** (imaginary) data sets.\n\n**BOOTSTRAP SAMPLE** = **sample with replacement** from the observed dataset to the sample size.\n\ne.g. $X_1, ..., X_5 \\stackrel{\\text{iid}}{\\sim} \\text{Poisson}(\\lambda)$\n$\\rightarrow P(X_i) = \\frac{\\lambda^{X_i} e^{-\\lambda}}{X_i!}$\n> could do w/ max. likelihood but suppose don't know dist.\n\nReal data (observed) $\\vec{X}_{obs} = (3, 8, 2, 4, 5)$.\n\nSuppose you want to estimate the **skewness** of $P(x)$ (asymmetry).\n> true mean -> = $\\lambda$\ni.e. skewness $= \\frac{E[(x-\\mu)^3]}{(\\sigma^2)^{3/2}}$\n> true variance\n\nSample skewness $g(\\vec{x}) = \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^3}{\\left(\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\bar{x})^2}\\right)^3}$\n\n---\n(Page 37)\n\nBootstrap B \"replicate\" datasets from observed dataset:\n> \"sample w/ replacement from original eg. sample 5 times from 3,8,2,4,5 gives...\"\n\n$\\vec{X}^{obs} = (3, 8, 2, 4, 5) \\rightarrow \\hat{g}_{obs} = g(\\vec{x}^{obs}) = 0.6927$\n$\\vec{X}^{b=1} = (2, 5, 4, 4, 4) \\rightarrow \\hat{g}_1 = g(\\vec{x}^{b=1}) = -0.8625$\n$\\vec{X}^{b=2} = (2, 4, 2, 8, 8) \\rightarrow \\hat{g}_2 = g(\\vec{x}^{b=2}) = 0.2115$\n$\\vec{X}^{b=3} = (5, 2, 8, 2, 5) \\rightarrow \\hat{g}_3 = g(\\vec{x}^{b=3}) = 0.3436$\n...\n$\\vec{X}^{b=B} = \\dots \\rightarrow \\hat{g}_B = \\dots$\n\nCan now compute sample variance\n$\\widehat{\\mathrm{Var}}(\\{\\hat{g}_1, ..., \\hat{g}_B\\})$\nStandard error $= \\sqrt{\\widehat{\\mathrm{Var}}}$\n$$ \\hat{g} = 0.6927 \\pm 0.635 \\approx 68\\% \\text{ C.I. } \\checkmark $$\n\n> (slides)\n> back to supernova plus example:\n> how do I get standard error on param?\n> can try Fisher matrix approach and compare this against bootstrap\n> get model to each bootstrapped realisation\n> plot max likelihood estimates for each as histograms\n> [Image of two small histograms. The left one is labeled \"true\" and shows a wide distribution. The right one is labeled \"naive\" and shows a narrower distribution, with an arrow pointing to it labeled \"(underestimates)\". Another arrow points to the naive histogram with the comment \"(this is a sketch, not necessarily true)\".]\n\n---\n(Page 38)\n\n## Lecture 10\n_14.2.25_\n\n### REGRESSION (slides)\n\n* Fit a function $E[y|x] = f(x;\\theta)$ for the **mean relation** between $y$ and $x$.\n* Basic approaches\n  - $\\rightarrow$ **ordinary least squares** (homoskedastic scatter)\n  - $\\rightarrow$ **generalized least squares** (heteroskedastic, correlated scatter)\n  - $\\rightarrow$ **weighted least squares** (minimum $\\chi^2$, known variance)\n  - $\\rightarrow$ **maximum likelihood**\n* Real data problems require more complex modelling\n  - $\\rightarrow$ **regression dilution** from covariate measurement errors\n\n[A scatter plot with x-axis \"luminosity\" and y-axis unlabeled. Data points with error bars are shown. A straight line is fitted through them.]\n> e.g. each point has diff measurement error. regression dilution; naively applying OLS gives biased fit when there are errors in x (sheet 2)\n\n### Ordinary least squares (OLS):\nLinear model $y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j X_{ij} + \\mathcal{E}_i$\n$$ Y = X\\beta + \\mathcal{E} $$\n$i=1, ..., N$ objects, $E[\\mathcal{E}_i]=0$, **homoskedastic**\n$\\mathrm{Var}[\\mathcal{E}_i] = \\sigma^2$ (known).\n\n---\n(Page 39)\n\n$$ y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ij} + \\mathcal{E}_i $$\n> don't think this is too important to memorise? (slides)\n> [Diagram showing points and a line fit, with vertical lines indicating residuals from the points to the line]\n> minimise sum squared of these distances\n> individual sum of squares\n\nMinimise w.r.t. $\\beta$ (solve for gradient=0):\n$$ \\text{RSS} = \\sum_{i=1}^N \\left(y_i - \\beta_0 - \\sum_j^{k-1} \\beta_j x_{ij}\\right)^2 = (Y-X\\beta)^T(Y-X\\beta) $$\n> (simple linear regression see wiki)\n> e.g. figure\n$$ \\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T Y $$\n> get except $\\hat{\\sigma}^2$?\n$$ \\mathrm{Var}(\\hat{\\beta}_{OLS}) = \\sigma^2(X^TX)^{-1} $$\n$$ E(\\hat{\\beta}_{OLS}) = \\beta \\text{ (unbiased)} \\rightarrow \\textbf{BLUE} $$\n> unbiased & has minimum variance under BLUE assumptions\n> best linear unbiased estimate\n\nEstimate unknown variance $\\hat{\\sigma}^2 = \\frac{1}{N-k}(Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta})$\n> OLS estimate of variance is $\\hat{\\sigma}^2 = \\frac{RSS}{N-k}$ (for 2 parameters)\n> X? Y? simple linear is 2, OK.\n\n**(WEIGHTED LEAST SQUARES) - aka $\\chi^2$ minimisation**:\n$$ \\chi^2 = \\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\sum_j \\beta_j x_{ij})^2}{\\sigma_i^2} \\leftarrow \\text{known!} $$\n> assuming per-part we have already estimated from data (k constraints)\nMinimise w.r.t. $\\beta$:\nIf Gaussian errors, at $\\beta=\\beta_{min}$, $\\chi^2 \\sim \\chi^2_{N-k}$.\n> $\\chi^2$ r.v. = sum of squared Gaussian r.v.\nModel check: $E(\\chi^2_{N-k}) = N-k$\n> (assumption true)\n$$ \\frac{\\chi^2}{N-k} \\approx 1 \\quad \\text{(for large } N-k) $$\n> (compare)\n\n> a sketch, should have a $\\chi^2$ distribution with N-k deg of freedom if gaussian errors?\n> what is $\\chi^2$ statistic: a way to test model fit\n\n---\n(Page 40)\n\nThese are special cases of **generalized least squares**.\nLinear model $y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ij} + \\mathcal{E}_i \\quad, i=1, ..., N$ objects\n$$ Y = X\\beta + \\mathcal{E} $$\n$E[\\mathcal{E}_i]=0$, **correlated errors**\n$\\mathrm{Var}[\\mathcal{E}] = \\mathrm{Cov}[\\mathcal{E}, \\mathcal{E}^T] = W$ (known)\n\nMinimize w.r.t. $\\beta$\n$$ \\hat{\\text{RSS}} = (Y-X\\beta)^T W^{-1} (Y-X\\beta) $$\n$$ \\hat{\\beta}_{GLS} = (X^T W^{-1} X)^{-1} X^T W^{-1} Y $$\n$$ E[\\hat{\\beta}_{GLS}] = \\beta \\text{ (unbiased)} $$\n$$ \\mathrm{Var}[\\hat{\\beta}_{GLS}] = (X^T W X)^{-1} $$\n\nThey can also be thought of as **maximum likelihood** (assuming Gaussian errors).\n$$ Y = X\\beta + \\mathcal{E}, \\quad y \\sim N(X\\beta, W) $$\nMaximize w.r.t. $\\beta$\n$$ L(\\beta) = P(Y|\\beta, X) = N(Y|X\\beta, W) $$\n\n> This week's lecture note one of 'quick edits'\n> (read up of slides)\n\n---\n(Page 41)\n\n> minimum $\\chi^2$ estimation gives values of $\\theta$ that make $\\chi^2(\\theta)$ small as possible\n\n### $\\chi^2$ MINIMISATION (board) (vs max likelihood)\n> weighted least squares\n> errors not iid. (but still indep) -> weight according to the $\\sigma_i^2$ to not have the dataset get dragged by points w/ large $\\sigma_i$\n**Case: Variance unknown**\n$(x_i, y_i) \\quad i=1, ..., N$\n$y_i = f(x_i, \\theta) + \\mathcal{E}_i$, $\\mathrm{Var}(\\mathcal{E}_i) = \\sigma_i^2$ **known**\n$= a + bx + \\mathcal{E}_i$ (linear)\n$$ \\chi^2(\\theta) = \\sum_{i=1}^N \\frac{(y_i - f(x_i, \\theta))^2}{\\sigma_i^2} $$\n$$ \\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmin}} \\chi^2 $$\n\n### Relation to maximum likelihood:\n$y_i = f(x_i, \\theta) + \\mathcal{E}_i, \\quad \\mathcal{E}_i \\stackrel{\\text{iid}}{\\sim} N(0, \\sigma_i^2)$\n$P(y_i|x_i) = N(y_i|f(x_i, \\theta), \\sigma_i^2)$\n$$ L(\\theta) = \\prod_{i=1}^N P(y_i|x_i, \\theta) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma_i} e^{-\\frac{1}{2}(y_i - f(x_i, \\theta))^2/\\sigma_i^2} $$\n$$ -2\\ln L(\\theta) = \\sum_{i=1}^N \\ln(2\\pi\\sigma_i^2) + \\sum_{i=1}^N \\frac{(y_i-f(x_i, \\theta))^2}{\\sigma_i^2} $$\n$$ = \\sum_{i=1}^N \\ln(2\\pi\\sigma_i^2) + \\chi^2(\\theta) $$\n> $\\chi^2(\\theta)$ is const.\n> (min $\\chi^2(\\theta)$ is same as max likelihood)\n\n$\\rightarrow$ If $\\sigma_i$ **known**: $\\hat{\\theta}_{\\text{min}\\chi^2} = \\hat{\\theta}_{MLE}$\n$\\rightarrow$ If $\\sigma_i^2$ (or a component of it) **unknown** then **NOT TRUE**!\n> (do you get why, ML is used in GS?)\n\n---\n(Page 42)\n\n**Case: Variance component unknown**\n$Y_i = f(x_i, \\theta) + \\mathcal{E}_{int} + \\mathcal{E}_{m,i}$\n$\\mathcal{E}_{int} \\sim N(0, \\sigma_{int}^2)$ $\\leftarrow$ **unknown**\n$\\mathcal{E}_{m,i} \\sim N(0, \\sigma_{m,i}^2)$ $\\leftarrow$ **known**\n$$ \\chi^2(\\theta, \\sigma_{int}^2) = \\sum_{i=1}^N \\frac{(Y_i - f(x_i, \\theta))^2}{\\sigma_{int}^2 + \\sigma_{m,i}^2} $$\n\n**Problem**: $\\chi^2$ is minimized when $\\sigma_{int} \\rightarrow \\infty$!!\n\n### Max. likelihood:\n$P(Y_i|X_i, \\theta, \\sigma_{int}^2) = N(Y_i|f(x_i, \\theta), \\sigma_{int}^2 + \\sigma_{m,i}^2)$\n$$ L(\\theta, \\sigma_{int}^2) = \\prod_{i=1}^N [2\\pi(\\sigma_{int}^2 + \\sigma_{m,i}^2)]^{-1/2} e^{-\\frac{1}{2}\\frac{(Y_i - f(x_i, \\theta))^2}{\\sigma_{int}^2 + \\sigma_{m,i}^2}} $$\n$$ -2\\ln L(\\theta, \\sigma_{int}^2) = \\sum_{i=1}^N \\underbrace{\\frac{(Y_i - f(x_i, \\theta))^2}{\\sigma_{int}^2 + \\sigma_{m,i}^2}}_{\\textcircled{1}} + \\underbrace{\\ln[2\\pi(\\sigma_{int}^2 + \\sigma_{m,i}^2)]}_{\\textcircled{2}} $$\n\nWhen $\\sigma_{int} \\rightarrow$ large, $\\textcircled{1} \\rightarrow$ but $\\textcircled{2} \\uparrow$.\nMax likelihood $\\rightarrow$ **finite estimate** of $\\sigma_{int}$!\n> e.g. pop. dispersion + measurement, but we derive from first principles, this is just a potentially approximate method.\n\n**Lesson**: $\\chi^2$ is an **ad-hoc** prescription. Likelihood is **derived** from explicit modelling assumptions.\n\n> slides: OLS leads to regression dilution, when there are errors in x (really measurement errors), which causes slope to be shallower than true value aka regression dilution\n> (i.e. gives shallower slope if you have error on x data)\n\n---\n(Page 43)\n\n### Probabilistic Generative Modelling (slides)\n> $P(D|\\alpha, \\theta)$\n\n* **Forward model** comprises series of probabilistic steps describing conceptually how the observed data was generated from the parameters of interest.\n* Can introduce intermediate parameters/unobserved **latent variables** (e.g. true values corresponding to the observed data).\n> ... what data would be had if there was no measurement error.\n* From **forward model**, derive the sampling distribution\n  e.g. $P(D|\\theta) = \\int P(D|\\alpha)P(\\alpha|\\theta) d\\alpha$\n  > integrate out to leave params of interest\n* Using observed data D, draw inference from **likelihood function**\n  $L(\\theta) = P(D|\\theta)$\n* Or if Bayesian with prior $P(\\theta)$: sample posterior\n  $P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$\n\n> params: $\\theta$ want, $\\alpha$ nuisance\n> $\\int P(\\theta|D) = \\int [\\int P(\\theta, \\alpha|D) d\\alpha]$\n> no way: $\\rightarrow \\int P(D|\\theta, \\alpha) P(\\alpha|\\theta) d\\alpha \\checkmark$\n> show as before from $\\int P(D|\\theta) = \\int P(D,\\alpha|\\theta)d\\alpha$\n\n---\n(Page 44)\n\n> (how do I come up with model for this complex situation where I have measurement error in my x and y data?)\n\n### Generative Model (slides)\n> (same example from L3)\n\n- **Population distribution** $\\hat{\\xi} \\sim N(\\mu, \\tau^2)$\n- **Regression Model** $\\eta_i|\\xi_i \\sim N(\\alpha+\\beta\\xi_i, \\sigma^2)$\n- **Measurement Error** $[x_i, y_i]|\\xi_i, \\eta_i \\sim N([\\xi_i, \\eta_i], \\Sigma)$\n\nfor this example let $\\Sigma = \\begin{pmatrix} \\sigma_{xi}^2 & 0 \\\\ 0 & \\sigma_{yi}^2 \\end{pmatrix}$ (for simplicity)\n\n$x_i = \\xi_i + \\mathcal{E}_{xi} \\leftarrow \\sigma_{xi}^2$\n$y_i = \\eta_i + \\mathcal{E}_{yi} \\leftarrow \\sigma_{yi}^2$\n> probabilistic generative model for linear regression.\n\n- Population dist. indep. variable $\\psi = (\\mu, \\tau)$\n- Regression Parameters $\\theta = (\\alpha, \\beta, \\sigma^2)$\n- Latent (true) variables (without error) $(\\xi_i, \\eta_i)$\n- Observed data with measurement uncertainties $(x_i, y_i)$\n  $\\rightarrow \\Sigma_i = \\begin{pmatrix} \\sigma_{xi}^2 & 0 \\\\ 0 & \\sigma_{yi}^2 \\end{pmatrix}$\n\n> Now can generate observed data from latent variables. (see slides -> same as L3).\n> (Now I've introduced all these new parameters how do I get rid of them?)\n> derive this! sheet 2. (and on slides) (most general form)\n### Formulating likelihood function: Marginalising latent variables\n\n$$ P(x_i, y_i|\\theta, \\psi) = \\iint P(x_i, y_i, \\xi_i, \\eta_i|\\theta, \\psi) d\\xi_i d\\eta_i $$\n> \"observed data likelihood\" $\\uparrow$\n> \"complete data likelihood\" $\\uparrow$\n$$ P(x_i, y_i|\\theta, \\psi) = \\iint \\underbrace{P(x_i, y_i|\\xi_i, \\eta_i)}_{\\text{measurement error}} \\underbrace{P(\\eta_i|\\xi_i, \\theta)}_{\\text{regression}} \\underbrace{P(\\xi_i|\\psi)}_{\\substack{\\text{population} \\\\ \\text{distribution of} \\\\ \\text{covariate}}} d\\xi_i d\\eta_i $$\n\n---\n(Page 45)\n\n## Lecture 11\n_17.2.25_\n\n> (slides)\n> outcome from previous, multiply marginal likelihood to get likelihood for all data.\n### Observed data likelihood:\n$$ P(x, y|\\theta, \\psi) = \\prod_{i=1}^N P(x_i, y_i|\\theta, \\psi) \\quad (\\text{indep.}) $$\n\nIn **frequentist statistics**: distinction between data and parameters: parameters are **fixed** and **unknown**, but not \"random\". Only \"data\" are random realisations of random variables.\n\n### What is the nature of the latent variables $\\xi_i, \\eta_i$?\n- They have distribution $(\\xi_i, \\eta_i) \\sim P(\\xi_i, \\eta_i|\\theta, \\psi) = P(\\eta_i|\\xi_i, \\theta)P(\\xi_i|\\psi)$\n- Often called **\"nuisance parameters\"** $\\rightarrow$ needed to complete the model but **not** the parameters of interest $(\\theta, \\psi)$.\n- Are the latent variables \"data\" or \"parameters\"?\n  > \"missing data\"\n  > \"nuisance parameters\"\n\n### BAYESIAN VIEWPOINT\n* There is a **symmetry** between data D and parameters $\\theta$ - both are r.v.s described by probability distributions.\n* Actually they are described by a **joint probability** $P(D, \\theta)$.\n\n---\n(Page 46)\n\n> (slides)\n\n* Data are r.v.s whose realisations are **observed**, parameters are r.v.s **not observed**.\n* Goal is to **infer** the **unobserved parameters** from the **observed data** using the rules of probability.\n  $$ P(\\theta|D) = \\frac{P(D, \\theta)}{P(D)} \\quad \\text{CONDITIONAL PROBABILITY} $$\n  $$ P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)} \\quad \\text{BAYES' THEOREM} $$\n* Probability interpreted as **degree of belief** / uncertainty in hypothesis.\n  > how sure are you in the values of that parameter?\n\n### Bayes' Theorem\n* **Joint probability of data & parameters**: $P(D, \\theta) = P(D|\\theta)P(\\theta) = P(\\theta|D)P(D)$\n* **Probability of parameters given data**:\n  $$ \\underbrace{P(\\theta|D)}_{\\substack{\\text{posterior prob.} \\\\ \\text{degree of belief}}} = \\frac{\\overbrace{P(D|\\theta)}^{\\substack{\\text{likelihood} \\\\ \\text{sampling distribution}}} \\overbrace{P(\\theta)}^{\\substack{\\text{prior prob.} \\\\ \\text{degree of belief}}}}{\\underbrace{P(D)}_{\\substack{\\text{normalisation} \\\\ \\text{constant}}}} $$\n\n> example: Bayesian inference of the dimensionality of space time. (see lecture)\n> object: light & electromagnetic parameter, signal from black hole merger (?)\n> test: OK, these seem both to infer dimensionality of spacetime\n> dimensionality... gravitational waves, long reach of waves, depends on how long light took to reach us look so strong in...\n\n---\n(Page 47)\n\n### Simple Gaussian Example (slides)\n\n### Frequentist Confidence vs. Bayesian Credible intervals\n\n1. **Frequentist Confidence Interval**\n   $Y_1, ..., Y_4 \\stackrel{\\text{iid}}{\\sim} N(\\mu, 1)$\n   Sampling dist. of statistic $\\bar{Y} \\sim N(\\mu, \\sigma^2/4)$\n   > (sample mean)\n   $[\\bar{Y} - \\sigma_{\\bar{Y}}, \\bar{Y} + \\sigma_{\\bar{Y}}]$ is a 68% **confidence interval**.\n   * Under **repeated experiments**, 68% of the (random) confidence intervals constructed this way will **contain** (cover) $\\mu$.\n   * This **does NOT mean** that the probability is 68% that $\\mu$ lies within the interval $(y_{obs})$.\n   > frequentist view is: $\\mu$ is fixed number, it does not have prob dist. interval will either cover that number or it doesn't.\n\n2. **Bayesian credible Interval** (board)\n   assume **flat prior**: $P(\\mu) \\propto 1$\n   > every value of $\\mu$ equally probable\n   > not normalizable from $-\\infty$ to $+\\infty$ so say $\\propto 1$ (?)\n   > \"improper prior\"\n   can derive posterior $P(\\mu|Y=y_{obs})$:\n   $Y_i \\stackrel{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)$\n   > unknown $\\mu$\n   > known $\\sigma^2$\n   Likelihood: $P(\\vec{y}|\\mu, \\sigma^2) = \\prod_{i=1}^N N(Y_i; \\mu, \\sigma^2)$\n   $$ = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(y_i - \\mu)^2/\\sigma^2} $$\n   $$ = (2\\pi\\sigma^2)^{-N/2} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i - \\mu)^2} $$\n\n---\n(Page 48)\n\nLikelihood $P(\\vec{y}|\\mu)$ only depends on data $\\vec{y}$ through the **sufficient statistics** $\\bar{y}$ and $S^2$. Any two datasets $\\vec{y}$ and $\\vec{y}'$ with same sufficient statistics $\\rightarrow$ model gives the same inference.\n> likelihood principle: all info about parameters from data is located in likelihood function\n$$ \\sum (y_i - \\mu)^2 = \\sum (y_i - \\bar{y} + \\bar{y} - \\mu)^2 = \\sum[(y_i - \\bar{y})^2 + 2(y_i - \\bar{y})(\\bar{y} - \\mu) + (\\bar{y} - \\mu)^2] = \\sum(y_i - \\bar{y})^2 + N(\\bar{y} - \\mu)^2 $$\n$$ P(\\vec{y}|\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} e^{\\frac{-(N-1)S^2}{2\\sigma^2}} e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} $$\nwhere\n$$ \\left. \\begin{array}{l} \\bar{y} = \\frac{1}{N}\\sum Y_i \\rightarrow \\text{sample mean} \\\\ S^2 = \\frac{1}{N-1}\\sum (y_i - \\bar{y})^2 \\rightarrow \\text{sample variance} \\end{array} \\right\\} \\begin{array}{l} \\textbf{sufficient} \\\\ \\textbf{statistics} \\end{array} $$\n> (unbiased) estimator for $\\sigma^2$ is $S^2$\n\n**Case 1**: $\\sigma^2=1$ is known, $P(\\mu) \\propto 1$\n> improper prior\nBayes' theorem $P(\\mu|\\vec{y}) \\propto P(\\vec{y}|\\mu)P(\\mu)$\n(ignore constant w.r.t $\\mu$)\n$$ P(\\mu|\\vec{y}) \\propto e^{-N/2(\\bar{y}-\\mu)^2} \\quad \\text{(unnormalised posterior)} $$\n> notice looks like part of a gaussian, only part gives variance is so this must define probability dist. over $\\mu$\n> integral is over $\\mu$\ncould: $P(\\mu|\\vec{y}) = A e^{-N/2(\\bar{y}-\\mu)^2} \\rightarrow \\int d\\mu \\rightarrow \\text{Find A}$\n> require integral\nor: $P(\\mu|\\vec{y}) = \\frac{1}{\\sqrt{2\\pi(1/N)}} e^{-\\frac{1}{2(1/N)}(\\mu-\\bar{y})^2} = N(\\mu|\\bar{y}, \\sigma^2/N)$ **normalised posterior**\n> gaussian on $\\mu$, tells us what degree of belief we have in various values of $\\mu$\n\n[Image of a Gaussian posterior distribution for $\\mu$. The x-axis is $\\mu$, the y-axis is $P(\\mu|\\vec{y})$. The peak is at $\\bar{y}$. The region between $\\bar{y}-\\sigma/\\sqrt{N}$ and $\\bar{y}+\\sigma/\\sqrt{N}$ is shaded, labeled \"68% posterior probability\" and \"credible interval\".]\n> integrate within limits gives prob\n> 68% prob $\\mu$ lies between $\\bar{y} \\pm \\sigma/\\sqrt{N}$\n\n> see slide: frequentist vs. bayes for more detailed comparison\n> written overlay.\n\n---\n(Page 49)\n\n> Reading: Sivia Ch 1-3 Gelman \"Bayesian data analysis\" F&B 3.8 Ivezic 5\n\n## Lecture 12 (slides)\n_19.2.25_\n\n### Frequentist vs. Bayes\n* **Frequentists** make statements about the **data** (or statistics or estimators = functions of the data) conditional on the **parameter** $P(D|\\theta)$ or $P(g(D)|\\theta)$.\n* Often goal is to get a **\"point estimate\"** or **confidence intervals** with good properties under repeated experiments $\\rightarrow$ **(\"long run\")**.\n* Arguments are based on datasets that **could've happened but didn't** e.g. null hypothesis testing.\n* **Bayesians** make statements about the **probability of parameters** conditional on the **dataset** $D=D_{obs}$ that you **actually observed**: $P(\\theta|D=D_{obs})$.\n* This requires an interpretation of the probability as quantifying a **\"degree of belief\"** in a hypothesis.\n* **Bayesian answer *is* the full posterior density** $P(\\theta|D=D_{obs})$, quantifying the **\"state of knowledge\"** after seeing the data. Any numerical estimates are attempts to (imperfectly) summarise the posterior.\n\n---\n(Page 50)",
      "timestamp": "2025-10-31T15:17:19.781351",
      "tokens": 12099,
      "cost": 0.087565625
    },
    {
      "role": "user",
      "content": "Continue transcribing the astrostatistics lecture notes. Transcribe pages 51-75 in the exact same markdown style as before. \n\nStart with page 51 and maintain:\n- LaTeX math notation in `$...$` or `$$...$$`\n- Page markers with `---` and `(Page N)`\n- Lecture headers with `## Lecture N` and `_date_`\n- Detailed diagram descriptions in `[Image...]`\n- Marginal notes with `>`\n- Bold and italics for emphasis\n\nContinue from page 51.",
      "timestamp": "2025-10-31T15:20:17.442880",
      "tokens": 55530,
      "cost": 0.08965624999999999
    },
    {
      "role": "assistant",
      "content": "### Bayes Advantages (slides)\n\n*   **Ability to include prior information** $P(\\theta)$\n    -   Incorporate info. from external datasets\n        $P(\\theta|D)$ is the posterior from some other data $P(\\theta|D_{ext})$\n        > e.g. cosmology, different datasets give different ways to probe cosmological parameters. can cascade external posterior from one analysis to the prior of another. One parameter often has multiple observables in cosmology.\n    -   **Regularisation**: penalises overfitting with complex model. e.g. gaussian process prior\n        > psychological link to regularisation\n        > e.g. linear regression $\\rightarrow$ ridge regression is linear regression with a penalty term; in bayesian land, can incorporate with analysis. e.g. can you say, I don't think large slopes will occur and can incorporate that into prior. Can also penalise complex models - Occam's razor.\n    -   **\"Non-informative\"/weakly informative/default priors** when you don't have much prior information.\n        > e.g. wide gaussian, triangle(?), flatlet\n        > e.g. might only be able to guess param is between 0 and 1, but still information! -> cannot have prior without some information, but reminder: definition of likelihood, the source is that you're plugged in observed dataset and are viewing that as a fn. of the parameters is not a probability density in the parameters. But multiply by a prior (even flat), and the (normalised) posterior is a probability density; conditional/marginal probabilities can be computed.\n\n*   **Likelihood** (you're plugged in observed dataset and are viewing that as a fn. of the parameters) is **not a probability density in the parameters**. But multiply by a prior (even flat), and the (normalised) posterior is a probability density; conditional/marginal probabilities can be computed.\n    > (e.g. in multi-parameter cases, useful to derive conditional densities (example later)).\n\n*   **Ability** to deal with high dimensional parameter space e.g. latent variables or nuisance parameters and marginalize them out (analytically/numerically)\n    > e.g. posterior on MLE is called maximum a-posteriori, can find peak and just use that as your point estimate.\n\n*   **Note**: Bayesian inference **not necessarily** completely opposed to frequentist statistics. Estimators derived from Bayesian arguments can still be evaluated in a frequentist basis (e.g. James-Stein estimators)\n    > a frequentist may use as part of their estimate (and generate estimation)\n    > original could derive intervals as intervals that contain some posterior prob 51 (?)\n\n---\n(Page 51)\n\n> Last time: gaussian example of bayesian inference (IID, flat prior $\\rightarrow$ like \"hello world\" of bayesian inference)\n> now slightly more involved example\n\nrecap:\n### Simple Gaussian Example w/ conjugate prior (board)\n$Y_i \\stackrel{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)$ ($\\mu$ unknown, $\\sigma^2$ known)\nLikelihood: $P(\\vec{y}|\\mu, \\sigma^2) = \\prod_{i=1}^N N(y_i; \\mu, \\sigma^2) = \\dots$\n$$ = (2\\pi\\sigma^2)^{-N/2} e^{-\\frac{(N-1)S^2}{2\\sigma^2}} e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} $$\n\nDefined sufficient statistics:\n> all info in data is contained in the likelihood & likelihood depends on sufficient statistics (e.g. if two different $\\vec{y}$ vectors with same s.s. but different ordering, posterior from $\\vec{y}$ vector is different)\n$\\bar{y} = \\frac{1}{N}\\sum_i y_i \\quad$ \"sample mean\"\n$S^2 = \\frac{1}{N-1}\\sum_i(y_i - \\bar{y})^2 \\quad$ \"sample variance\"\n\n**Conjugate Prior**: $P(\\mu) = N(\\mu|\\mu_0, \\tau_0^2)$\n(equivalently can write $\\mu \\sim N(\\mu_0, \\tau_0^2)$) $\\leftarrow$ prior mean & prior variance\n$$ P(\\mu) = \\frac{1}{\\tau_0\\sqrt{2\\pi}} e^{-\\frac{1}{2\\tau_0^2}(\\mu-\\mu_0)^2} $$\n\n**Posterior**: $P(\\mu|\\vec{y}) \\propto P(\\vec{y}|\\mu)P(\\mu)$\n> Only dep. on $\\mu$ is last term\n$$ \\propto e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} e^{-\\frac{1}{2\\tau_0^2}(\\mu-\\mu_0)^2} $$\n> note: this is a quadratic in $\\mu$ (exp of a quadratic in $\\mu$ must be a gaussian?) so we expect to get a gaussian.\n> will get messy if you multiply, expand, then complete the square. but can do that\n[warm up ex! - product of gaussian densities]\n$$ P(\\mu|\\vec{y}) = N(\\mu|\\mu_N, \\tau_N^2) \\leftarrow \\substack{\\text{posterior} \\\\ \\text{mean}} \\substack{\\text{posterior} \\\\ \\text{variance}} $$\n\n---\n(Page 52)\n\n> (from eg sheet 1)\n**reminder**: when you have product of gaussian densities, the precision = variance of the resulting gaussian = sum of precision of individual gaussians\n$$ \\left[\\frac{1}{\\tau_N^2} = \\frac{1}{\\tau_0^2} + \\frac{N}{\\sigma^2}\\right] \\quad \\text{POSTERIOR PRECISION} $$\n> prob means $N(\\mu, \\sigma^2_1)$ add them. multiply pdfs $N(\\mu, \\sigma^2_1)\\sigma^2_2 / \\sigma^2_1+\\sigma^2_2$ etc\n**Posterior precision** = sum of precisions from likelihood + prior.\n\n**reminder**:\n$$ \\left[\\mu_N = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{N}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2} + \\frac{N}{\\sigma^2}}\\right] \\quad \\text{POSTERIOR MEAN} $$\n(divide by weights to get mean)\n**Posterior mean** = precision weighted average of means from likelihood & prior.\n> this gives you a nice interpretation, tells us how our prior & likelihood are combined to give us the bayesian inference.\n> send $\\tau_0 \\rightarrow \\infty$, flat prior, you recover previous example\n\nFor fixed prior $\\tau_0$, as $N \\rightarrow \\text{large}$\n> (if only) $N(\\mu|\\bar{y} ...)$\n$\\mu_N \\rightarrow \\bar{y}$\n$\\tau_N^2 \\rightarrow \\sigma^2/N$\n(Data $N \\rightarrow$ large, likelihood dominates over prior.)\n> basically after a certain number of data points, likelihood will interpret.\n\n**Side note**: what is conjugate prior?\nFor a given likelihood, if you combine it with a conjugate prior coming from \"nice\" distribution, then the posterior is guaranteed to come from the same class of \"nice\" distributions. (e.g. gaussian, wishart, laplace)\n\n---\n(Page 53)\n\n> PAPER: C. Bailer-Jones, Estimating distances from parallaxes\n### Astrophysics Example (slide)\n### BAYESIAN INFERENCE FOR PARALLAX\n- parallax is a way to measure distances\n  [Image of parallax effect: Earth is shown at two points in its orbit around the Sun. A nearby star appears to shift its position against the background of distant stars. The parallax angle $\\omega$ is half the total shift. The distance is r.]\n  $\\omega_{true} = \\frac{\\text{parsec}}{\\text{arcsec}} = \\frac{1}{r}$ (in units)\n- Gaia satellite makes parallax measurement to measure distances & map stars in our galaxy.\n- measurement error in parallax angle. measured parallax is different to true parallax.\n  $P(\\hat{\\omega}|r) = \\frac{1}{\\sigma_{\\omega}\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2\\sigma_{\\omega}^2}(\\hat{\\omega} - \\frac{1}{r})^2\\right] \\quad \\sigma_{\\omega} > 0$\n  measured parallax has some distribution: assume gaussian distributed for simplicity.\n- fractional measurement error $f = \\sigma_{\\omega}/\\hat{\\omega}$\n  [Image of two Gaussian distributions on an x-axis labeled $\\omega$. The left peak is taller and narrower, labeled $\\sigma_{\\omega}=0.02, f=0.2$, \"more precise instrument\". The right peak is wider and shorter, labeled $\\sigma_{\\omega}=0.05, f=0.5$, \"less precise instrument\".]\n  > if star at large distance, actual measurement error could be very large fraction of distance\n- possible to get negative parallax due to measurement error.\n  [Image of two likelihood curves on an x-axis labeled 'r'. A solid blue line, labeled \"true likelihood\", peaks and goes to zero at r=0. A dashed purple line, labeled \"approx likelihood\", is shown for comparison. The y-axis is labeled $P(\\hat{\\omega}|r)$.]\n  > true likelihood, approx likelihood, get skewed dist of r's\n\n---\n(Page 54)\n\n- Laplace approximation gives gaussian, incidentally this is mathematically equivalent to doing propagation of error (assume gaussian measurement error propagated to $r=...$)\n- this is an approximation. (dotted vs solid line)?\n  [Image of several likelihood curves for different values of $\\omega$. The x-axis is r, y-axis is $P(\\hat{\\omega}|r)$. Curves for $\\omega=0.1, 0.05, 0.01, -0.01$ are shown. For positive $\\omega$, the curves peak at some positive r. For $\\omega=-0.01$, the curve is a monotonically decreasing function from r=0.]\n  > utility function\n  > likelihood for various values of position\n  > (plotting likelihood fn over positive r gives similar result)\n- likelihood is positive on negative values of distance (unphysical) - see all values of mapping from $\\omega \\rightarrow r$\n- negative measurements have no mode (MLE).\n  > We peak - most likely peak for the but not for -ve.\n\nThey try to fix this:\n- impose our knowledge that we know distances should be positive.\n  $P_{pos}(r|\\hat{\\omega}) \\propto P(\\hat{\\omega}|r)p(r)$\n  $p(r) = \\begin{cases} 1 & r>0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n  > note: improper distribution $\\rightarrow$ not normalisable\n\n- this cuts off -ve values of r in our graph.\n  [Left image: Three posterior curves $p(r|\\hat{\\omega})$ for $\\omega=0.01$ and different values of $f=\\sigma_{\\omega}/\\omega = 1, 0.5, 0.2$. The curves peak at positive r. Right image: a single posterior curve for $\\omega=-0.01, \\sigma=0.02$, showing a monotonically decreasing function from r=0.]\n  > unnormalised posterior\n  > for -ve parallaxes, not getting a mode at all -> no mode\n\n- improper posterior: not-normalisable, no mean, variance etc.\n- mode $(r>0)$ exists for +ve $\\omega$, but undefined for -ve $\\omega$ ($r=0$).\n> still have some problems\n\n---\n(Page 55)\n\n- impose a proper distance prior (impose limit for max distance of a star in our survey)\n  $p(r) = \\begin{cases} 1/r_{lim} & 0<r \\le r_{lim} \\\\ 0 & \\text{otherwise} \\end{cases}$\n  $p(r|\\hat{\\omega}) \\propto p(\\hat{\\omega}|r)p(r)$\n  unnormalised posterior $p^*(r|\\hat{\\omega}, \\sigma_{\\omega}) = \\begin{cases} 1/r_{lim} P(\\hat{\\omega}|r, \\sigma_{\\omega}) & 0<r \\le r_{lim} \\\\ 0 & \\text{otherwise} \\end{cases}$\n  [Image of three posterior curves for $r_{lim}=100$. Curves for $\\omega=0.1, 0.05, 0.025$ are shown. They are truncated at $r=100$.]\n  > 100 cut off at r_lim\n- implies unrealistic stellar density $\\sim 1/r^2$\n- cutoff of mode at edge $r_{lim}$ for small or negative measured parallax $(\\omega<0)$ (but better than going to infinity)\n  rest $= \\begin{cases} \\frac{1}{\\omega} & 0 < \\frac{1}{\\omega} \\le r_{lim} \\\\ r_{lim} & \\frac{1}{\\omega} \\ge r_{lim} \\\\ r_{lim} & \\omega \\le 0 \\end{cases}$ (extreme mode)\n- still have some problems $\\rightarrow$ can go towards more astrophysically motivated priors.\n- problem w/ previous prior: volume density $p(r)$.\n  > unphysical\n  $P_r(\\text{distance} \\in [r, r+dr]) \\propto p(r)4\\pi r^2 dr$\n  $p(r) \\propto 1/r^2, r < r_{lim} \\Rightarrow p(r) \\propto \\text{const} \\quad r<r_{lim}$\n  $0 \\quad \\text{otherwise} \\quad 0 \\quad \\text{otherwise}$\n- more physical: copernican principle $p(r) \\propto \\text{const}, p(r) \\propto \\text{const} \\cdot 4\\pi r^2$\n  > rounded up, some principle as previous priors.\n\n---\n(Page 56)\n\n> see slides for reading refs\n## Lecture 13\n_21.2.25_\n- astrophysics example\nrecap: understanding how priors impact inference\n- keeps going as before: introduce more & more astrophysically motivated priors (see slides)\n\n### Conclude: Priors in Bayesian Inference\n*   **Priors** can be used to encode **background info.** / external knowledge about parameters.\n    -   weak mathematical constraints on physical parameters e.g. positivity of distances\n    -   astrophysical info. e.g. distributions of stars\n*   test sensitivity of your inference to the priors (is likelihood). Under various assumptions of the model.\n    > note: also say measurement error is gaussian, not just prior.\n    $P(\\theta|\\hat{\\omega}, I) \\leftarrow$ measured likelihood $p(r|\\omega) \\propto p(\\omega|r)p(r) \\leftarrow$ prior, posterior\n\n---\n(Page 57)\n\n### MULTIPARAMETER BAYESIAN MODELS (slides)\nExample: Gelman BDA 3.2, sheet 2\n> analyze posterior for gaussian model with non-informative prior\ndata generating process: $y_i \\stackrel{\\text{iid}}{\\sim} N(\\mu, \\sigma^2) \\quad i=1,...,n$\nlikelihood: $p(\\vec{y}|\\mu, \\sigma^2) \\propto (\\sigma^2)^{-n/2}\\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\\right)$\n> sufficient statistics\n\n(non informative) $p(\\mu) \\propto 1$\nimproper prior: $P(\\log\\sigma^2) \\propto 1 \\text{ or } P(\\sigma^2) \\propto \\sigma^{-2} \\quad$ **JEFFREYS' PRIOR** $(\\sigma^2>0)$\n> equivalent to ordering mean $\\sigma$, $\\sigma^2$\n> even if prior is improper, need proper posterior (corresponds to real obs)\n\n**joint posterior**: $p(\\mu, \\sigma^2|\\vec{y}) \\propto p(\\vec{y}|\\mu, \\sigma^2) \\times p(\\mu, \\sigma^2)$\n$p(\\mu, \\sigma^2|\\vec{y}) \\propto (\\sigma^2)^{-n/2-1}\\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\\right)$\n> equivalent, can write $P(\\mu, \\sigma^2|\\vec{y}) = P(\\mu|\\sigma^2, y)P(\\sigma^2|y)$\n\njoint is useful to find conditional/marginals:\n**conditional posterior**: $p(\\mu|\\sigma^2, \\vec{y}) = N(\\mu|\\bar{y}, \\sigma^2/n)$\n> (ex. sheet 2)\n**marginal posteriors**: $p(\\sigma^2|\\vec{y}) = \\int p(\\mu, \\sigma^2|\\vec{y}) d\\mu = \\text{Inv-}\\chi^2(\\sigma^2|n-1, s^2)$\n> can derive these analytically so useful\n$$ p(\\mu|\\vec{y}) = \\int p(\\mu, \\sigma^2|\\vec{y}) d\\sigma^2 $$\n$$ \\propto \\left[1 + \\frac{n(\\mu-\\bar{y})^2}{(n-1)s^2}\\right]^{-n/2} $$\n$$ = t_{n-1}(\\mu|\\bar{y}, s^2/n) \\quad \\text{t distribution} $$\n\nNote: (1) **inverse chi-squared distribution**\n$\\text{Inv-}\\chi^2(\\theta|\\nu, s^2) \\propto \\theta^{-(\\nu/2+1)} \\exp\\left(-\\frac{\\nu s^2}{2\\theta}\\right), \\quad \\theta>0$\ndeg. of freedom $\\nu > 0$\nscale parameter $s > 0$\n[Image of an inverse chi-squared distribution, which is skewed to the right, starting at 0.]\n\n---\n(Page 58)\n\n(2) Same as **inverse-gamma**\n$\\text{Inv-gamma}(\\theta|\\alpha, \\beta) \\propto \\theta^{-(\\alpha+1)} \\exp(-\\beta/\\theta), \\quad \\theta>0$\nshape parameter $\\alpha = \\nu/2$\nscale parameter $\\beta = \\nu s^2/2$\n> useful, named distributions have random number generators in common python packages.\n\n(3) **Student's t distribution**\n$t_{\\nu}(\\theta|\\mu, \\sigma^2) \\propto \\left[1+\\frac{1}{\\nu}\\left(\\frac{\\theta-\\mu}{\\sigma}\\right)^2\\right]^{-(\\nu+1)/2}$\ndeg. of freedom $\\nu > 0$\nlocation parameter $\\mu$\nscale parameter $\\sigma > 0$\n> See distribution in book Gelman BDA appendix or just wikipedia for your examples sheet.\n[Image of a Student's t distribution, which looks similar to a Gaussian but with heavier tails.]\n\nWhat if you can't compute marginals/expectations analytically? $\\rightarrow$ **Bayesian Computation**\n*   **Bayesian answer**: full posterior $P(\\theta|D)$, numerical estimates are attempts to (imperfectly) summarise the posterior e.g. mean, mode.\n*   Often these are **posterior expectations** $E[f(\\theta)|D] = \\int f(\\theta)P(\\theta|D)d\\theta$ (computationally difficult)\n*   **Bayesian computation**: algorithms to \"map out\"/sample the posterior $P(\\theta|D)$ and compute expectations $E[f(\\theta|D)]$.\n*   e.g. MCMC, nested sampling, importance sampling\n    > all models are wrong, some are useful. even if your computation is right!\n\n---\n(Page 59)\n\n> object of computation is to calculate some integral (posterior expectations). can do with monte carlo integration.\n### MONTE CARLO INTEGRATION (slides)\nTypically we want to summarize the posterior & compute **expectations** of the form\n$$ I = E[f(\\theta)|D] = \\int f(\\theta)P(\\theta|D)d\\theta $$\nUsing $m$ samples from the posterior $\\theta_i \\sim P(\\theta|D)$\n$$ \\hat{I} = \\frac{1}{m} \\sum_{i=1}^m f(\\theta_i) \\rightarrow I \\quad (\\text{LLN for large N}) $$\n**Monte Carlo Error** (derive later, on next page)\n$\\mathrm{Var}[\\hat{I}] = \\frac{1}{m^2}\\sum_{i=1}^m \\mathrm{Var}[f(\\theta_i)] = \\frac{1}{m}\\mathrm{Var}[f(\\theta)] \\propto \\frac{1}{\\sqrt{m}} \\mathrm{Var}[E[f(\\theta)]]$\n> Monte carlo error $\\sim 1/\\sqrt{m}$ indep of dim $[\\theta]$ - is convenient\n\n**Fundamental Theorem of Monte Carlo** $\\rightarrow$ Bayesian computation using sampling\n$$ \\underbrace{E[f(\\theta)|D] = \\int f(\\theta) P(\\theta|D) d\\theta}_{\\text{posterior expectation}} \\approx \\underbrace{\\frac{1}{m} \\sum_i^m f(\\theta_i)}_{\\text{sample average}} $$\nE.g.\nPosterior mean $\\mu \\quad f(\\theta)=\\theta$\nPosterior variance $\\quad f(\\theta)=(\\theta-\\mu)^2$\nProbability in an interval $[a,b] \\quad f(\\theta) = I_{[a,b]}(\\theta)$\n> reporting this is Maximum?\n> **indicator function** = one when value between a & b.\n\n---\n(Page 60)\n\n### Monte Carlo Error (board)\nsuppose: $g$ is some invertible fn. of $\\theta$\n$\\theta_i \\stackrel{\\text{iid}}{\\sim} P(\\theta) \\quad i=1,...,m$\n> monte carlo sample size $m$ is not same as data sample size\n$\\rightarrow g(\\theta_i)$ are iid\n\nDef: $\\hat{I} = \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i)$\n$\\mathrm{Var}(\\hat{I}) = \\mathrm{Cov}(\\hat{I}, \\hat{I}) = \\mathrm{Cov}\\left[\\frac{1}{m}\\sum_i f(\\theta_i), \\frac{1}{m}\\sum_j f(\\theta_j)\\right]$\n$= \\frac{1}{m^2}\\sum_{i=j}\\sum_{j=1}^m \\mathrm{Cov}[f(\\theta_i), f(\\theta_j)]$\n> $\\theta_i, \\theta_j$ indep $\\rightarrow$ only non-zero contribution when i=j.\n$= \\frac{1}{m^2}\\sum_{i=1}^m \\mathrm{Var}(f(\\theta_i))$\n$= \\frac{1}{m}\\mathrm{Var}(f(\\theta_i))$\n> std dev of MC error $\\frac{\\sigma_f}{\\sqrt{m}} \\propto \\frac{1}{\\sqrt{m}}$\n$\\text{CLT}: m \\rightarrow \\text{large} \\quad \\hat{I} \\sim N(E[f(\\theta)], \\mathrm{Var}(f(\\theta))/m)$\n> slow convergence (sourcing of MC error)\n> but! $\\rightarrow$ nice: indep. of dim($\\theta$)!\n> so MC error rate is indep of dimensions, great!\n**approximate variance**\n$\\mathrm{Var}(\\hat{I}) \\approx \\frac{1}{m} \\widehat{\\mathrm{Var}}(\\{f(\\theta_i)\\})$\n> MC sample variance\n$$ \\widehat{\\mathrm{Var}}[\\{\\hat{f}(\\theta_i)\\}] = \\frac{1}{m-1}\\sum_{i=1}^m (f(\\theta_i)-\\hat{I})^2 $$\n> MC error scales as $m^{-1/2}$\n> slow but indep of dim($\\theta$)! an approx.\n\n---\n(Page 61)\n\n> note on useful papers of gaussians on moodle; also for ex sheet 2.\n## Lecture 14\n_24.2.25_\n\nToday: **Bayesian Computation, Direct sampling, Importance Sampling**\n\n### Monte carlo direct sampling (slides)\n> Example from before, a gaussian model with non-informative prior.\nHow does sample from joint? $\\rightarrow$ it factorises.\nFactorise posterior: $P(\\mu, \\sigma|y) = P(\\mu|\\sigma^2, y)P(\\sigma^2|y)$\n1. $\\sigma_i^2 \\sim P(\\sigma^2|y) \\quad \\text{Inv-}\\chi^2$\n2. $\\mu_i|\\sigma_i^2 \\sim P(\\mu|\\sigma_i^2, y) \\quad \\text{Normal}$\n   > $\\rightarrow (\\mu_i, \\sigma_i^2) \\sim p(\\mu, \\sigma^2|y)$\n   > joint-draw from posterior!\n[Image showing a scatter plot of samples from a 2D joint posterior of $\\mu$ and $\\sigma^2$. The x-axis is $\\mu$, y-axis is $\\sigma^2$. Histograms for the marginal distributions of $\\mu$ and $\\sigma^2$ are shown along the axes. Example values: $y=0, s^2=1, n=10$.]\n> compute posterior summary stats\n> e.g. $E[\\mu|y]=0.00$, $Std[\\mu|y]=0.14$, $E[\\sigma|y]=1.10$, $Std[\\sigma|y]=0.25$\n\n(did not need to solve analytically to get marginals)\n\n**Kernel density estimate** = estimate a smooth density from samples.\n[Image of a histogram with a smooth curve (KDE) drawn over it.]\n> can think of this as a \"smoothed histogram\"\n\n---\n(Page 62)\n\n### KERNEL DENSITY ESTIMATION (board)\n$\\theta_i \\stackrel{\\text{iid}}{\\sim} P(\\theta|D) \\quad i=1,...,m$\nApprox $P(\\theta|D)$ using $\\hat{P}(\\theta|D) \\approx \\frac{1}{m}\\sum_{i=1}^m N(\\theta|\\theta_i; bw^2)$\n$\\int \\hat{P}(\\theta|D) d\\theta = 1 \\quad \\checkmark$\n> nice optimal bandwidth\n> select bandwidth\n\nLots of ways to choose bandwidth, nice one is\n**Silverman's Rule of Thumb**:\n$bw = \\left(\\frac{4\\hat{\\sigma}^5}{3m}\\right)^{1/5} \\quad \\hat{\\sigma}^2 = \\mathrm{Var}[\\theta|D] \\approx \\widehat{\\mathrm{Var}}(\\{\\theta_i\\})$\n> sample variance $\\hat{\\mathrm{Var}}(\\{\\theta_i\\}) \\approx \\frac{1}{m-1}\\sum(\\theta_i-\\bar{\\theta})^2$\n\n### SUMMARISING POSTERIOR UNCERTAINTIES\n*   Can compute posterior mean (or median) $\\pm$ posterior standard deviation.\n*   If posterior $P(\\theta|D)$ is Gaussian, then this contains 68% posterior probability.\n*   But not necessarily for non-gaussian posterior!\n\n### CREDIBLE INTERVALS\n> in bayesian land talks about \"credible intervals\"\n> Prob $\\theta$ is in interval, don't confuse w/ freq. confidence interval (which is random)\n[Left image: An asymmetric posterior distribution $P(\\theta|D)$ with the 68% central credible interval marked. The interval is between the 16% and 84% quantiles, leaving 16% of the probability in each tail. Right image: The same distribution with a different 68% interval shown, shaded under the main peak.]\n> **NOT UNIQUE!**\n\n---\n(Page 63)\n\n### (HPD) HIGHEST POSTERIOR DENSITY CREDIBLE INTERVAL (board)\n- **Unique**\n- **Narrowest** interval that contains X% posterior probability\n  [Image of a unimodal posterior distribution $P(\\theta|\\text{D})$. A horizontal line at height $p$ intersects the curve at two points, defining an interval. All points inside the interval have a posterior density greater than $p$. Different values of $p$ ($p_1, p_2$) define different intervals.]\n  Find $p$ s.t. $[l(p), u(p)]$ contains X% posterior probability.\n\nHPD works for **multi-modal posteriors!**\n[Image of a bimodal posterior distribution $P(\\theta|D)$. A horizontal line at height $p$ intersects the curve in four places, defining two disconnected intervals. The total area of these intervals is the credible region.]\nX% HPD-C.I. = set $\\Theta$ (possibly disconnected) with highest $p$ s.t.\n$$ \\forall \\theta \\in \\Theta, P(\\theta|D) > p \\quad \\text{and} \\quad \\int_{\\Theta} P(\\theta|D) d\\theta = X $$\n\n---\n(Page 64)\n\n### [What if you can't directly sample the posterior $\\theta_i \\sim P(\\theta|D)$?] (board)\n$E[f(\\theta)|D] = \\int f(\\theta)P(\\theta|D)d\\theta \\approx \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i)$\ne.g. likelihood might be output of some compute program\n*   **Importance sampling**:\n    draw from an easier \"tractable\" distribution (importance function) $\\theta_i \\sim Q(\\theta)$ and weight the samples by $w_i = P(\\theta_i|D)/Q(\\theta_i)$ to compute expectations.\n*   **Posterior simulation**:\n    MCMC, Nested sampling etc. generates draws from the posterior density iteratively in long-run\n    $$ P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)} $$\n\n---\n(Page 65)\n\n### IMPORTANCE SAMPLING (board)\nGiven a probability density $P(\\theta)$ [e.g. the posterior]\nWant to estimate\n$$ I = E_P[f(\\theta)] = \\int f(\\theta)P(\\theta)d\\theta $$\nHowever, $P(\\theta)$ is **intractable**, i.e. difficult to directly sample, e.g. $\\theta_i \\sim P(\\theta)$, but can evaluate $P(\\theta_i)$ for a parameter value $\\theta_i$.\n\nSelect an **instrumental distribution** (importance function) from which it is easy to draw samples!\n$$ \\theta_i \\sim Q(\\theta) \\rightarrow \\text{tractable} $$\n> can easily draw/sample + evaluate\nand can evaluate $Q(\\theta_i)$, and $Q(\\theta)>0$ wherever $P(\\theta)>0$.\n\nRewrite $I = E_P[f(\\theta)] = \\int f(\\theta)\\frac{P(\\theta)}{Q(\\theta)}Q(\\theta)d\\theta$\n$$ = E_Q\\left[f(\\theta)\\frac{P(\\theta)}{Q(\\theta)}\\right] $$\nDraw samples $\\theta_i \\stackrel{\\text{iid}}{\\sim} Q(\\theta) \\quad (i=1,...,m)$\nApproximate $I$ with importance sampling estimate:\n$$ \\hat{I}^* = \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i)\\frac{P(\\theta_i)}{Q(\\theta_i)} = \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i)w_i^* $$\nwhere $w_i^* = w^*(\\theta_i) = \\text{importance weights} = \\frac{P(\\theta_i)}{Q(\\theta_i)}$\n\n---\n(Page 66)\n\n$E_Q[\\hat{I}^*] = \\frac{1}{m}\\sum_{i=1}^m E_Q[f(\\theta_i)w^*(\\theta_i)]$\n$= \\frac{1}{m}\\sum_{i=1}^m E_Q\\left[f(\\theta_i)\\frac{P(\\theta_i)}{Q(\\theta_i)}\\right]$\n$= \\frac{1}{m}\\sum_{i=1}^m \\int f(\\theta)\\frac{P(\\theta)}{Q(\\theta)}Q(\\theta)d\\theta$\n$= \\frac{1}{m}\\sum_{i=1}^m \\int f(\\theta)P(\\theta)d\\theta$\n$= \\frac{1}{m}\\sum_{i=1}^m E_P[f(\\theta)] = E_P[f(\\theta)]$\n$=I \\quad (\\textbf{unbiased})$\n> reminder: $\\mathrm{Var}(aX+bY) = a^2\\mathrm{Var}X+b^2\\mathrm{Var}Y$\n$\\mathrm{Var}_Q[\\hat{I}^*] = \\frac{1}{m^2}\\sum_{i=1}^m \\mathrm{Var}\\left(f(\\theta)w^*(\\theta)\\right)$\n$= \\frac{1}{m}\\mathrm{Var}\\left(f(\\theta)w^*(\\theta)\\right)$\n$\\approx \\frac{1}{m}\\widehat{\\mathrm{Var}}(\\{f(\\theta_i)w^*(\\theta_i)\\})$\n> $\\widehat{\\mathrm{Var}}[\\{Y_i\\}] = \\frac{1}{m-1}\\sum (y_i w_i^* - \\hat{I}^*)^2$\n\n---\n(Page 67)\n\n> useful papers & Gelman's book on Moodle for sheet 2\n## Lecture 15\n_26.2.25_\n\n**Today**: case study \"Importance sampling the mass of the milky way galaxy\"\n**Continuing**: Bayesian computation, importance sampling, MCMC (Gelman BDA ch10-12)\n\n> recap: MC integration, want to summarize posterior, estimate w/ samples from the posterior. replace integral with MC sum, which converges to desired expectation for large N. Error on estimate $\\propto 1/\\sqrt{N}$ and is indep of parameter space!\n\n### Importance sampling: recap (slides)\nObjective: compute expectation w.r.t. distribution $P(\\theta)$\n$$ I = E[f(\\theta)] = \\int f(\\theta)P(\\theta)d\\theta $$\nExample: Posterior (suppress \"|D\" here)\n$$ P(\\theta) = \\frac{L(\\theta)\\pi(\\theta)}{\\int L(\\theta)\\pi(\\theta)d\\theta} \\quad \\substack{\\text{likelihood} \\\\ \\text{prior}} $$\nCan evaluate $P(\\theta)$ but not sample from it directly. Choose importance function $Q(\\theta)$ you can evaluate and sample.\n> named distribution\n**Importance sampling estimate** $\\theta_1, ..., \\theta_m \\stackrel{\\text{iid}}{\\sim} Q(\\theta)$\n$$ \\hat{I}^* = \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i)w^*(\\theta_i) $$\n$$ w_i^* = w^*(\\theta_i) = P(\\theta_i)/Q(\\theta_i) $$\n\n---\n(Page 68)\n\n### SELF-NORMALISED IMPORTANCE SAMPLING (slides)\nObjective: compute expectation\n$$ I = E[f(\\theta)] = \\int f(\\theta)P(\\theta)d\\theta $$\nbut can only evaluate unnormalised $\\hat{P}(\\theta)$.\n\nExample: unnormalised posterior\n$$ \\hat{P}(\\theta) = L(\\theta)\\pi(\\theta). $$\n> evidence: hard to evaluate\nNormalised posterior is $P(\\theta) = \\frac{\\hat{P}(\\theta)}{Z_p} = \\frac{\\hat{P}(\\theta)}{\\int \\hat{P}(\\theta)d\\theta}$,\nbut, cannot easily calculate $Z_p = \\int L(\\theta)\\pi(\\theta)d\\theta$ so cannot evaluate $P(\\theta)$.\n\n---\n(Page 69)\n\n### Self-Normalised Importance Weighting (board)\nOften in Bayesian Analysis, you can only compute/evaluate posterior $P(\\theta)$ up to a constant.\n$P(\\theta) = \\frac{L(\\theta)\\pi(\\theta)}{Z_p} = \\frac{\\hat{P}(\\theta)}{Z_p} \\leftarrow$ unnormalised\nnormalised, $Z_p = \\int\\hat{P}(\\theta)d\\theta$\n\n**GOAL**: Estimate $I = E_P[f(\\theta)] = \\int f(\\theta)P(\\theta)d\\theta$\n$$ = \\int f(\\theta)\\frac{\\hat{P}(\\theta)}{Z_p} d\\theta = \\frac{\\int f(\\theta)\\hat{P}(\\theta)d\\theta}{\\int \\hat{P}(\\theta)d\\theta} $$\n$$ = \\frac{\\int f(\\theta)\\frac{\\hat{P}(\\theta)}{Q(\\theta)}Q(\\theta)d\\theta}{\\int \\frac{\\hat{P}(\\theta)}{Q(\\theta)}Q(\\theta)d\\theta} $$\n> if you can't solve a hard problem, solve an easy problem!\nChoose $Q$ s.t. drawing samples $\\theta_i \\stackrel{\\text{iid}}{\\sim} Q(\\theta)$ is easy $(i=1,...,m)$.\n> instrumental distribution, proposal distribution\nApproximate $I$ with\n$$ I \\approx \\hat{I} = \\frac{\\sum_{i=1}^m f(\\theta_i)\\hat{w}(\\theta_i)}{\\sum_{i=1}^m \\hat{w}(\\theta_i)} = \\sum_{i=1}^m f(\\theta_i)w(\\theta_i) $$\n> (FTMC)\n> (Estimate normalisation using same samples drawn from Q)\nwhere the **self-normalised weights** are,\n$$ w(\\theta_i) = \\frac{\\hat{w}(\\theta_i)}{\\sum_{i=1}^m \\hat{w}(\\theta_i)} = \\frac{\\hat{P}(\\theta_i)/Q(\\theta_i)}{\\sum_{i=1}^m \\hat{P}(\\theta_i)/Q(\\theta_i)} $$\n> nice, works, when $\\hat{P}$ is unnormalised, we have done the bottom integral by importance sampling itself.\n\n---\n(Page 70)\n\n(Also works if $Q$ is unnormalised, since const. factor in $Q$ cancel above).\n> whereas importance sampling we looked at before required normalised $Q$\n\n**COST**: $\\hat{I}$ is **slightly biased**\n$$ E[\\hat{I}] = I + O(1/m) $$\nb/c of estimating denominator (work out yourself)\n> biased but consistent? as $m \\rightarrow \\infty$, $\\hat{P}(Z) \\rightarrow Z$\n\n'bias arises from ill-domination' process\nreminder: $E[AB] = E[A]E[B] + \\mathrm{Cov}(A,B)$ since $\\mathrm{Cov}(A,B)=E[AB]-E[A]E[B]$\n'cheat' GET attempt:\n$E_Q[\\hat{I}] = \\sum_i E_Q[f(\\theta_i)w(\\theta_i)] = \\sum_i (E_Q[f(\\theta_i)]E_Q[w(\\theta_i)] + \\mathrm{Cov}(f(\\theta_i),w(\\theta_i)))$\nunclear steps:\n$E_Q[w(\\theta_i)] = \\frac{1}{m} + O(1/m^2)$ (and $\\mathrm{Cov}(f(\\theta_i), w(\\theta_i))$ is order $O(1/m^2)$?)\nbecause $\\sum_i w_i = 1$?\n$\\rightarrow E_Q[\\hat{I}] = \\sum_i (E_Q[f_i\\theta_i])(\\frac{1}{m} + O(\\frac{1}{m^2}))$\n$= E_Q[f(\\theta)] + O(1/m)$.\n> some kind of perturbation/taylor expansion?\n\n$$ \\mathrm{Var}[\\hat{I}] = \\sum_{i=1}^m \\mathrm{Var}[f(\\theta_i)w(\\theta_i)] = \\frac{1}{m}\\mathrm{Var}[f(\\theta_i)w(\\theta_i)] $$\n$$ \\approx \\frac{1}{m}\\widehat{\\mathrm{Var}}(\\{f(\\theta_i)w(\\theta_i)\\}) \\quad ? $$\n\n---\n(Page 71)\n\n### Contrived example: Gaussian Mixture, normalised PDF (slides)\nIMPORTANCE SAMPLING EXAMPLE (not self normalised)\n[Top image: plot of two distributions vs x. A bimodal \"target P(x)\" and a wider unimodal \"impt. fn. Q(x) - we choose gaussian\". Target mean = 0.20. (I have analytic soln but suppose we didn't have this & try to estimate)]\n**Generate 1000 samples from impt. fn.**\n[Middle image: Two plots vs x. Top one shows the importance weight curve. Bottom one shows a histogram of the 1000 draws from the importance function.]\nMC estimate of mean\n$E x_i w_i^* = 0.215 \\pm 0.028 \\quad$ (within $1\\sigma$ of target)\n$m=1000$ draws from impt. fn.\n> could also do this for posterior variance etc.\n$m=10,000$ draws: mc estimate of mean is\n$\\sum x_i w_i^* = 0.199 \\pm 0.009 \\quad$ (error does down as $1/\\sqrt{m}$)\n\n---\n(Page 72)\n\n### Parallax Example (slides)\n> (get both measurement error in $\\hat{\\omega}$?)\nLikelihood $\\omega \\sim N(1/r, \\sigma_{\\omega}^2)$\n$p(\\hat{\\omega}|r) = \\frac{1}{\\sigma_{\\omega}\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2\\sigma_{\\omega}^2}(\\hat{\\omega} - \\frac{1}{r})^2\\right], \\quad \\sigma_{\\omega}>0$\nTrue relation (no errors): $\\omega = \\frac{\\text{parsec}}{\\text{arcsec}} = \\frac{1}{r} \\leftarrow$ distance\n[Image of parallax effect. Earth orbits the sun. A nearby star is viewed against a distant background. The parallax angle $\\omega$ is defined.]\n> since $\\omega \\ll 1$, $\\tan\\omega \\approx \\omega$ to v. good approx. `parsec` is defined as the distance.\n\nIntroduce physically motivated prior\n$p(r) = \\begin{cases} \\frac{1}{2L^3}r^2 e^{-r/L} & \\text{if } r>0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n> as can argue some kind of ...\nExponential decrease in density of stars with galactic length scale $L$.\n\n$P(r|\\hat{\\omega}) \\propto P(\\hat{\\omega}|r)P(r)$\nUnnormalised posterior\n$P_{rear}^* (r|\\hat{\\omega}, \\sigma_{\\omega}) = \\begin{cases} \\frac{r^2 e^{-r/L}}{\\sigma_{\\omega}} \\exp\\left[-\\frac{1}{2\\sigma_{\\omega}^2}(\\hat{\\omega} - \\frac{1}{r})^2\\right] & r>0 \\\\ 0 & \\text{otherwise} \\end{cases}$\ncan't compute normalisation analytically.\n> (in reality, this is a 1D example so we could actually evaluate this on a grid and do integral numerically, that's fine. let's pretend we can't do that)\n\n---\n(Page 73)\n\nWant to compute posterior mean. consider posterior of distance\n[Top image: Two unnormalized posterior curves for $\\omega=-0.01$. One is the prior. One is the posterior, which looks similar to the prior. An unnormalized posterior for $f=0.33$ is also shown.]\n> what's the variety here?\n> unnormalised posterior $\\rightarrow$ short tail at low r.\n[Middle image: two posteriors. A bimodal one for $f=\\sigma_\\omega/\\hat{\\omega}=0.31$, low signal-to-noise. A unimodal one for $f=0.33, \\omega=0.01$.]\n> this is bimodal for $\\omega=0.001$ with error $f=0.33$ want to use this to use importance sampling to deal with this, choose impt fn.\n[Bottom image: the target posterior $p(r|\\hat{\\omega})$ for $\\omega=0.01, f=0.33$ and a chosen importance function $Q(r)$, a t-distribution.]\n> choose, say, student t-dist with $v=2$ (low deg of freedom). Choose because tail of student t is thicker than gaussian. (gaussian drops too fast, but student t will have greater probability mass than the target in the tails so we want a named dist)\n$M=10^4$ draws from $Q(r)$:\nMC estimate of mean\n$\\sum x_i w_i = 2391.263 \\pm 3.918$\n[Bottom two plots: importance weights vs distance r, and histogram of M=10000 draws from importance fn. vs distance r.]\n> Think of weights like adjusting histogram for good/bad...\n> From source: ... a conceptual introduction to MCMC\n> Imagine approximately integral over grid of points in 1D space, then integral is a sum over values of fn, each multiplied by a weight to reflect how much of space that grid point represents. ... In high-d space, we can't grid. So we need ... reject drawings with small weights/probability so we concentrate our work in regions that count most towards ... this leads to concept of \"bias\" sample weight otherwise. so small sample may be...\n> Idea of ESS: in high-d space, if weight non uniform and very variable, we get more effective info from each point. good IS has low variance in the weights.\n\n---\n(Page 74)\n\n### CHOOSING A GOOD IMPORTANCE FN. (slides)\n> most important part of importance sampling.\nWithin class of dists (sheet 2), that theoretically optimal (minimum variance) importance fn. is\n$$ Q^*(\\theta) = \\frac{|f(\\theta)|P(\\theta)}{\\int |f(\\theta)|P(\\theta) d\\theta} $$\n> high variance in importance weights $\\rightarrow$ high variance in estimates ($\\rightarrow$ small effective sample size)\n\n*   However, if we can't directly sample from $P(\\theta)$, then we probably can't sample from $|f(\\theta)|P(\\theta)$.\n*   Want to keep the importance weights roughly const., otherwise large variations in $P(\\theta)/Q(\\theta)$ will lead to high variance of estimate, smaller ESS.\n*   **Effective sample size**\n    $$ ESS = \\frac{m}{1+\\mathrm{Var}[\\mathcal{E}w^*(\\theta_i)]} $$\n    > importance sampling similar concept to \"re-sample\"\n    > e.g. as in parallax example\n*   In practice, find thick-tailed distribution $Q(\\theta)$ that is positive everywhere and similar in shape to $|f(\\theta)|P(\\theta)|$.\n*   Don't want $Q(\\theta)$ small when $|f(\\theta)|P(\\theta)|$ large!\n    > problem: small sample size, high variance.\n    > ideal importance fn is proportional to $|f(\\theta)|P(\\theta)|$\n    > want importance weights `const` $\\rightarrow$ small variance in weights $\\rightarrow$ large ESS. (think)\n    > in practice want positive, everywhere, covers the tail (thicker tail), and roughly resembles target $f(\\theta)P(\\theta)$ (named distribution).\n\n---\n(Page 75)",
      "timestamp": "2025-10-31T15:20:17.443649",
      "tokens": 10990,
      "cost": 0.08965624999999999
    },
    {
      "role": "user",
      "content": "Continue transcribing pages 76-100 of the astrostatistics lecture notes. Maintain the same markdown formatting style with LaTeX math, page markers, lecture headers, image descriptions, and marginal notes.",
      "timestamp": "2025-10-31T15:23:11.093358",
      "tokens": 66563,
      "cost": 0.09841687499999999
    },
    {
      "role": "assistant",
      "content": "### Intro (slides)\n> satellite galaxies\n> large magellanic cloud\n> milky way (our galaxy)\n\n### Astrostatistics case study\n**\"Bayesian estimates of the milky way and andromeda masses using high-precision astrometry & cosmological simulations\"**\n> astrometry, measurements of positions\n*   Want to measure mass of milky way (constrain mass of our galaxy)\n*   Satellite galaxies of our milky way (LMC) -> use their orbits/motions to estimate mass of our own galaxy\n*   measure distance of satellite galaxy to central galaxy, angular momentum etc.\n*   use simulation to plot \"kinematic properties\" against mass of host galaxy -> plot kinematic properties (from simulations)\n*   use bayesian inference to infer actual mass of our galaxy from measured kinematic properties & results of simulations\n> illustrate simulation: sample models of galaxies, logic, attract mass, galaxies form, attract gas, which form stars, galaxies\n*   can measure properties of satellite galaxies (no errors in theory) vs get catalogue of properties\n*   simulation generates samples (prior mass) p(angular mom...) -> intractable, in sense that we can't write it down but we can draw from it\n*   velocities (v), positions (r), momenta (J) of a satellite correlated with central galaxy mass. via galaxy formation simulators (physical prior).\n    $x \\equiv$ latent (true) values, $v, r, j$\n    $M_{vir} = \\text{mass of galaxy}$, $\\theta = \\text{params of } M_{vir}$\n    (joint prior $p(x, M_{vir})$ from simulations)\n*   measure LMC data, $v, r, j$. we have stochastic measurement! likelihood $p(\\hat{x}|x)$, now use importance sampling to estimate posterior!\n    > $L(M) = L(\\hat{x}|M) = N(V_{obs}|V, \\sigma_v^2) \\times N(J_{obs}|J, \\sigma_J^2) \\times N(V_{rad}|V_{rad}, \\sigma_r^2)$\n    > Bayesian inference\n    > Importance sampling\n    > kernel density estimation\n\n[Image of two elliptical orbits around a central point labeled LMC. One orbit is a solid line, the other is dashed. Text says \"trajectories depend on mass of milky way\". The central region is labeled \"MW disk plane\".]\n\n---\n(Page 76)\n\n### IMPORTANCE SAMPLING THE MILKY WAY GALAXY (Mald & HD) (board)\n\n$\\hat{\\vec{d}} = $ (**noisy**) **measurements** of **kinematic properties** of our Milky way satellite's (e.g. large Magellanic Cloud) properties. (e.g. distance from central galaxy, velocity, angular momentum...)\n$\\vec{d} = \\vec{v}, \\vec{r}, \\vec{j}$\n\n$\\vec{d} = $ latent (true) values of satellite properties\n$m = \\log_{10}[\\text{mass of central galaxy}]$\n\n**GOAL**: Estimate $E[f(m)|\\hat{\\vec{d}}] = \\int f(m)P(m|\\hat{\\vec{d}})dm$\n(e.g. let $f(m)=m$ for posterior mean or $f(m)=(m-E[m|\\hat{\\vec{d}}])^2$ for posterior variance)\n\nPosterior mean: $E[m|\\hat{\\vec{d}}] = \\int m P(m|\\hat{\\vec{d}})dm$\n> marginal posterior\n> full posterior\n$= \\int m \\left[ \\int P(\\vec{d}, m|\\hat{\\vec{d}})d\\vec{d} \\right] dm$\n(Gaussian) measurement likelihood\n$P(\\vec{d},m|\\hat{\\vec{d}}) = \\frac{P(\\hat{\\vec{d}}|\\vec{d},m)P(\\vec{d},m)}{\\iint P(\\hat{\\vec{d}}|\\vec{d},m)P(\\vec{d},m)d\\vec{d}dm}$\n> physical prior (encoded in cosmo sims)\n> why not $p(d|m)$? pragmatic assumption. if know m then kinematics of d may provide no additional info on m.\n\n$$ E[m|\\hat{\\vec{d}}] = \\frac{\\iint m P(\\hat{\\vec{d}}|\\vec{d})P(\\vec{d},m)d\\vec{d}dm}{\\iint P(\\hat{\\vec{d}}|\\vec{d})P(\\vec{d},m)d\\vec{d}dm} $$\n> (bottom is the normalisation integral again)\n\nCosmo. sims generate galactic systems that can be thought of as giving **u**samples $(\\vec{d}_j, m_j)$ $j=1,...,m$ from the physical prior (intractable) $P(\\vec{d}, m)$.\n> so we need $E(m|\\hat{d})$ so we need $P(m|\\hat{d})$ which we get from marginalising $P(d,m|\\hat{d})$.\n> we know $p(d,m)$ from sim, so need $p(\\hat{d}|d,m)$ and $p(\\hat{d}|d)$.\n> then integrate to get $p(m|\\hat{d})$ and integrate again to get $E(m|\\hat{d})$.\n\n---\n(Page 77)\n\n$\\rightarrow$ use the **prior itself** as the **importance function**!\n> we can get draws from the intractable prior, since they cancel?\n\nApply **Monte Carlo approx**:\n> if begin sims, we have samples from prior but not posterior\n$$ E[m|\\hat{\\vec{d}}] \\approx \\frac{\\frac{1}{m}\\sum_{j=1}^m m_j P(\\hat{\\vec{d}}|\\vec{d}_j)}{\\frac{1}{m}\\sum_{j=1}^m P(\\hat{\\vec{d}}|\\vec{d}_j)} = \\sum_{j=1}^m m_j w_j $$\n> now we only need few-point samples for $p(\\hat{d}|\\vec{d})$ (and don't need $p(d,m)$ at all!)\n\n(Repeat for posterior variance)\n> **self-norm importance weight**\n> $w_j = \\frac{P(\\hat{\\vec{d}}|\\vec{d}_j)}{\\sum_{k=1}^m P(\\hat{\\vec{d}}|\\vec{d}_k)}$\n> $\\sum_{j=1}^m w_j = 1$\n\n### REMARKS\n$P(\\hat{d}|d, m) = P(\\hat{d}|d)$\n**$\\hat{d}$ conditionally indep. of m given $d$**\n> conditional independence assumption. if I know the true value, no more information is contained in the model.\n> meaning distance of glory, a satellite galaxy does not provide info directly; the prior info on mass comes from arranging in the prior pmf.\n\n> reminder: by using instrumental posterior for Q, now... so what happens to that? It effectively cancels out because we used prior as Q.\n$$ w_j \\propto \\frac{P(\\hat{d}|d_j)P(d_j, m_j)}{Q(d_j, m_j)} $$\n> because we are using prior as importance function\n> we just get away with not being able to evaluate density itself because we can only get samples from (expensive distribution) since it cancels with the prior!\n\n---\n(Page 78)\n\n## Lecture 16\n_28.2.25_\n\nToday: continuing Bayesian computation, importance sampling, MCMC\nrecommend: paper Speagle J \"A conceptual Introduction to Markov chain Monte Carlo Methods\"\n> paper doesn't just do qc sampling, does importance sampling and all this.\n\n### WEIGHTED KERNEL DENSITY ESTIMATE (slides)\n$$ Wkde(\\theta) = \\sum_{s=1}^m W_s \\times N(\\theta|\\theta_s, bw^2) \\quad (W_s = \\frac{\\hat{P}(\\theta_s)/Q(\\theta_s)}{\\sum \\hat{P}(\\theta_s)/Q(\\theta_s)}) $$\n$W_s = $ normalised importance weights\n> comes from Q?\nbandwidth: Silverman's rule of thumb $bw = \\left(\\frac{4\\hat{\\sigma}^5}{3n}\\right)^{1/5}$ vs.\nEstimate $\\sigma^2$ from posterior $\\mathrm{Var}[\\theta]$ estimate.\nUse importance sampling **effective sample size (ESS)** for $n$.\n$$ ESS = \\frac{m}{1+\\mathrm{Var}_Q(w^*)}$$\n\n**Ideal case**:\nif equal weights: $w_i = 1/m$ reduces to\n$kde(\\theta) = \\sum_{s=1}^m \\frac{1}{m} \\times N(\\theta|\\theta_s, bw^2)$\n\n---\n(Page 79)\n\nContinue from example before:\n### Posterior of Milky Way w/ weighted KDE (slides)\n> Baurato\n> arxiv 17.. ?\n[Image of two probability distributions on an x-axis labeled $M_{vir}$. A wide, flat curve is labeled \"prior $P(M_{vir}, v, r, j)$\". A narrower, taller curve is labeled \"posterior $P(M_{vir}|d)$\". The posterior peak gives an estimate $MW M_{vir} = 1.7^{+1.33}_{-0.52} \\times 10^{12}$. The y-axis is labeled $P(M_{vir}|d)$. The x-axis ranges from $10^{11}$ to $10^{13}$.]\n> what about width? mean weight?\n\n### Posterior HPD\nX% HPD (X% credible interval(s)) with highest density containing X% of posterior)\n\n[Image of an asymmetric posterior distribution. The 40% HPD interval is marked as [11.83, 12.65].]\n[Image of the same distribution with the 68% HPD interval marked as [12.13, 12.42].]\n[Image of the same distribution showing the mean at 12.25 and the mode at 12.28.]\n\n> can make combining posterior from data from multiple satellite galaxies & do same calculation with weighted KDE\n> combine inferences from different sources to get more accurate estimate (this is an ideal case for Bayesian inference)\n\n---\n(Page 80)\n\n### MARKOV CHAIN MONTE CARLO (MCMC) (slides)\n> Fundamental: MCMC samples from posterior, which is in principle, the Bayesian answer. To summarize expectation, ...\n> Class of algorithms for sampling from some probability dist. useful when you dist. that are too complex or high dimensional to study with analytic techniques alone (not to say dimensionally).\nGoal is to **evaluate the posterior** $P(\\theta|D)$.\n\n*   Simple likelihoods/conjugate priors admit **analytic solutions to the posterior**.\n*   Simple models may allow direct draws $\\theta_i \\sim P(\\theta|D)$ i.e. \"direct simulation\".\n*   Small numbers of parameters p (small dimensionality) $\\rightarrow$ **Evaluate posterior on a p-dimensional grid**. Inefficient for $p > 3$ (end up wasting compute time evaluating $P(\\theta|D)$ where it is close to zero.\n*   Realistic models with many parameters p (**high dimensional parameter space**)\n    > (and we have no analytic solution)\n    $\\rightarrow$ **Markov Chain Monte Carlo is the gold standard!**\n\n**Whole point of MCMC** is to generate samples from the posterior when we can't directly sample from it.\n$$ E[f(\\theta)|D] = \\int f(\\theta)P(\\theta|D)d\\theta \\approx \\frac{1}{m}\\sum_{i=1}^m f(\\theta_i) $$\n*   **Posterior simulation - MCMC**\n*   **How?** - Generate a **correlated sequence (chain)** of random variates (Monte Carlo) that (in a limit) are draws from the posterior. The next value in the sequence only depends on the **current** values (Markov).\n    > (explain how later)\n*   Algorithm cleverly constructed to ensure distribution of chain values $\\rightarrow$ posterior dist. = stationary dist. in the long run.\n\n---\n(Page 81)\n\n### Simplest MCMC: METROPOLIS ALGORITHM (slides)\n> special case of symmetric proposal\n1.  Choose random starting point $\\mu_0$\n    > do w/ N_mc steps\n2.  At each step $i=1,...,N_{mc}$ propose a new parameter value $\\mu_{prop}$ via $J(\\mu)$. The proposal distribution $J(\\mu)$ is usually $N(\\mu_i, \\tau^2)$\n    > draw from proposal dist.\n    > say jump\n    > history matters\n    > breeding noise\n    The proposal scale $\\tau$ is chosen 'cleverly' for efficiency!\n3.  Evaluate **ratio of posteriors** at proposed vs. current values\n    **Metropolis Ratio** $r = \\frac{P(\\mu_{prop}|y)}{P(\\mu_i|y)}$\n4.  If $\\mu_{prop}$ is a better solution (higher posterior), $r>1$, **accept** the new value\n    $\\mu_{i+1} = \\mu_{prop}$\n    Else **accept with probability r** (i.e. accept with probability $\\min(r,1)$). **stay** at same value\n    $\\mu_{i+1} = \\mu_i$\n    (and include in chain - keep repeated values).\n5.  Repeat 2-4 until reach some measure of **convergence** and gather enough samples to **compute your inference**.\n\n> $\\mu_{i+1}$'s next value depends on current value\n> Monte carlo: randomness\n\n---\n(Page 82)\n\n### Example: simple gaussian unknown $\\mu$ (slides)\n> return to example w analytic solution\nLikelihood: $y_i \\sim N(\\mu, \\sigma^2=1) \\quad i=1,...,N$\nPrior: $P(\\mu) \\propto 1$\n$\\Rightarrow$ Posterior: $P(\\mu|y) = N(\\mu|\\bar{y}, \\sigma^2/N)$\n> (analytic solution) (L12)\n> Do the MCMC steps (proposal scale 0.6). Code on moodle so try yourself.\n> track no. of acceptances, trace summary statistics, average acceptance rate for the posterior.\n**1000 iterations metropolis algo.**\n[Left image: Two trace plots vs \"chain step\". Top plot shows $\\mu$, which oscillates around a mean value of 0. Bottom plot shows $\\log P(\\mu|y)$, which oscillates around a mean value below -100. Both plots show initial \"burn-in\" period. An \"acceptance ratio = 0.50\" is noted. Right image: A histogram of the MCMC samples labeled \"posterior histogram\". An overlaid curve is labeled \"analytic posterior\". The two match well. $N_{mc}=1000$.]\n\nDuring burn-in e.g. first 50 iterations, acceptance ratio = 0.42.\n> acceptance ratio goes up as we make small steps. what do we want for sampling?\nCan also **cut off 1st half of chain to cut burn in** (aggressive solution, more empirical cut-offs exist).\n\n[Image of a shorter trace plot for $\\mu$ and $\\log P(\\mu|y)$ up to 500 steps. Correspondingly, a histogram from these samples is shown.]\n> (really, if we built iid sampler, wouldn't need to do MCMC, we just use CDF, invert & sample it, easy, get draw from dist. but to show, here is example.)\n\n---\n(Page 83)\n\n## Lecture 17\n_3.3.25_\n> (joint of S, sheet 2 on moodle)\n\nToday: continuing Bayesian computation: MCMC\n\n### Recap MCMC:\n...clearly, want std dev. for long run, want to get through entire state space, this works\n...not iid (correlated), but looks like iid dist from large N_mc\nGenerate a **correlated sequence (chain)** of random variates (Monte Carlo) that (in a limit) are draws from the posterior. The next value in the sequence only depends on the current values (Markov).\n\n### D-DIM METROPOLIS ALGORITHM (slides)\nPosterior $P(\\theta|D)$ where $\\dim(\\theta)=d$\nSymmetric proposal/jump dist. $J(\\theta^*|\\theta) = J(\\theta|\\theta^*)$\n1.  Choose random starting point $\\theta_0$\n2.  At step $i=1,...,N$ propose new parameter value\n    $\\theta^* \\sim N(\\theta_{i-1}, \\Sigma_p)$\n    > note to self: what's this symmetric proposal thingy. using Normal is fine. why normal, to ensure metropolis is symmetric for instance?\n    The proposal distribution is\n    $J(\\theta^*|\\theta_{i-1}) = N(\\theta^*|\\theta_{i-1}, \\Sigma_p)$\n3.  Evaluate **ratio of posteriors** at proposed vs. current values\n    $r = \\frac{P(\\theta^*|y)}{P(\\theta_{i-1}|y)}$\n\n---\n(Page 84)\n\n4.  Accept $\\theta^*$ with probability\n    $\\min(r,1): \\theta_i = \\theta^*$\n    If not accept, stay at same value $\\theta_i = \\theta_{i-1}$ for the next step and include in chain.\n5.  Repeat 2-4 until reach some measure of convergence (G-R) and gather enough **independent** samples to compute your inference. (reduce Monte Carlo error).\n\n---\n(Page 85)\n\n> widely used way to sample from MVG: (see step 2 d-dim metropolis)\n### Multivariate Gaussian Draws (board)\n\n$\\vec{\\theta} \\in \\mathbb{R}^d$\n$\\vec{\\theta} \\sim N(\\vec{\\mu}, \\Sigma)$\nMetropolis proposal $\\vec{\\theta}^* \\sim N(\\vec{\\theta}_{i-1}, \\Sigma_p)$\n$\\Sigma_p = $ real, symmetric, positive definite $d \\times d$ matrix\n$\\det(\\Sigma_p)>0$ and $\\Sigma_p^{-1}$ exists.\n\n1.  **Cholesky decomposition**, find L s.t. $\\Sigma_p = L L^T$\n    > L is lower triangular Cholesky factor\n    > \"matrix square root\"\n    [Image of a lower triangular matrix, with non-zero elements on and below the diagonal, and zeros above.]\n2.  Draw $\\vec{\\theta}^* \\sim N(\\vec{\\theta}_{i-1}, \\Sigma_p)$\n    > cholesky decomp is expensive so dont want to do cholesky decomp at every point in your loop, so instead do this.\n    by $\\vec{\\theta}^* = \\vec{\\theta}_{i-1} + L\\vec{z}$\n    > (do this computationally w/ code)\n    where $\\vec{z}$ is a d-vector of iid **uniform** unit normal r.v.s.\n    $z_i \\stackrel{\\text{iid}}{\\sim} N(0,1) \\quad i=1,...,d$\n    > check this is sampling $\\theta^*$ like this is indeed the step of sampling from MVG.\n    > wiki $\\theta_i+Lz$ has the desired dist, due to the affine transformation of a gaussian is a gaussian.\n    **Check**: $E[\\vec{\\theta}^*] = \\vec{\\theta}_{i-1}$\n    $\\mathrm{Cov}[\\vec{\\theta}^*, \\vec{\\theta}^{*T}] = \\mathrm{Cov}[L\\vec{z}, (L\\vec{z})^T]$\n    > (bilinear)\n    $= L \\mathrm{Cov}[\\vec{z}, \\vec{z}^T] L^T = L I_d L^T = L L^T = \\Sigma_p$.\n> use to very summary gaussian dist did we get from cholesky?\n> 1. for real matrix A find cholesky L\n> 2. draw d-vector of iid unif norm\n> 3. and construct matrix\n\n---\n(Page 86)\n\n### Example: (slides)\nRecall: analytic posterior density for Gaussian $(\\mu, \\sigma^2)$ model. With non-informative prior $P(\\mu) \\propto 1, P(\\sigma^2) \\propto \\sigma^{-2}, \\sigma^2>0$. (multi-param. inference)\n\n*   **Joint posterior**: $(\\sigma^2 > 0)$\n    $P(\\mu, \\sigma^2|y) \\propto (\\sigma^2)^{-(n-1)/2 - 1} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\\right)$\n*   **Marginal of $\\mu$**:\n    $P(\\mu|y) = \\int P(\\mu, \\sigma^2|y)d\\sigma^2 \\propto \\left[1 + \\frac{n(\\mu-\\bar{y})^2}{(n-1)s^2}\\right]^{-n/2} = t_{n-1}(\\mu|\\bar{y}, s^2/n)$\n    [Image of a t-distribution for $\\mu$.]\n*   **Marginal of $\\sigma^2$**:\n    $P(\\sigma^2|y) = \\int P(\\mu, \\sigma^2|y)d\\mu = \\text{Inv-}\\chi^2(\\sigma^2|n-1, s^2)$\n    [Image of an Inv-chi-squared distribution for $\\sigma^2$.]\n*   **Joint posterior**\n    $P(\\mu, \\sigma^2|y) = P(\\mu|\\sigma^2, y)P(\\sigma^2|y) = N(\\mu|\\bar{y}, \\sigma^2/n) \\times \\text{Inv-}\\chi^2(\\sigma^2|n-1, s^2)$\n    [Image of concentric, elliptical contour plot of the joint posterior in the $(\\mu, \\sigma^2)$ plane.]\n    > (draw from this w/ metropolis)\nMetropolis 2D example\n[Left image: Two histograms labeled \"analytic\" and \"posterior histogram\" for $p(\\mu|y)$ and $p(\\sigma^2|y)$. The analytic curves and histograms match. Right image: A scatter plot of MCMC samples in the $(\\mu, \\sigma^2)$ plane. An arrow points from the joint posterior plot to this scatter plot. MC chain N_mc=2000, acc. ratio=0.39.]\n> Note on cov matrix of multi dim gaussians.\n> Reminder $\\vec{x} \\sim N(\\vec{\\mu}, \\Sigma)$, $\\vec{x} = (\\vec{x}_a, \\vec{x}_b)^T$, $\\Sigma = \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix}$. Think of components one by one to check this.\n> e.g. $\\Sigma_1 = \\mathrm{Cov}(x_i, x_i)$.\n> Think in components!\n> $\\mathrm{Cov}[\\theta_i, \\theta_j] = \\mathrm{Cov}[L_i \\vec{z}, L_j \\vec{z}] = L_i k \\mathrm{Cov}[\\vec{z}, \\vec{z}] k_j L_j = L_i k_j L_j$\n> $\\mathrm{Cov}[\\theta_i, \\theta_j] = \\mathrm{Cov}(\\sum L_{ik} z_k, \\sum L_{jl} z_l) = \\sum \\sum L_{ik} L_{jl} \\mathrm{Cov}(z_k, z_l)$\n> $= \\sum L_{ik} L_{jk}$\n\n---\n(Page 87)\n\n### METROPOLIS-HASTINGS ALGORITHM (slides)\n> (can also use this to get \"Markov process\" + combine this w/gibbs to give us a MCMC)\n> side note\n\nMore General Jumping rule $J(\\theta^*|\\theta_i)$\n(Need not be symmetric $J(\\theta_a|\\theta_b) \\neq J(\\theta_b|\\theta_a)$)\n1.  Choose random starting point $\\theta_0$\n2.  At step $i=1,...,N$ propose a new parameter value,\n    $\\theta^* \\sim J(\\theta^*|\\theta_{i-1})$\n3.  Evaluate M-H ratio of posteriors at proposed vs current values\n    $$ r = \\frac{P(\\theta^*|y)/J(\\theta^*|\\theta_{i-1})}{P(\\theta_{i-1}|y)/J(\\theta_{i-1}|\\theta^*)} $$\n4.  Accept $\\theta^*$ with probability\n    $\\min(r,1): \\theta_i = \\theta^*$\n    If not accept, stay at same value $\\theta_i = \\theta_{i-1}$ and include in chain.\n5.  Repeat 2-4 until reach some measure of convergence and gather enough samples to compute your inference.\n> METROPOLIS-HASTINGS THEORY: see appendix\n> \"Markov process\" (chain) to reach stationary distribution (i.e. approximates the posterior). A markov chain which satisfies certain conditions, detailed balance (+... see text), will converge to state $\\theta_i^*$ in such a way that its distribution is stationary i.e. it is the target dist.\n> one of these is 'ergodicity', process of complex between states is such that progression is that it is guaranteed to visit all states in phase space.\n> this ergodicity of the markov chain (regularity plus...) means behavior over time is the same as behavior over ensemble. In any case, to get to the stationary point is necessary to reach convergence due to these 2 conditions.\n> design a markov process (by constructing transition probability) that fulfills these 2 conditions (DB and ergodicity) $\\rightarrow$ this is how metropolis-hastings algo works.\n\n---\n(Page 88)\n\n*   d-dim Metropolis is just a special case, where\n    $J(\\theta^*|\\theta_i) = N(\\theta^*|\\theta_i, \\Sigma_p) = N(\\theta_i|\\theta^*, \\Sigma_p) = J(\\theta_i|\\theta^*)$\n    is a symmetric proposal dist.\n*   More general asymmetric proposals, allow \"biased\" proposals $\\rightarrow$ more probable to propose towards a certain direction.\n*   With some knowledge of structure of the posterior, can sometimes engineer a clever proposal $J(\\theta_i|\\theta_j)$.\n    > e.g. HMC\n\n### GIBBS SAMPLING (board)\n> use when can sample from $P(\\theta_1|D)$ and can sample from $P(\\theta_2|D)$ directly. (conditionals)\n*   Special case of Metropolis-Hastings (Gelman BDA ch 11)\n*   Multi-dim sampling when you can utilize the set of **conditional posterior distributions** as proposal distns. for each parameter - Metropolis Hastings ratio = 1 (always accept).\n*   If joint posterior is $P(\\theta, \\phi|D)$\n*   And you can solve for tractable (you can draw) conditionals:\n    $P(\\theta|\\phi, D)$\n    $P(\\phi|\\theta, D)$\n*   Jump along each parameter-dimension one at a time.\n> derive: $p(\\theta_2|\\theta_1, \\mathcal{D}) = p(\\theta_1, \\theta_2|\\mathcal{D}) / p(\\theta_1|\\mathcal{D})$. This is how stationary dist arises. to construct this, choose proposal dist $p(\\theta_2|\\theta_1, \\mathcal{D})$. ... $p(\\theta_1, \\mathcal{D}) = \\int p(\\theta_1, \\theta_2) d\\theta_2 / p(\\mathcal{D})$\n> 1) given transition $P(\\theta_1|\\mathcal{D})$, choose $J(\\theta_2|\\theta_1)$. A good choice...\n> 2) so, $A(\\theta_2|\\theta_1) = P(\\theta_1|\\mathcal{D})J(\\theta_2|\\theta_1)$.\n> 3) choose, acceptance ratio $A(\\theta_2|\\theta_1)$, s.t. this condition... `min(p,1)` with $p = P(\\theta_1|\\mathcal{D})J(\\theta_2|\\theta_1)/P(\\theta_2|\\mathcal{D})J(\\theta_1|\\theta_2)$.\n\n---\n(Page 89)\n\n### 2-dim GIBBS SAMPLER (slides)\n> conditional requires always accept\n1.  Choose random starting point $(\\theta_1^0, \\theta_2^0)$\n2.  At cycle $t$, update\n    $\\theta_1^t \\sim P(\\theta_1|\\theta_2^{t-1}, D)$\n    > first depends on this\n3.  Then update\n    $\\theta_2^t \\sim P(\\theta_2|\\theta_1^t, D)$\n    (each complete set (pair) of updates is called a **Gibbs cycle**)\n4.  Record current values of chain $(\\theta_1^t, \\theta_2^t)$\n5.  Repeat 2-4 until reach some measure of convergence (G-R) and gather enough indep. samples to compute your inference! (reduce Monte Carlo error)\n\n---\n(Page 90)\n\n### d-DIM GIBBS SAMPLER\nParameter vector $\\theta = (\\theta_1, ..., \\theta_d)$\nCurrent state at j-th update within cycle $t$:\n> value of component j at iteration t-1\n$\\theta_{-j}^t \\equiv (\\theta_1^t, ..., \\theta_{j-1}^t, \\theta_{j+1}^{t-1}, ..., \\theta_d^{t-1})$\n> value of all components except j at iteration t cycle j updates\n> d-1 dim vector\n1.  Choose random starting point $\\theta_0$\n    > iteration t-1 value, component already updated?\n    > position of component being updated (?), depends on components not yet updated. Why? only use $\\theta_{i-1}$?\n2.  At cycle $t$, update through the d-parameters:\n    For each $j=1,...,d$, move the jth parameter to\n    $\\theta_j^t \\sim P(\\theta_j|\\theta_{-j}^t, D)$\n    (update $\\theta_j$ conditional on current values of all other parameters)\n3.  After updating all d parameters, record current state $\\theta^{t+1}$\n4.  Repeat 2-3 until reach convergence and enough samples.\n> (e.g. d=3) start: $\\theta^0=(\\theta_1^0, \\theta_2^0, \\theta_3^0)$\n> $\\theta_1^1 \\sim P(\\theta_1|\\theta_2^0, \\theta_3^0)$ cycle\n> $\\theta_2^1 \\sim P(\\theta_2|\\theta_1^1, \\theta_3^0)$\n> $\\theta_3^1 \\sim P(\\theta_3|\\theta_1^1, \\theta_2^1)$\n> Gelman slightly different notation:\n> $\\theta_{-j}^t = (\\theta_1^t, ..., \\theta_{j-1}^t, \\theta_{j+1}^{t-1}, ..., \\theta_d^{t-1})$\n> propose $\\theta_j^t \\sim P(\\theta_j|\\theta_{-j}^{t-1})$\n\n---\n(Page 91)\n\n### Gibbs Sampling: Example (slides) (Gelman BDA section 11.1)\n> consider single observation $y=(y_1, y_2)^T$\nLikelihood: $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right)$\n> ($\\rho$ known)\nPriors: $P(\\theta_1) = P(\\theta_2) \\propto 1$\nPosterior: $\\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix}|y \\sim N \\left( \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right)$\n$P(\\theta|y) = P(\\theta_1, \\theta_2|y) = P(\\theta_1|\\theta_2, y)P(\\theta_2|y) = P(\\theta_2|\\theta_1, y)P(\\theta_1|y)$\n> simple to derive conditionals, demonstrating gibbs sampling with this toy model\n> Bivariate Gaussian, joint can be decomposed into conditionals.\n**Conditional Posteriors** (e.g. properties of MVGs): See moodle\n$\\rightarrow \\theta_1|\\theta_2, y \\sim N(y_1 + \\rho(\\theta_2 - y_2), 1-\\rho^2)$\n$\\rightarrow \\theta_2|\\theta_1, y \\sim N(y_2 + \\rho(\\theta_1 - y_1), 1-\\rho^2)$\n> 1D gaussians we can sample from\n\n[Image showing three plots. Left: Contour plot of the joint posterior $P(\\theta_1, \\theta_2|y)$ with elliptical contours centered near the origin. Middle: The same contour plot with a \"Gibbs sampling trace path\" overlaid. The path moves in right angles, parallel to the axes, exploring the high-probability region. Right: Two histograms for the marginal posterior densities $p(\\theta_1|y)$ and $p(\\theta_2|y)$ generated from the Gibbs samples, with the true marginal curves overlaid.]\n> trace plots in parameter space look like grid.\n> right angles, only update 1 parameter at a time.\n\nRemark: FLAW\n> in practice, for highly correlated params, gibbs sampler performs poorly. vertical/horizontal axis steps are diagonal, so hard for gibbs sampler to explore the space! Gibbs will also be a problem.\n> Metropolis-Hastings but especially a problem here!?\n\n---\n(Page 92)\n\n## Lecture 18\n_5.3.25_\n> see sheet 3\n> read w/sampling tut on moodle\n\nToday: Continuing Bayesian Computation:\nMetropolis-Hastings, Gibbs, Sampling, applications to SN cosmology, Assessing Convergence and Mixing.\n\n### GIBBS SAMPLING STEP AS A SPECIAL CASE OF METROPOLIS-HASTINGS (Gelman 11.3) (board)\n> Gibbs is special case of metropolis hastings that always accepts.\n> show this?\nAt the jth parameter update within iteration t (target $\\pi(\\vec{\\theta}) = P(\\vec{\\theta}|D)$)\nM-H ratio: $r = \\frac{\\pi(\\vec{\\theta}^*) / J(\\vec{\\theta}^*|\\vec{\\theta}^{t-1})}{\\pi(\\vec{\\theta}^{t-1}) / J(\\vec{\\theta}^{t-1}|\\vec{\\theta}^*)}$\n> PTO for main notes, circles\nRecall $\\theta_{-j}^{t-1} = (\\theta_1^t, ..., \\theta_{j-1}^t, \\theta_{j+1}^{t-1}, ..., \\theta_d^{t-1})$\n> current components of $\\theta$ except $\\theta_j$\nhave updated up to component j-1.\n\nIn Gibbs, we choose\n> can only move to new proposals where others are not updating so they stay the same\n$J(\\vec{\\theta}^*|\\vec{\\theta}^{t-1}) = \\begin{cases} \\pi(\\theta_j^*|\\theta_{-j}^{t-1}) & \\text{if } \\vec{\\theta}^*_{-j} = \\vec{\\theta}_{-j}^{t-1} \\\\ 0 & \\text{otherwise} \\end{cases}$\n> only move for jth component as all others are fixed.\n\n$r = \\frac{\\pi(\\vec{\\theta}^*) / \\pi(\\theta_j^*|\\theta_{-j}^{t-1})}{\\pi(\\vec{\\theta}^{t-1}) / \\pi(\\theta_j^{t-1}|\\theta_{-j}^{t-1})}$\n> $\\vec{\\theta}^*_{-j} = \\vec{\\theta}_{-j}^{t-1}$\n> use this to expand terms into products\n\n$= \\frac{\\pi(\\theta_j^*) \\pi(\\theta_{-j}^{t-1}) / \\pi(\\theta_j^*|\\theta_{-j}^{t-1})}{\\pi(\\theta_j^{t-1}) \\pi(\\theta_{-j}^{t-1}) / \\pi(\\theta_j^{t-1}|\\theta_{-j}^{t-1})} = \\frac{\\pi(\\theta_{-j}^{t-1})}{\\pi(\\theta_{-j}^{t-1})} = 1$\n> ALWAYS ACCEPT\n\nSaves on computation $\\rightarrow$ always accept, don't need to calc. ratio\n\n---\n(Page 93)\n\n### Metropolis within Gibbs (slides)\n> Gibbs problem: params need tractable conditional distributions in gibbs, but sometimes we can't solve for conditionals for the full substep of metropolis rule\n$\\theta = (\\theta_1, ..., \\theta_d) \\quad \\theta_{-j} = (\\theta_1, ..., \\theta_{j-1}, \\theta_{j+1}, ..., \\theta_d)$\n*   When you can't solve for tractable conditional distributions for all $\\theta_j$:\n    $P(\\theta_j|\\theta_{-j}, D)$\n*   Replace each substep for updating each jth parameter $\\theta_j$ with a separate Metropolis rule, compute Metropolis ratio and accept/reject.\n*   Cycle through all parameters and repeat for all N MCMC steps\n    > multiple proposal/rejects within full state of parameters\n\nPREVIOUS PAGE\nGibbs as special case of M-H:\n> do I need to explain this?\nMHS: $r = \\frac{P(\\vec{\\theta}^*|D) J(\\vec{\\theta}^{t-1}|\\vec{\\theta}^*)}{P(\\vec{theta}^{t-1}|D) J(\\vec{\\theta}^*|\\vec{\\theta}^{t-1})}$\n> $\\theta_{-j}^*$ is current component of $\\vec{\\theta}_{-j}^{t-1}$ in a gibbs cycle\nFor Gibbs:\nPropose from conditional distribution $J(\\theta^*|\\theta^{t-1}) = \\begin{cases} P(\\theta_j^*|\\theta_{-j}^{t-1}) & \\text{if } \\theta_{-j}^* = \\theta_{-j}^{t-1} \\\\ 0 & \\text{otherwise} \\end{cases}$\n> all components must be the same\n> we only propose new points where all components are the same except $\\theta_j$, where $\\theta_j$ is picked according to $P(\\theta_j^*|\\theta_{-j}^{t-1})$.\n$r = \\frac{P(\\theta_j^*, \\theta_{-j}^{t-1}) P(\\theta_j^{t-1}|\\theta_{-j}^{t-1})}{P(\\theta_j^{t-1}, \\theta_{-j}^{t-1}) P(\\theta_j^*|\\theta_{-j}^{t-1})} = \\frac{P(\\theta_j^*|\\theta_{-j}^{t-1})P(\\theta_{-j}^{t-1})}{P(\\theta_j^{t-1}|\\theta_{-j}^{t-1})P(\\theta_{-j}^{t-1})} = \\frac{...}{...} = 1 \\implies \\text{ALWAYS ACCEPT}$\n> expands conditionals\n> (all cancel see page 93)\n> e.g. $\\theta_j^*, \\theta_{-j}^{t-1}$\n> essentially we propose a new value for $\\theta_j$ but we've needed to go through these steps\n> so could be well defined as a metropolis that is designed to integrate this logic?\n\n---\n(Page 94)\n\n### d-dim Metropolis-within-Gibbs Sampler (slides)\n$\\theta=(\\theta_1, ..., \\theta_d) \\quad \\theta_{-j}^t = (\\theta_1^{t+1}, ..., \\theta_{j-1}^{t+1}, \\theta_{j+1}^t, ..., \\theta_d^t)$\n1.  Choose a random starting point $\\theta_0$\n2.  At cycle $t=1,...,N$, cycle through the d-parameters:\n    A. For each $j=1,...,d$, propose a new jth parameter value from a 1-dimensional Gaussian.\n       $\\theta_j^* \\sim N(\\theta_j^t, \\tau_j^2)$\n       > proposal scale $\\tau_j^2$\n       > overcome z-gaussian\n    B. Evaluate ratio of posteriors at proposed vs current values:\n       $r = P(\\theta_j^*|\\theta_{-j}^t, D) / P(\\theta_j^t|\\theta_{-j}^t, D)$\n       $= P(\\theta_j^*, \\theta_{-j}^t|D) / P(\\theta_j^t, \\theta_{-j}^t|D)$\n       > (new point) (old point)\n       > note, only component j changes in num/den\n    C. Accept $\\theta_j^{t+1} = \\theta_j^*$ with prob $\\min(r,1)$,\n       otherwise $\\theta_j^{t+1} = \\theta_j^t$.\n3.  After full cycle, record current values $\\theta^{t+1}$\n4.  Repeat steps 2 for all parameters until convergence and enough samples\n    > marginals true posterior\n    [Image of a trace plot and posterior histograms for two parameters, $p(\\theta_1|y)$ and $p(\\theta_2|y)$.]\n> Note: adjust $\\tau_j^2$\n> empirically aim for 30%-40% acceptance rates\n> terminal step will accept everything, but not explore param space, too long now accept where more anywhere\n\n---\n(Page 95)\n\n*Mixed Gibbs Sampler*\n> always accept\n> e.g. can derive one conditional derived from posterior, gives algorithm info about which direction is likely to have more probability\n> disadvantage: requires analytic properties, but if we can't derive them, can still use metropolis within Gibbs.\n$\\rightarrow$ Can replace sampling from conditionals with accept/reject for just the parameters with intractable conditionals.\n> in cycle t\n> for each j\n> if we do have analytic conditional, just sample normally from that & always accept.\n> if not, propose a new point from proposal distribution & accept w/ prob min(r,1) where r is metropolis ratio $r = \\frac{P(\\theta_j^*, \\theta_{-j}^t)}{P(\\theta_j^t, \\theta_{-j}^t)}$.\n\n---\n(Page 96)\n\n### Tuning d-dim Metropolis\n> (this is proposal scale)\n*   $\\theta^* \\sim N(\\theta; \\Sigma_p)$: If proposal scale $\\Sigma_p$ is too **large**, will get too many rejections and not go anywhere. If proposal scale too **small**, you will accept very many small moves: inefficient random walk.\n*   **Laplace Approximation**\n    $P(\\theta|D) \\approx N(\\theta|\\theta_{MAP}, A^{-1})$\n    $\\theta_{MAP}$ = posterior mode\n    $A_{ij} = - \\left.\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\ln P(\\theta|D)\\right|_{\\theta=\\theta_{MAP}}$\n    > approx posterior cov matrix $A^{-1}$\n*   Choose $\\Sigma_p = c^2 A^{-1}$, $c \\approx 2.4/\\sqrt{\\dim(\\theta)}$\n*   Scale proposal to aim for **acceptance ratio** of **44%** in 1D, **23%** in dim $>5$.\n    > note: will capture correlation e.g. of high correlation.\n    > Laplace approx. serves proposal w/ some correlation structure.\n    > target highly correlated\n\n---\n(Page 97)\n\n### LAPLACE APPROXIMATION (slides)\n> now trying to find $P(\\theta)$\nUnnormalised Posterior: $\\hat{P}(\\theta|D) = P(D|\\theta)P(\\theta)$\nFind MAP estimate:\n> \"max. a posterior\"\n> same as max likelihood but for posterior\n$$ \\theta_{MAP} = \\underset{\\theta}{\\mathrm{argmax}} \\ln \\hat{P}(\\theta|D) $$\n> $P(\\theta|D)$ with non-informative prior?\nTaylor expansion (first deriv. is zero):\n$$ \\ln \\hat{P}(\\theta|D) \\approx \\ln \\hat{P}(\\theta_{MAP}|D) - \\frac{1}{2}(\\theta-\\theta_{MAP})^T A (\\theta-\\theta_{MAP}) + ... $$\n$$ A_{ij} = -\\left.\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\ln \\hat{P}(\\theta|D)\\right|_{\\theta=\\theta_{MAP}} $$\n$$ \\hat{P}(\\theta|D) \\approx \\hat{P}(\\theta_{MAP}) \\times \\exp\\left(-\\frac{1}{2}(\\theta-\\theta_{MAP})^T A (\\theta-\\theta_{MAP})\\right) $$\n**Gaussian Approximation**:\n> approx (unnormalised posterior)\n$$ P(\\theta|D) \\approx N(\\theta|\\theta_{MAP}, A^{-1}) $$\n$A^{-1}$ is an **approximate posterior covariance matrix**.\n> reminder: multi-dim taylor exp:\n> $\\ln \\hat{P}(\\theta) = \\ln \\hat{P}(\\theta_{MAP}) + \\underbrace{\\nabla\\ln\\hat{P}(\\theta_{MAP})}_{0}(\\theta-\\theta_{MAP}) + \\frac{1}{2}(\\theta-\\theta_{MAP})^T H(\\theta_{MAP})(\\theta-\\theta_{MAP}) + ...$\n> $\\implies (\\theta) \\approx L(\\theta_{MAP}) - \\frac{1}{2}(\\theta-\\theta_{MAP})^T A (\\theta-\\theta_{MAP})$\n> Hessian of $\\ln\\hat{P}$ (num. of 2nd derivatives) evaluated at $\\theta$.\n> $A$ is -H eval at $\\theta_{MAP}$.\n\n---\n(Page 98)\n\n### Astrostatistics Case study: Supernova Cosmology\n> ex sheet 3\n> measure brightness of SN\nCosmology: precise, causal relationship between distance & redshift due to expansion. EoS parametrized by w ($P=w\\rho$).\nModern cosmology: $\\rightarrow$ Is $w=-1$?\n- type Ia SN almost standard candles\n- measure their magnitude & plot vs redshift.\n- curve shape depends on how universe expands.\n- params $\\Omega_m, \\Omega_\\Lambda$ -> so observed data constrains the parameters -> sample analysis, hold vs. $\\Omega_\\Lambda > 0$.\n> [A plot of distance modulus vs redshift, with a line showing an increasing relationship.]\n- comoving distance is integral from zero to redshift\n- distance probed by SN is luminosity distance\n- get d_L by multiplying that by dimensions (or univ. MEA?)\n- distance redshift also influenced by curvature of universe, etc.\n- choose diff processes, plot theoretical dist. modulus vs. redshift.\n- diff curve for diff params etc.\n> [A plot of distance modulus vs redshift z, showing several different theoretical curves.]\n- data\n> [A small scatter plot of data points.]\n- want to fit data to curve to infer params?\n\n---\n(Page 99)\n\n(slides) (lots of inconsistent notation here -> ignore it)\nFor us, simplify, let's say SNae are standard candles.\n$$ M_s \\sim N(M_0, \\sigma_{int}^2) \\quad \\text{Population Distribution} $$\n> if distribution could tell us\n$M_s = m_s + \\mu(z_s, \\vec{\\Omega})$ (log) invt. square law.\nAssume data (apparent magnitudes and redshifts) $\\{\\hat{m}_s, z_s\\}$ are measured perfectly.\n> specific to SN Ia, for now we will ignore more\n$\\theta = (H_0, \\Omega_m, \\Omega_c, w) \\quad$ Cosmological Parameters\nFirst assume $w=-1$ (cosmological constant)\nDerive model, likelihood, posterior\n$\\vec{\\Omega} = (\\Omega_m, \\Omega_c, w)$, $h = H_0/(100 \\text{km s}^{-1} \\text{Mpc}^{-1})$\n> has a nice distance modulus relation derived from the standard candle assumption.\n> Transversely, distance to an object determines redshift from the standard candle model\n$\\mu(z_s; H_0, \\vec{\\Omega}) = 25 + 5\\log_{10}\\left(\\frac{c}{H_0}\\frac{d_L(z_s, \\vec{\\Omega})}{Mpc}\\right)$\n> dimensionless luminosity distance $d_L=c \\int dz/H_e$.\n$M_s = m_s + \\mu(z_s; H_0, \\vec{\\Omega})$\n$P(m_s|z_s) = ...$\n(dimensional)\n\n$M_0$ and $\\log(h)$ cannot be separately constrained! (not separately identifiable)\n> nuisance parameter\n> multiple solutions\n> overlap \"mixing\" good.\n> not getting stuck in region space\n[A sketch of a bimodal or multimodal posterior, showing a sampler getting stuck in one mode.]\n> ex sheet 3\n> have file of 40 metropolis, use laplace approx, time to sample posterior\n\n---\n(Page 100)",
      "timestamp": "2025-10-31T15:23:11.094462",
      "tokens": 11363,
      "cost": 0.09841687499999999
    },
    {
      "role": "user",
      "content": "Continue transcribing pages 101-125 of the astrostatistics lecture notes. Maintain the same markdown formatting style with LaTeX math, page markers, lecture headers, image descriptions, and marginal notes.",
      "timestamp": "2025-10-31T15:25:38.152076",
      "tokens": 77970,
      "cost": 0.09745124999999999
    },
    {
      "role": "assistant",
      "content": "## Lecture 19\n_7.3.25_\n\nToday: continue MCMC - applications to SN cosmology, assessing convergence & mixing, comparison of algos.\n> (sheet 3)\n\n### New case:\nNow assume flat univ. $\\Omega_\\Lambda=1-\\Omega_m$ but unknown $M_0, \\sigma_{int}^2, \\Omega_m, w$\n> [A sketch of two MCMC trace plots for 'w' and '$\\Omega_m$'. The traces oscillate without clear structure, indicating some mixing.]\n> see notes\nchanging $w \\rightarrow$ v. small effect, so hard to constrain $w$\n> sample w/ metropolis-within-gibbs\nnow instead use **metropolis-within-gibbs** **4x1D** for each parameter (fix all other params to their current value and update one) tune **4x proposal scale & 4x acceptance ratios**\n> (visually doesn't look like the best sampling on slides (not mixed well))\n2D: plot shows banded structure in samples: sampling on the algorithm. (death of MCMC)\ncan use **estimated cov. matrix** (from initial MCMC or Laplace approx) for **correlated proposal dist.** in **4D metropolis**!\n> helps it know what direction to sample in\nuse this as proposal for **4D metropolis $\\rightarrow$ see much better movement/mixing**, chains look more similar $\\rightarrow$ **improvement!**\n> [A 2D scatter plot of samples in the $(\\Omega_m, w)$ plane showing a correlated, elliptical cloud of points, indicating better mixing. An arrow points to this plot from a previous, more grid-like plot.]\n> (sound decision)\n\n> [A sketch of a banana-shaped posterior distribution. An arrow points from it, with the text: \"better define a parametrised dist to better match the posterior, guides us in sampling in the right direction (effectively explore sample space)\".]\n> estimate cov matrix from sample points. This initial run is used as a proposal dist for a new 4D metropolis run.\n\n---\n(Page 104)\n\n### How Many Iterations to get an Independent Sample? Autocorrelation Function\n> (slides)\n\nFor each scalar parameter $\\theta$\n> Chain: $(\\theta_1, ..., \\theta_N)$\nConsider sample mean $\\bar{\\theta} = \\frac{1}{N}\\sum_i \\theta_i$\n\nIf chain were **serially uncorrelated**: $\\mathrm{Cov}(\\theta_i, \\theta_j)=0, \\quad (i \\neq j)$\nthen $\\mathrm{Var}[\\bar{\\theta}] = \\mathrm{Var}(\\theta)/N$.\n\nHowever, chain is typically **serially correlated**:\n> autocorrelation coefficient\n$\\mathrm{Cov}[\\theta_i, \\theta_j] = C_{|i-j|} = \\mathrm{Var}(\\theta)\\rho_{|i-j|}$.\n\nThen for large $N$: $\\mathrm{Var}[\\bar{\\theta}] = \\mathrm{Var}(\\theta) \\times \\frac{\\tau}{N} = \\mathrm{Var}(\\theta) / N_{eff}$\nWhere $\\tau = 1 + 2\\sum_{t=1}^\\infty \\rho_t$ is the **AUTOCORRELATION TIMESCALE**\n> basically tells us how many steps are req'd to 'forget' where you are now i.e. get an indep. sample?\n\nand $N_{eff} = N/\\tau$ is the **EFFECTIVE SAMPLE SIZE (ESS)** (equivalent number of independent samples).\n> after N samples, only get $N/\\tau$ indep. samples underlying dist.\n\n**PROOF**:\n$\\rightarrow$ Find $\\mathrm{Var}[\\bar{\\theta}]$: reminder $\\mathrm{Var}(\\sum a_i X_i) = \\sum \\sum a_i a_j \\mathrm{Cov}(X_i, X_j)$\n$\\mathrm{Var}[\\bar{\\theta}] = \\mathrm{Var}[\\frac{1}{N}\\sum_i \\theta_i] = \\frac{1}{N^2}\\mathrm{Var}[\\sum_i \\theta_i] = \\frac{1}{N^2}(\\sum_i \\mathrm{Cov}(\\theta_i, \\theta_i) + \\sum_{i \\neq j} \\mathrm{Cov}(\\theta_i, \\theta_j))$\n$= \\frac{1}{N^2}(N\\mathrm{Var}(\\theta) + \\sum_{i \\neq j} \\mathrm{Var}(\\theta)\\rho_{|i-j|} + N\\mathrm{Var}(\\theta))$\n> correction:\n$\\frac{1}{N^2}\\sum_{i,j} \\mathrm{Cov}(\\theta_i, \\theta_j) = \\frac{1}{N^2}[N\\mathrm{Var}(\\theta) + \\sum_{i \\neq j} \\mathrm{Var}(\\theta)\\rho_{|i-j|}]$\n> explanation:\n> for a \"geometrically\" fast chain which is stationary, for infinitely large chain imagine going in chain direction & adding correlations.\n> if it disappears very fast, it goes to one right & factors of N cancel.\n> sum lag 1, lag 2 etc. for each point in the chain.\n> sum over large N: $\\mathrm{Var}(\\bar{\\theta}) \\approx \\frac{\\mathrm{Var}(\\theta)}{N} [1 + 2\\sum_{t=1}^\\infty \\rho_t]$\n\n---\n(Page 105)\n\n### Estimating the Autocorrelation / ESS\n> do this in practice:\n\nFor each scalar parameter $\\theta$\n> want to estimate: $\\hat{\\rho}_t = \\mathrm{Cov}(\\theta_i, \\theta_{i+t}) / \\mathrm{Var}(\\theta)$\n> using for ex. $t=1, 2, ..., T$.\n\nSample covariance of lag $t$:\n> take samples that are distance t apart and compute covariance\n$$ \\hat{c}_t = \\frac{1}{N-t}\\sum_{i=1}^{N-t} (\\theta_i - \\bar{\\theta})(\\theta_{i+t} - \\bar{\\theta}) $$\n$\\hat{c}_0 = $ sample variance of $\\theta$\n\nSample autocorrelation of lag $t$:\n> only support for short-lag samples, chain-2 part, sample distance far apart (doesn't work).\n$$ \\hat{\\rho}_t = \\hat{c}_t / \\hat{c}_0 $$\n$$ \\hat{\\tau} = 1 + 2\\sum_{t=1}^T \\hat{\\rho}_t $$\n**Estimated autocorrelation time.** Truncate at $T$ lags s.t. $\\hat{\\rho}_T \\approx 0.1$.\n$$ N_{eff} = N/\\hat{\\tau} $$\n**Effective number of independent samples**\n> could get negative correlations $\\hat{\\rho}_t$, indicates \"anti-correlation\", good for mixing.\n> slowest parameter is the limiting one!\nIn practice, what matters is: **Compute time / Effectively Independent Sample.**\n> metric\n> weigh up how fast algo is vs how many effective indep samples is needed to choose between algos. good to compare!\n\n> see slide \"Autocorr vs lag\"\n> shows better sampler -> don't see banded structure in sampling.\n> [Image of a plot with x-axis 'lag' (log scale) and y-axis 'Autocorrelation'. A curve starts at 1 and decreases towards 0. Labels \"end of burn in\" and \"example\" are present.]\n> method: 1) 4D M-within-G, N=10,000, $N_{eff} \\approx 13$ not good. 2) 4D metropolis, better looks good sampler ($N_{eff}/N_{its}$). -> $\\approx$3% to get one independent sample.\n\n---\n(Page 106)\n\n### Mixed Samplers (slides)\n> can mix & match these algos e.g.\n> with Gibbs we are \"stuck\"\n> with metropolis, only one param updated\n\nTarget Posterior (intractable - cannot sample): $\\pi(\\alpha, \\beta, \\theta)$\nSuppose these are tractable\n$\\pi(\\alpha|\\beta, \\theta)$\n$\\pi(\\beta|\\alpha, \\theta)$\nbut $\\pi(\\theta|\\alpha, \\beta)$ is not tractable\n> so can't do complete gibbs sample\n\n**Mixed sampler**:\n> so gibbs start for not-place $\\theta$ step with metropolis proposal ratio. split-step.\n1) sample $\\alpha^t \\sim \\pi(\\alpha|\\beta^{t-1}, \\theta^{t-1})$\n2) sample $\\beta^t \\sim \\pi(\\beta|\\alpha^t, \\theta^{t-1})$\n3) Metropolis (proposal/acc./rej.) update $\\theta^t$\n\n### Parameter Blocking\nTarget posterior (intractable - cannot sample): $\\pi(\\alpha, \\beta, \\theta)$.\nAs before, suppose $\\pi(\\theta|\\alpha, \\beta)$ not tractable.\n> Other conditionals tractable $\\pi(\\alpha, \\beta|\\theta)$\n**Mixed Sampler with Blocking**: (more efficient)\n1) Jointly sample $\\alpha^t, \\beta^t \\sim \\pi(\\alpha, \\beta|\\theta^{t-1})$\n2) Metropolis (proposal/acc./rej.) update $\\theta$.\n> usually more efficient\n\n---\n(Page 107)\n\n### (more abstract how MCMC works -> now show why it works)\n(slides)\n**Run 10,000 chains**:\n> (start at choices of some value)\n**Histogram of initial values of chains (t=1)**\n[Image of a histogram labeled 'pdf' vs 'x'. It shows a uniform distribution of initial points.]\n> histogram of ensemble of chains. at time t=1 this single chain is one value.\n\n**Histogram of chain values at step t=100**\n[Image of a histogram labeled 'pdf' vs 'x'. It shows a distribution resembling the target posterior. Text says \"looks like dist for 1 chain\" and \"also looks like true dist.\"]\n\n### ERGODICITY:\n**Convergent distribution of chain ensemble at single time-slice = distribution-over-time of single chain = target (posterior) distribution (convergence to a stationary distribution)**\n**Time average = Ensemble average**\n> own behaviour over time is the same as behaviour averaged over many diff. instances ('ensembles') of the system.\n> key of why MCMC works: as blue gets one sample, so ...\n> matches que distribution, but we don't want to run a chain 100 times to reach that dist -> so we just want to run one chain & have it approximate the dist. at large times\n\n---\n(Page 108)\n\n### A SKETCH OF MCMC THEORY (board)\n**Markov chain**: A sequence of random variables\n$\\theta^0, \\theta^1, ..., \\theta^{t-1}, \\theta^t \\in \\Theta \\quad$ (state space = parameter space)\nwith **Markov property**: $P(\\theta^t|\\theta^{t-1}, ..., \\theta^1, \\theta^0) = P(\\theta^t|\\theta^{t-1})$\n> only depends on previous state, not all its history\n\n**Joint distribution**: $P(\\theta^t, \\theta^{t-1}, ..., \\theta^1, \\theta^0)$\n$= P(\\theta^t|\\theta^{t-1}) \\times P(\\theta^{t-1}|\\theta^{t-2}) \\times ... \\times P(\\theta^1|\\theta^0) P(\\theta^0)$\n$$ = \\pi_0(\\theta^0) T_1(\\theta^0 \\to \\theta^1) \\times ... \\times T_{t-1}(\\theta^{t-2} \\to \\theta^{t-1}) \\times T_t(\\theta^{t-1} \\to \\theta^t) $$\n> call $\\pi_0(\\theta^0)$ (pdf of initial state)\n> transition probabilities\n> (ill) $P(\\theta^t, \\theta^{t-1}, ..., \\theta^0) = \\pi_0(\\theta^0) \\prod_{i=1}^t P(\\theta^i|\\theta^{i-1})$\n\nIf $T_t(\\theta^{t-1} \\to \\theta^t) = T(\\theta^{t-1} \\to \\theta^t)$ not explicitly time-dependent $\\rightarrow$ \"**time homogeneous**\"\n> e.g. proposal transition state you go to is same whether at beginning or end of chain\n\n**Marginal distribution**:\n$$ P(\\theta^t) = \\int \\pi_0(\\theta^0) T(\\theta^0 \\to \\theta^1) \\times ... \\times T(\\theta^{t-1} \\to \\theta^t) d\\theta^0...d\\theta^{t-1} $$\n\n**Goal of MCMC**: Want to engineer Markov chain s.t.\n> stationary distribution exists (if unique).\n(1) **Limiting distribution**\n    $\\pi_{lim}(\\theta) = \\lim_{t \\to \\infty} P(\\theta^t = \\theta) = \\pi_{stat}(\\theta)$ **stationary distribution aka invariant distribution**\n    and it is equal to the target\n(2) $\\pi_{lim}(\\theta) = \\pi_{stat}(\\theta)$ is equal to target posterior $\\pi_{post}(\\theta)$\n> (1) system converges to unique stationary dist (?)\n> (2) that dist is the target.\n\n---\n(Page 109)\n\n### Good Properties for MCMC: (board)\n> (can explore parameter space in good-fast ways)\n\n1) **IRREDUCIBLE**\n   MC can get to **any** state from any other state in a finite # of steps.\n\n2) **APERIODIC**\n   [Periodic: MC has positive probability of returning to a state **only** with period $k \\ge 1$]\n   > system does not return to the same state at fixed intervals\n\n3) **NOT TRANSIENT**\n   > be in contrast to transient\n   > (aka recurrent)\n   [Transient: Non-zero probability of **never returning** to a particular state]\n   > problem: will tell us nothing about a state if we never visit it\n   > better condition than positive recurrent. (will return to state but could take infinitely long to get there)\n\n> If a regular M-H algorithm has a unique stationary dist (implied by condition of detailed balance).\n> A stationary dist is unique if the M chain is generated by regularity which requires:\n> aperiodic, irreducible $\\rightarrow$ positive recurrent\n> (are we guaranteed to converge?) to taking dist to be journey?\n\n---\n(Page 110)\n\n## Lecture 20\n_10.3.25_\n> demo mcmc nodes on moodle\n> \"should be working on ex sheet 3, multiple components on the one side of convergence will show the same, some ideas for diff algorithms, will investigate\"\n> e.g. comparing different algorithms for the same problem.\n\nToday: finishing Bayesian computation (MCMC), soon: gaussian processes\n\n### A sketch of MCMC theory cont. (board)\n(First, some jargon :)\n**Positive Recurrent**: has finite expected time to return to each state.\n\n**Ergodic**: Aperiodic & positive recurrent (time avg = expectation over ensemble)\n\nIf we have:\n> Markov Chain\nMarkov chain which is\n1) Irreducible\n2) Aperiodic\n3) Not transient\n(stronger condition) positive recurrent\n$\\left. \\begin{array}{l} \\text{1) Irreducible} \\\\ \\text{2) Aperiodic} \\\\ \\text{3) Positive recurrent (stronger)} \\end{array} \\right\\}$\n> these do not guarantee convergence\n$\\rightarrow$ MC converges to a unique, **stationary distribution** $\\pi_{stat}$ as the **limiting** $\\pi_{lim}$ $(t \\to \\infty)$.\n\nWe want to construct transition probabilities s.t. $\\pi_{stat} = \\pi_{post}$ (target)\n> (want our MCMC to be the true posterior)\nTo do that, show that **target distribution is invariant** under transition probabilities, i.e. our target is the **unique stationary distribution**.\n\n---\n(Page 111)\n\n### Stationary Distribution (board)\n> suppose we are at this point\nSuppose $\\theta^{t-1}=a \\sim \\pi(\\theta)$.\nNext step: $P(\\theta^t=b) = \\int P(\\theta^t)T(\\theta^{t-1}=a \\to \\theta^t=b) d\\theta^{t-1}$\n> next point depends on probability of starting point.\n$$ = \\int \\pi(a) T(a \\to b) da $$\n> this must equal $\\pi(b)$\niff $P(\\theta^t=b) = \\pi(b)$ then $\\pi$ is the stationary distribution $\\pi_{stat}$.\n> (target dist $\\pi$ is invariant under transition probabilities)\n(transition probabilities do not induce any net change in pdf over all states)\n\n### Detailed Balance\nA sufficient condition - that chain respects D.B. aka reversible, with $\\pi$ as stationary distribution\n**Reversible**: $P(\\theta^{t-1}=a, \\theta^t=b) = P(\\theta^{t-1}=b, \\theta^t=a)$\n> (some time invariant dist $\\pi$ that satisfies D.B.)\n> probability goes 'just way' = prob it goes the other way\n**Condition of D.B.**:\n$$ \\pi(a)T(a \\to b) = \\pi(b)T(b \\to a) $$\n> allowance mechanism\n(flow of probability $a \\to b$ perfectly balanced by flow from $b \\to a$)\nIf $\\theta^{t-1} \\sim \\pi$, $P(\\theta^t=b) = \\int \\pi(a) T(a \\to b) da$\n$P(\\theta^t=b) = \\pi(b)\\int P(a|b) da = \\pi(b)$\n$$ = \\pi(b) \\int T(b \\to a) da $$\n$\\rightarrow \\pi$ is the stationary distribution $\\pi_{stat}$!\n\n---\n(Page 112)\n\n### Metropolis algorithm respects D.B. with $\\pi_{stat} = \\pi_{post}$ (board)\n> (SEE APPENDIX)\n> If transition prob satisfies these conditions, then we will converge to stationary distribution.\n> want to show that the Metropolis rule algorithm we constructed is guaranteed to satisfy D.B.\nSuppose wlog we pick points $a,b \\in \\Theta$ s.t. $\\pi_{post}(a) < \\pi_{post}(b)$, and a proposal distribution $J(b|a) = J(a|b)$ (symmetric).\n\n**Forward**:\n> structure represents symmetric proposal\n> $P(\\theta^{t-1}=a, \\theta^t=b) = \\pi_{post}(a) T(a \\to b)$\n> prob propose transition\n$$ = \\pi_{post}(a) J(b|a) \\times 1 $$\n> ($r>1$)\n\n**Backward**:\n> prob accept transition\n$P(\\theta^{t-1}=b, \\theta^t=a) = \\pi_{post}(b) T(b \\to a)$\n$$ = \\pi_{post}(b) J(a|b) \\times \\min\\left(\\frac{\\pi_{post}(a)}{\\pi_{post}(b)}, 1\\right) $$\n$$ = \\pi_{post}(b) J(a|b) \\times \\frac{\\pi_{post}(a)}{\\pi_{post}(b)} $$\n$$ = J(a|b) \\pi_{post}(a) = J(b|a) \\pi_{post}(a) $$\n\n**Forward = Backward**. $\\rightarrow$ transition probabilities respect D.B. w/ $\\pi_{stat} = \\pi_{post}$\n> from previous paper this implies stationary...\n\n### (Exercise)\n> (see appendix)\n1) Show DB also holds for Metropolis Hastings w/ asymmetric proposal $J(a|b) \\neq J(b|a)$\n2) Gibbs sampling: suppose $\\pi_{post}(\\theta, \\phi)$ is the posterior over two parameters $\\theta, \\phi$. The conditionals $\\pi(\\theta|\\phi)$ and $\\pi(\\phi|\\theta)$ are tractable. By considering a full cycle of the Gibbs sampler, show that $\\pi_{post}(\\theta, \\phi)$ is the stationary dist.\n   ...show that one step of a Gibbs sampler is equivalent to a M-H step where the acceptance ratio is always 1.\n\n---\n(Page 113)\n\n### MCMC in Practice (slides)\n> (peaks)\n> [Image of a posterior distribution with a mode labeled $\\theta_{MAP}$ and a wider curve labeled \"proposal scale\".]\n1.  **Find the model(s)** using optimisation, run **Laplace approx.** to obtain a **proposal cov matrix**\n    > estimate location & shape of posterior. sometimes completely wrong if posterior is complicated (multimodal)\n2.  **Begin multiple (4-8 parallel) chains** at starting positions **dispersed** around the model(s) (can draw from approx. cov. matrix)\n    > start chains at diff points not just the peak\n3.  **Scale Metropolis proposals** to tune **25-50% acceptance rate** (depending on dimensionality of jump).\n4.  Use **proposal cov. matrix** that reflects the **shape** of the posterior\n    > (banana posterior)\n5.  After run, look at chains (if possible) to **check for obvious problems**\n    > should look like noise\n6.  Compute **Gelman-Rubin ratio** comparing within-chain-variance to between-chain-variance to check that chains are **well-mixed** (should be very close to 1), and assess **burn-in**\n    > not just mixing, but not getting stuck in a mode\n7.  Compute **autocorrelation timescale** and **effective sample size** to make sure you have **enough independent samples** for inference\n8.  If all checks out, remove burn-in, thin, and **combine chains to compute posterior inferences**.\n\n---\n(Page 114)\n\n### Overview of MCMC\n*   **MCMC Algorithms**\n    > most other algos are just special cases of M-H\n    -   Metropolis / M-H algorithms\n    -   Gibbs sampling\n    -   Metropolis-within-Gibbs\n    -   (don't cover: more modern MCMC use gradient (auto-diff)) - Hybrid / HMC / NUTS\n*   **Assessing / Comparing performance of MCMC algo.s**\n    -   Gelman-Rubin statistics for comparing mixing of multiple chains\n    -   Autocorrelation time - how long to get an independent sample?\n    -   Effective sample size - how many indep. samples do I have?\n*   **Detailed balance & theoretical considerations**\n> end of MCMC!\n> next time: gaussian processes.\n\n---\n(Page 115)\n\n## Lecture 21\n_12.3.25_\n\n### GAUSSIAN PROCESSES (slides)\n### What is a Gaussian Process?\n\n*   A **GP** is a (possibly infinite) collection of R.V.s $\\{\\xi_t\\}$, (typically indexed w/ some ordering in time, space, or wavelength), such that any **finite subset** of R.V.s have a jointly **multivariate Gaussian dist.**\n*   Any vector $\\vec{\\xi} = \\{\\xi_{t=1}, ..., \\xi_{t=N}\\}$ of a finite subset is multivariate Gaussian, therefore it is completely described by a mean $E[\\vec{\\xi}]$ and cov. matrix $\\mathrm{Var}[\\vec{\\xi}] = \\mathrm{Cov}[\\vec{\\xi}, \\vec{\\xi}^T]$.\n*   Elements of the cov. matrix are determined by a **function of the coordinates** e.g. $\\mathrm{Cov}[\\xi_t, \\xi_{t'}] = k(t,t')$ called the **covariance function or kernel**.\n*   A G.P. w/ mean function $m(t)$ and kernel $k(t,t')$ is denoted\n    $$ f(t) \\sim GP(m(t), k(t,t')) $$\n*   A G.P. provides a **distribution over functions**.\n> the dist of a GP, i.e. the joint of all these (countably many) variables, i.e. this is a dist over fns with a continuous domain (e.g. time or space)\n> e.g. $\\vec{f} = \\begin{pmatrix} f(t_1) \\\\ f(t_2) \\end{pmatrix}, \\begin{pmatrix} f(t_1) \\\\ f(t_2) \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} m(t_1) \\\\ m(t_2) \\end{bmatrix}, \\begin{bmatrix} k(t_1,t_1) & k(t_1,t_2) \\\\ k(t_2,t_1) & k(t_2,t_2) \\end{bmatrix} \\right)$ think?\n> such a process is completely defined by its mean $m(t)$ and covariance $k(t,t')$.\n\n---\n(Page 116)\n\n> see notes on moodle\n### Review: Properties of multivariate Gaussian (slides)\n\nFull prob. density: ($\\Sigma$ positive definite)\n$$ N(f|\\mu, \\Sigma) = [\\det(2\\pi\\Sigma)]^{-1/2} \\exp(-1/2(f-\\mu)^T\\Sigma^{-1}(f-\\mu)) $$\nJoint dist. of components:\n$$ f = \\begin{pmatrix} U \\\\ V \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} U_0 \\\\ V_0 \\end{bmatrix}, \\begin{bmatrix} \\Sigma_U & \\Sigma_{UV} \\\\ \\Sigma_V & \\Sigma_V \\end{bmatrix} \\right) $$\nIf you observe/know/condition on V:\nConditional dist. $U|V \\sim N(E[U|V], \\mathrm{Var}[U|V])$\nConditional mean $E[U|V] = U_0 + \\Sigma_{UV}\\Sigma_V^{-1}(V-V_0)$.\nConditional variance $\\mathrm{Var}[U|V] = \\Sigma_U - \\Sigma_{UV}\\Sigma_V^{-1}\\Sigma_{VU}$.\n\nIf V = observed data, U = unobserved parameters, then $P(U|V)$ is a **posterior pdf**!\n> reminder, marginals: $P(V) = \\int P(U,V)dU = N(V|\\mu_V, \\Sigma_V)$\n> $P(U) = N(U|\\mu_U, \\Sigma_U)$\n> (simply just drop irrelevant variable from mean vector & for matrix: e.g. $\\mu = \\begin{bmatrix} \\mu_U \\\\ \\mu_V \\end{bmatrix} \\to \\mu_V$, $\\Sigma = \\begin{bmatrix} \\Sigma_{UU} & \\dots \\\\ \\dots & \\Sigma_{VV} \\end{bmatrix} \\to \\Sigma_{VV}$)\n> then conditionals came from $P(U|V) = P(U,V)/P(V)$.\n> (Don't know if I need to know priors for this?)\n\n---\n(Page 117)\n\n### What are GPs used for in Astrophysics? (slides)\n*   Some physical models are very nearly gaussian\n    e.g. CMB G.L. on the sphere\n    > (e.g. damped random walks) or v. gaussian curves, overparameterisation\n*   \"**Nonparametric\" models**: flexible g.p.s to use when an accurate parametric astrophysically-motivated fn. is not available or is imperfect.\n    -   Nonparametric $\\implies$ no. of parameters / latent variables grows with the dataset.\n*   **Interpolation / Emulation**: to generate a smooth curve going through some observation or simulation points\n*   **Correlated noise / error model**: When you marginalise out the latent function, you are effectively accounting for noise/fluctuations correlated over time/space/wavelength $\\rightarrow$ \"**nuisance function**\"\n    > (goals sound similar: individual spectra from same stars, combined spectrum. down-sample, flux=f($\\lambda$), plus noise?)\n\nex1)\n> eg. disentangle signal from binary\n> GP models for stellar spectra\n> GP prior $\\rightarrow$ observe data & do inference. (which of the random functions best describes the observed data?)\n> [Sketch of a star with an orbiting planet. A light curve shows periodic dips. Another plot shows a more complex, noisy signal.]\n> (brightest when planet crosses front of star, get light from both, then planet crosses back of star so only get light from star)\n\nex2)\n> lensed quasar: brightness randomly fluctuates, on time which, behind galaxy, it will be lensed & light takes two copied paths such that an image will appear fixed time later than the other image\n> GP to assume underlying process, so we can said, time delays.\n> use GP to model underlying latent (true) light curve. e.g. a damped random walk.\n\nex3)\n> supernova can also be represented by some velocity, at some redshift.\n> not copying magnitude, by some value is a minimum stop between objects?\n> model light curves as realising GP. Introduce params to estimate $\\theta, \\beta$?\n\n---\n(Page 118)\n\n### G.P. as a prior on functions (slides)\n> e.g. fit GP to SN light curve time series\nBut we **only ever evaluate the fn. on a finite set of points**!\nA grid of times: $t = (t^1, ..., t^N)^T$\nA vector of fn. values evaluated on the time grid:\n$$ f = (f(t^1), ..., f(t^i), ..., f(t^N))^T $$\nAssume a **squared exponential kernel / cov. fn.**:\n$\\mathrm{Cov}[f(t), f(t')] = k(t, t') = A^2 \\exp(-(t-t')^2/\\tau^2)$\nAssume a **const. prior mean fn.**:\n$E[f(t)] = m(t) = C$\n> (often assume zero-mean C=0).\n\n**Prior on function**: $P(f|A,\\tau) = N(f|1c, K)$\n**Drawing from prior**: $f|A,\\tau \\sim N(1c, K)$\n**Cov. matrix K is populated by evaluating the kernel**:\n$\\mathrm{Cov}[f(t), f(t')] = K(t,t') = A^2 \\exp(-(t-t')^2/\\tau^2)$\nfor all pairs of points in t:\n$K_{ij} = k(t_i, t_j) = A^2 \\exp(-(t_i-t_j)^2/\\tau^2)$\n> how `cov.shod` kernel, set A,$\\tau$ to positive values & generate say 20 random fns from GP prior\n> [A small sketch showing several random functions drawn from a GP prior.]\n> (notice how prior change if we change $\\tau, A$)\n> hyperparameters $A, \\tau$ have physical significance & control statistical behaviour.\n\n---\n(Page 119)\n\n### Fitting a GP to data (slides)\n(1) **PREDICTION**\n    If we knew the characteristic scales of the kernel (hyperparameters) $(A, \\tau^2)$ then how do we fit the data at observed times to find the curve for unobserved times?\n(2) **MODEL SELECTION**\n    Given the observed data, how do we fit for the characteristic scales of the kernel (hyperparameters)?\n\n**Posterior inference w/ G.P.s, Estimating the underlying curve**:\n$f_0 = $ fn. at observed times $t_0$ (training set)\n$f_* = $ fn. at unobserved times $t_*$ (prediction set)\n**Joint**: $\\begin{pmatrix} f_0 \\\\ f_* \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} 1c \\\\ 1c \\end{bmatrix}, \\begin{bmatrix} K(t_0, t_0) & K(t_0, t_*) \\\\ K(t_*, t_0) & K(t_*, t_*) \\end{bmatrix} \\right)$\nPopulating the cov. matrix:\n$K(t,t')$ has $i,j$-th entry $= k(t_i, t_j)$\nUsing the assumed kernel fn.\n$\\mathrm{Cov}[f(t), f(t')] = k(t,t') = A^2 \\exp(-|t-t'|^2/\\tau^2)$\n> virtue of defining MVG vector partition\n\n---\n(Page 120)\n\n### Posterior (conditional on observations, is also Gaussian):\n$f_*|f_0 \\sim N(E[f_*|f_0], \\mathrm{Var}[f_*|f_0])$\n\n**Posterior predictive mean**:\n$$ E[f_*|f_0] = 1c + K(t_*,t_0)K(t_0,t_0)^{-1}(f_0-1c) $$\n**Posterior predictive variance / covariance**:\n$$ \\mathrm{Var}[f_*|f_0] = K(t_*,t_*) - K(t_*,t_0)K(t_0,t_0)^{-1}K(t_0,t_*) $$\n> your measurement error -> bad\n> get this to get by going through centre of every single data point.\n> include measurement error: good\n> (how do we account for this?)\n\n### Accounting for measurement error\n$y_0|f_0 \\sim N(f_0, W)$\n> \"Made of measured values & a gaussian process\"\n$y_0$ are measured values of $f_0$ at time $t_0$.\nat each observation $y_0^i|f_0^i \\sim N(f_0^i, \\sigma_i^2)$\nW is measurement cov. matrix. Most common case: **heteroskedastic uncorrelated measurement error**!\n$\\mathrm{Cov}(E_i, E_j) = W_{ij} = \\delta_{ij}\\sigma_i^2$\n\nMeasurement error model: (data = latent value + meas. error)\n$y_0 = f_0 + E$\n$E \\sim N(0, W)$\n(mean-zero Gaussian noise)\n> can make W\n\n---\n(Page 121)\n\n### [Derivation as the sum of two GPs at the observed times: Probabilistic Generative Model]\n**GP of intrinsic curve**\n$f(t) \\sim GP(m(t)=c, k(t,t'))$\n$f_0 = $ latent fn. at observed times $t_0$\nGP Prior: $f_0 \\sim N(1c, K(t_0, t_0))$\n\n**GP of measurement error**:\n$y_0 =$ measurements (with error) of $f_0$ at times $t_0$\n$y_0|f_0 \\sim N(f_0, W)$\nSame as:\n$y_0 = f_0 + E \\quad$ (mean-zero error) $E \\sim N(0, W)$\n\n**GOAL**: Need Joint Dist. of data and latent fn.\n> data $y_0$, $f_*$ for unobserved times\n> $(y_0, f_*) \\sim N \\left( \\begin{bmatrix} ? \\\\ ? \\end{bmatrix}, \\begin{bmatrix} ? & ? \\\\ ? & ? \\end{bmatrix} \\right)$\nThen can calc. fn. (posterior) prediction at unobserved points\n$f_*|y_0 \\sim N(E[f_*|y_0], \\mathrm{Var}[f_*|y_0])$\nUsing conditional properties of MV Gaussian as before.\n> this is the logic, my notebook has it slightly different\n$$ \\begin{pmatrix} y_0 \\\\ f_* \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} E[y_0] \\\\ E[f_*] \\end{bmatrix}, \\begin{bmatrix} \\mathrm{Cov}(y_0, y_0) & \\mathrm{Cov}(y_0, f_*) \\\\ \\mathrm{Cov}(f_*, y_0) & \\mathrm{Cov}(f_*, f_*) \\end{bmatrix} \\right) $$\n\n---\n(Page 122)\n\n**Intrinsic/Latent Process** $f_0 \\sim N(1c, K(t_0, t_0))$\n**Measurement Process** $y_0|f_0 \\sim N(f_0, W)$\n$y_0 = f_0 + E \\quad E \\sim N(0, W)$ (meas. error)\n\nTo find cov. submatrices, apply **bilinearity of covariance**\n$\\mathrm{Cov}(y_0, y_0) = \\mathrm{Cov}(f_0, f_0) + \\mathrm{Cov}(E, E) + 2\\mathrm{Cov}(f_0, E)$\n$\\mathrm{Cov}(f_0, f_0) = K(t_0, t_0) \\quad$ (GP of intrinsic curve)\n$\\mathrm{Cov}(E, E) = W \\quad$ (measurement noise)\n(the two processes are indep. $\\rightarrow$ uncorrelated)\n$2\\mathrm{Cov}(f_0, E) = 0$\n$\\therefore \\mathrm{Cov}[y_0, y_0] = K(t_0, t_0) + W$\n\nUse joint of latent $f$ at observed and unobserved times\n$$ \\begin{pmatrix} f_0 \\\\ f_* \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} 1c \\\\ 1c \\end{bmatrix}, \\begin{bmatrix} K(t_0, t_0) & K(t_0, t_*) \\\\ K(t_*, t_0) & K(t_*, t_*) \\end{bmatrix} \\right) $$\nTo derive similar arguments for\n$\\mathrm{Cov}[y_0, f_*] = \\mathrm{Cov}[f_0, f_*] + \\mathrm{Cov}[E, f_*] = K(t_0, t_*) + 0$\n$\\mathrm{Cov}[f_*, f_*] = K(t_*, t_*)$\n> remember $K$ symmetric\n\nFill out cov. matrix:\n$$ \\begin{pmatrix} y_0 \\\\ f_* \\end{pmatrix} \\sim N \\left( \\begin{bmatrix} 1c \\\\ 1c \\end{bmatrix}, \\begin{bmatrix} K(t_0, t_0) + W & K(t_0, t_*) \\\\ K(t_*, t_0) & K(t_*, t_*) \\end{bmatrix} \\right) $$\n\n---\n(Page 123)\n\nnow can compute fn. prediction at unobserved points:\n### Posterior Predictive (conditional on data $y_0$)\n$f_*|y_0 \\sim N(E[f_*|y_0], \\mathrm{Var}[f_*|y_0])$\n**Using MV Gaussian conditional properties**:\n**Posterior predictive mean**\n$$ E[f_*|y_0] = 1c + K(t_*, t_0)[K(t_0, t_0)+W]^{-1}(y_0-1c) $$\n**Posterior predictive variance/covariance**\n$$ \\mathrm{Var}[f_*|y_0] = K(t_*, t_*) - K(t_*, t_0)[K(t_0, t_0)+W]^{-1}K(t_0, t_*) $$\n> [Left image: A plot of noisy data points with a curve that passes exactly through each point. Labeled \"ignore meas. error... overfitting\". Right image: A plot with the same data points, now with error bars. A smooth curve passes through the error bars, not necessarily through the points themselves. Labeled \"include meas. error\". An arrow points to this plot with the note \"looks much like an actual supernova\".]\n> (see slides)\n\n---\n(Page 124)\n\nNow we know how to do (1) **PREDICTION**, how do we do (2) **MODEL SELECTION**?\n### Tuning hyperparameters (A, $\\tau$)\n\nRecall our probabilistic generative model:\n**Intrinsic/latent GP process** (make explicit dependence on hyperparameters)\n$$ f(t) \\sim GP(m(t)=C, K_{A, \\tau^2}(t,t')) $$\nkernel: $k_{A, \\tau^2}(t,t') = A^2 \\exp(-|t-t'|^2/\\tau^2)$\n$f_0 \\sim N(1c, K_{A, \\tau^2}(t_0, t_0))$\n**Gaussian measurement process** $y_0 = f_0 + E \\quad \\sim N(0,W)$\n> $(y|f_0, A, \\tau^2) = N(f_0, W)$. then $P(f_0|A, \\tau^2) = N(\\dots)$. then $P(y_0|A, \\tau^2) = \\int \\dots df_0$.\n\nIntegrating out the latent fn. $f(t)$ (or use addition of MVN R.V.s) gives us the **marginal likelihood**:\n> $P(y_0|A,\\tau^2, H_0) = \\int P(y_0,f_0|A,\\tau^2)df_0 = \\int P(y_0|f_0, A,\\tau^2) P(f_0|A,\\tau^2) df_0$\n> meas-error, latent GP\n> joint normal $P(y_0,f_0) = N(\\dots)$. marginalise $f_0$.\n$P(y_0|A,\\tau^2) = \\int N[y_0|f_0, W] \\times N[f_0|1c, K(t_0, t_0)] df_0$\n$$ L(A, \\tau^2) = P(y_0|A, \\tau^2) = N[y_0|1c, K_{A, \\tau^2}(t_0, t_0) + W] $$\n> W = meas. error covariance\n> $K_{A, \\tau^2}$ = GP covariance\n\nWhich we can **optimize** (max likelihood) or specify a **prior** on $(A, \\tau)$ and sample from posterior.\n> results for the example on slides\n> (1) find for the case that it actually is that we have no sign on the parameters\n\n---\n(Page 125)",
      "timestamp": "2025-10-31T15:25:38.152694",
      "tokens": 9744,
      "cost": 0.09745124999999999
    },
    {
      "role": "user",
      "content": "Continue and finish transcribing pages 126-166 (the final pages) of the astrostatistics lecture notes. Maintain the same markdown formatting style with LaTeX math, page markers, lecture headers, image descriptions, and marginal notes.",
      "timestamp": "2025-10-31T15:28:09.709438",
      "tokens": 87765,
      "cost": 0.131303125
    },
    {
      "role": "assistant",
      "content": "> remember we had a lensed quasar shifted by a time delay. each has its own shifted copies of latent function. how do we deal with this using GP?\n\n### Case study: Construct Bayesian Model\nOne latent fn. modelled as a draw from a GP\n$f(t) \\sim GP(c, k(t,t')) \\quad k(t,t')=A^2\\exp[-(t-t')^2/\\tau^2]$\nData of each image are measurements $(\\Delta t_i, \\Delta m_i)$-shifted copies of this latent function\nFor image S1 w/ observations $y_i$ at times $t_i$:\n$y_i(t_i) = f(t_i) + E_i \\quad E_i,j \\sim N(0, \\sigma^2_{ij})$ ($j$ indexes the observations at times $t_ij$)\n\nImages S#i=2,3,4 w/ observations $y_i$ at times $t_i$ w/ (unknown) time delays and magnitude shifts (relative to S1): $(\\Delta t_i, \\Delta m_i)$\n$i=2,3,4 \\quad y_i(t_i) = \\Delta m_i + f(t_i - \\Delta t_i) + E_i \\quad E_{ij} \\sim N(0, \\sigma^2_{ij})$\n\nSample Posterior: $P(\\Delta m, \\Delta t, A, \\tau^2|D)$\n> using MCMC.\n> e.g. results, sample w/ metropolis-within-gibbs, one parameter per example is needed\n> [Two small histograms are shown side-by-side, labeled '$\\Delta t_A$' and '$\\Delta t_B$'. The text reads \"your best estimate for each time delay\".]\n> Shifting by estimated $\\Delta t_i, \\Delta m_i$, datasets line up pretty well as hoped!\n\n---\n(Page 126)\n\n## Lecture 22\n_14.3.25_\n> gelman BDA ch 21,5\n\nToday: Finishing Gaussian processes, starting hierarchical Bayes / Probabilistic Graphical Models\n\n### Gaussian Processes cont.\n\nPreviously: used \"**squared exponential kernel**\"\n$\\rightarrow \\mathrm{Cov}[f(t), f(t')] = k(t,t') = A^2\\exp(-|t-t'|^2/\\tau^2)$\n$f|A,\\tau \\sim N(\\Delta c, K)$.\n> characteristic amplitude, characteristic timescale/correlation\nThis kernel gives very smooth curves (infinitely differentiable)\n> also has properties:\n**Stationary**: $k(t,t')$ invariant to time translation\n$t \\to t+k$, $t' \\to t'+k$\n> correlation only depends on displacement, not absolute position\n**Symmetric**: $k(t,t') = k(t',t)$\n\n### OTHER COV FUNCTIONS:\n\"**Ornstein Uhlenbeck Process**\" (damped random walk)\nExponential covariance function\n> often used to model AGN variability\n$$ k(t,t') = A^2\\exp(-|t-t'|/\\tau) $$\nLong-run dist. of stochastic differential eq.\n> mean-reversion d-r walk\n$d f(t) = \\tau^{-1}[\\mu-f(t)]dt + \\sigma dW_t$\n> long-term mean, mean-reversion timescale, volatility\n$A = \\tau\\sigma/2$\n> [A sketch of a jagged, random-walk-like path is shown.]\n> more jagged $\\rightarrow$ vs. squared exponential\nthis kernel is everywhere continuous but **not** differentiable\n> it won't be jagged if what you want is something else\n\n\"**Matern class**\" of cov. functions:\n> somewhere between smooth & jagged\n$\\nu=1/2$ special case of Matern kernel:\n$$ K_{Matern}(r) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}r}{L}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}r}{L}\\right) $$\nwith five parameters $\\nu, \\tau, L$, where $K_\\nu$ is a modified Bessel fn.\nWhen $\\nu = p+1/2$ is half-integer; exponential $\\times$ p-polynomial.\n$$ k_{\\nu=p+1/2}(r) = \\exp\\left(-\\frac{\\sqrt{2\\nu}r}{L}\\right)\\frac{\\Gamma(p+1)}{\\Gamma(2p+1)}\\sum_{i=0}^p \\frac{(p+i)!}{i!(p-i)!}\\left(\\frac{\\sqrt{2\\nu}r}{L}\\right)^{p-i} $$\n\"**Periodic cov. functions**\"\n> ex sheet 3\ne.g. $k(t,t') = A^2\\exp\\left(-\\frac{2\\tau^2}{L^2}\\sin^2(\\pi(t-t')/T)\\right)$\n> [Two sketches of periodic, oscillating functions are shown.]\n\n> End of GPs\n\n---\n(Page 127)\n\n### PROBABILISTIC GRAPHICAL MODELS (board)\n> conditional independence\n(Depicting complex probability distributions)\n\nSuppose a,b,c are r.v.s with a joint pdf:\n$P(a,b,c) = P(c|a,b)P(a,b) \\quad (\\text{general factorisation})$\n$= P(c|a,b)P(b|a)P(a)$\n\n**Directed Acyclic Graph**:\n[Image of a Directed Acyclic Graph (DAG) with three nodes: 'a', 'b', and 'c'. Arrows point from 'a' to 'c' and from 'b' to 'c'. 'a' and 'b' are labeled as 'parent'.]\n> DAG\n> nodes are connected by one-way edges that do not form any cycles.\n> why we care? very nice way of prescribing how to construct joint dist of a number of r.v.s\n\nGenerally, if $\\vec{x}$ is a collection of k r.v.s, then given a DAG, the factorisation is written as\n$$ P(\\vec{x}) = \\prod_{k=1}^K P(x_k | \\text{Parents of } x_k) $$\n> if $x_k$ has no parents then just $P(x_k)$\n\nEq)\n[Image of a DAG with 7 nodes, $x_1$ through $x_7$. Arrows point from $x_1$ to $x_2$, $x_2$ to $x_3$, $x_1$ to $x_4$, $x_2$ and $x_3$ to $x_5$, $x_4$ and $x_5$ to $x_6$, and $x_4$ and $x_5$ to $x_7$.]\n$$ P(x_1, ..., x_7) = P(x_1) P(x_2|x_1) P(x_3|x_2) $$\n$$ \\times P(x_4|x_1) P(x_5|x_2, x_3) P(x_6|x_4,x_5) $$\n$$ \\times P(x_7|x_4, x_5) $$\n(the lack of connections implies some model structure!)\n> we call this conditional independence. fact it isn't fully connected implies conditional independence (structure)\n\n---\n(Page 129)\n\n### Generative Models (slides)\n> imagine you want to simulate an image with lots of galaxies in it\n**Causal process for generating images**\n> no mixing\n[Image of a DAG. Nodes 'object', 'position', and 'orientation' all point to the node 'image'. Arrows are annotated: 'object' -> 'no galaxy or star, w/ prior prob dist', 'orientation' -> 'how elliptical? round galaxy?'.]\n> if I sample joint dist of (object, pos, orientation) can produce an image of galaxy/star. We think of this joint dist. as a graph (!?)\n\n### Notation (board)\n[A square or circle is shown.] open node = latent parameter/unobserved data\n[A shaded square is shown.] shaded node = observed parameter or data (conditioned on)\n[A filled dot is shown.] filled dot = fixed and known constant\n[A large rectangle labeled 'i=1...N' enclosing smaller nodes is shown.] plate: N independent replications of what's in side\n\n---\n(Page 130)\n\n(Eg) Representing bayesian relationships with graphs\nGaussian i.i.d. data $y_i \\stackrel{\\text{iid}}{\\sim} N(\\mu, \\sigma^2=1) \\quad i=1,...,N$\n[Image of a PGM. An open square node for $\\mu$ and a solid dot for $\\sigma^2=1$ point to a sequence of open square nodes $y_1, ..., y_N$.]\n$= $\n[Image of an equivalent PGM using a plate. An open square $\\mu$ and solid dot $\\sigma^2=1$ point to a plate labeled 'i=1...N' which contains an open square $y_i$.]\n$= P(\\mu, \\vec{y}) = P(\\mu)[\\prod_{i=1}^N P(y_i|\\mu)]$\n\nIf y observed, (shaded)\n[Image of a PGM. An open square $\\mu$ and solid dot $\\sigma^2=1$ point to a plate 'i=1...N' containing a shaded square $y_i$.]\n$\\propto P(\\mu|\\vec{y}) \\propto P(\\mu)[\\prod_{i=1}^N P(y_i|\\mu)]$\n\n(Eg) Both $\\mu, \\sigma^2$ unknown\n[Image of a PGM. Two open squares for $\\mu$ and $\\sigma^2$ point to a plate 'i=1...N' containing an open square $y_i$.]\n$= P(\\mu, \\sigma^2, \\vec{y})$\n$= P(\\mu)P(\\sigma^2)[\\prod_{i=1}^N N(y_i; \\mu, \\sigma^2)]$\n$P(\\mu, \\sigma^2)$\n\n---\n(Page 131)\n\n### Conditional Independence and Probabilistic Graphical Models\n\nLet a,b,c be r.v.s\n(Marginal) **Independence**\n$P(a,b) = P(a)P(b) \\rightarrow a \\perp b | \\emptyset$\n> (not conditioning on anything)\n\n**Conditional Independence**\n$P(a,b|c) = P(a|c)P(b|c) \\rightarrow a \\perp b | c$\ni.e. $P(a|b,c) = P(a|c)$\n> if I know c, then b gives me no more info about a.\n\n(Eg)\n[Image of a DAG. Node 'c' points to 'a' and 'b'.]\n$P(a,b,c) = P(c)P(a|c)P(b|c)$\n$P(a,b) = \\int P(c)P(a|c)P(b|c) dc \\neq P(a)P(b)$\n$\\rightarrow a \\not\\perp b | \\emptyset$\n> \"a is not indep of b marginally\"\n\n$P(a,b|c) = \\frac{P(a,b,c)}{P(c)} = P(a|c)P(b|c)$\n$\\rightarrow a \\perp b | c$\n> \"a is indep of b conditionally\"\n\n---\n(Page 132)\n\n(Eg)\n[Image of a DAG. Nodes 'a' and 'b' both point to node 'c'.]\n$P(a,b,c) = P(a)P(b)P(c|a,b)$\n$P(a,b) = P(a)P(b)\\int P(c|a,b) dc = P(a)P(b) \\cdot 1$\n$\\rightarrow a \\perp b | \\emptyset$\n> \"marginal independence between a & b\"\n\n$P(a,b|c) = \\frac{P(a,b,c)}{P(c)} \\neq P(a|c)P(b|c)$\n$\\rightarrow a \\not\\perp b | c$\n> i.e. by observing c, the child of a & b, you introduce a dependence, a,b are now correlated.\n> a is no longer indep of b if you condition on c.\n\n### D-separation Example (slides)\n> way to test what nodes are indep given other nodes\n> read off independence properties from these graphical models.\n[Image of a DAG. Arrows go from a to e, f to e, e to c, b to e, f to h.]\n$a \\not\\perp b | c$\n> path a->e->b is not blocked so $a \\not\\perp b | c$\n\n[Image of a second DAG. Arrows from a to e, e to c, b to e, g to f, f to e, f to h.]\n$a \\perp b | g$\n> path a->e<-b is blocked so $a \\perp b | g$\n\n---\n(Page 133)\n\n### D-separation (slides)\n\n*   A, B and C are non-intersecting subsets of nodes in a directed graph.\n*   A path from A to B is **blocked** if it contains a node such that either\n    a) The arrows on the path meet either **head-to-tail** ($\\rightarrow \\cdot \\rightarrow$) or **tail-to-tail** ($\\leftarrow \\cdot \\rightarrow$) at the node, and the node is in the set C, or\n    b) The arrows meet **head-to-head** ($\\rightarrow \\cdot \\leftarrow$) at the node, and **neither** the node, nor any of its descendants are in the set C.\n*   If **all paths** from A to B are blocked, A is said to be **d-separated** from B by C.\n*   If A is d-separated from B by C, the joint over all variables in the graph satisfies $A \\perp B | C$.\n\nexample: see slides (very brief, don't dwell on this in lecture)\n[A small diagram of a DAG with nodes 1-7 and several arrows.]\n$X_2 \\perp X_4 | X_1$\n$X_2 \\not\\perp X_4 | F$\n$X_2 \\not\\perp X_4 | F, G$\n\n---\n(Page 134)\n\n## Lecture 23\n_17.3.25_\n> way to think about statistics: how do I generate data? -> first step, this provides way to do so via probabilistic graphical models\n> what's the model for astronomical observations? very complex process and so is its PGM\n> data generating process: specify likelihood + prior. story starts with DGP. Gelman BDA.\n\nToday: Hierarchical Bayes / Bayesian Model Selection\n\n(e.g.) generative model (slides)\n[Image of a PGM. Nodes for 'object', 'position', 'orientation' (light source) all point to 'image'. These are in a plate labeled 'k sources'. The 'image' node points to 'pixel' in a plate 'j pixels'.]\n> data generating model for astronomical images. each light source giving brightness to each pixel.\n\nPGM linked to hierarchical bayesian models\n(e.g. surveys research is basically hierarchical bayesian model)\n**Supernovae**: complex, more complex hierarchical bayesian model\n[Image of a large PGM for supernovae. Nodes include: dist pop, R_v, T; dust effect $A_v^s, E_c^s$; intrinsic SN mag $\\mathfrak{M}_k, \\theta_k$; observed mag $m_{jk}^s$; distance $d_s$, redshift $z_s$; and cosmological parameters $\\Omega$. Plates for s=1...N_{SN} and k=1...N_{filters}.]\n> model: intrinsic properties, counting from population dist of these, parameters.\n> model spectral energy distribution SED: SEDs how brightness (flux) of an object varies with wavelength\n> how diff SN types have diff time scales vs spectra: trace it to variations in time.\n> slides: dummies, slides desiring\n> 3 SN -> N SN (plate)\n> stuff outside plate defines, pop distribution / cosmological parameters (things not related to individual SN) and stuff inside plate describes stuff related to individual supernovae.\n> [some handwritten formulas and arrows.]\n\n---\n(Page 135)\n\n### Common Problems in Astronomy (slides)\n> do not test on assignment, but for later reference. For future research.\n\n*   Want to learn about a **population** of objects from a **finite sample** of **individuals**, each measured with **error**.\n*   Observed data is actually a combination of uncertain **astrophysical** & **instrumental** & **selection effects**. Need to model them to infer the \"**intrinsic**\" properties of the object or population of objects (\"deconvolve\").\n\n### What is Hierarchical Bayes?\n\"simple\" Bayes: $D|\\theta \\sim \\text{Model}(\\theta)$\n[A simple PGM is shown: An open square $\\theta$ points to a shaded square D.]\nPosterior (Bayes' theorem): $P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$\n\nHierarchical Bayes: $\\theta_i$: parameter of individual\n$\\alpha, \\beta$: hyperparameters of population\n$D_i|\\theta_i \\sim \\text{Model}(\\theta_i)$\n$\\theta_i|\\alpha, \\beta \\sim \\text{Pop Model}(\\alpha, \\beta)$\n[A PGM is shown. Open squares $\\alpha, \\beta$ point to an open square $\\theta_N$, which in turn points to a shaded square $D_N$.]\n\nJoint posterior:\n$$ P(\\vec{\\theta}, \\alpha, \\beta | \\vec{D}, S) \\propto \\left[ \\prod_{i=1}^N P(D_i|\\theta_i) P(\\theta_i|\\alpha, \\beta) \\right] P(\\alpha, \\beta) $$\nbuild up complexity by layering conditional probabilities.\n> def. hierarchy: e.g. population level parameters, pop. dist. of host galaxy dist of systems. object level parameter, e.g. property of individual SN.\n\n---\n(Page 136)\n\n**Forward model**:\n[A PGM showing $\\alpha, \\beta$ pointing to $\\theta_N$, which points to $D_N$.]\n**Plate notation**: (loop over individuals in sample)\n[A PGM showing $\\alpha, \\beta$ pointing to a plate labeled 'i=1...N' containing an arrow from $\\theta_i$ to $D_i$.]\n\n### Advantages of Hierarchical Bayesian Models\nO. Common problem in astronomy: infer properties of population from finite sample of individuals with noisy measurements.\nO. Incorporate multiple sources of randomness & uncertainty as \"latent variables\" with distributions underlying the data.\nO. Express structured probability models adopted to data-generating process (\"forward model\").\nO. Bayesian: Full (non-gaussian) probability distribution = global, coherent quantification of uncertainties.\nO. Completely explore & marginalise posterior trade-offs / degeneracies between parameters / hyperparameters.\n\n---\n(Page 137)\n\n### Simplest Hierarchical Bayesian/Multi-level Model: \"Normal-Normal\" for Standard Candle Mag. (slides)\n$s=1,...,N$\n\n**Level 1**: Pop. distribution of latent variables (absolute mags.)\n> population dist / prior\n$M_s \\sim N(M_0, \\tau^2)$\n> latent variables (pop. means, pop. variance) -> hyperparams\n\n**Level 2**: Measurement error process\n> measurement likelihood\n$D_s|M_s \\sim N(M_s, \\sigma_s^2)$\n> measurements (data), heteroskedastic meas. error variance (known)\n\n**Joint probability density of data, latent variables, hyperparameters**\n> usually PGM for the joint\n$P(\\vec{D}, \\vec{M}, H = \\{M_0, \\tau^2\\})$\n[A PGM is shown. Open squares for $M_0, \\tau^2$ point to a plate 's=1...N'. Inside the plate, an open square $M_s$ points to a shaded square $D_s$. A solid dot for $\\sigma_s^2$ also points to $D_s$.]\n> joint factors into conditional and marginal pdfs based on model assumptions.\n\n**Joint probability density of all the things**: data, latent variables, hyperparameters\n$$ P(\\vec{D}, \\vec{M}, H) = \\left[ \\prod_{s=1}^N P(D_s|M_s) P(M_s|M_0, \\tau^2) \\right] \\times P(H) $$\n> measurement likelihood, population distribution/prior, hyperprior\n\n---\n(Page 138)\n\n### Putting the Bayesian in Hierarchical Bayesian\nJoint posterior of all unknowns given the data\n$$ P(\\vec{M}, H | \\vec{D}) = \\frac{P(\\vec{M}, H, \\vec{D})}{P(\\vec{D})} \\leftarrow \\text{ignorable normalisation const \"ignore until next time\"} $$\n(posterior on N+2 dim. parameter space)\n$$ P(\\vec{M}, H | \\vec{D}) \\propto \\left[ \\prod_{s=1}^N P(D_s|M_s) P(M_s|M_0, \\tau^2) \\right] \\times P(H) $$\n[The PGM from the previous page is shown again.]\n\n### Hierarchical vs Regular Bayes\n*   Could regard as just a general Bayesian inference problem in a very high dim. parameter space, e.g.\n    $\\theta = \\{M_1, ..., M_N, M_0, \\tau^2\\} = \\{\\vec{M}; M_0, \\tau^2\\}$\n    $P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$\n    $P(\\theta|D) \\propto P(D|\\vec{M})P(\\vec{M}|M_0, \\tau^2)P(M_0, \\tau^2)$\n    $P(\\theta|D) \\propto \\left[ \\prod_{s=1}^N P(D_s|M_s)P(M_s|M_0, \\tau^2) \\right] P(M_0, \\tau^2)$\n    > key pt. in bayesian inference, taking advantage of the structure of the problem.\n*   However, special hierarchical structure is useful for modelling, estimation & computation.\n*   For large N, wouldn't want to do N+2 dim. Metropolis MCMC!\n\n---\n(Page 139)\n\n### Gibbs Sampling & Hierarchical Bayes\n> can develop gibbs sampler in an efficient way\nUtilises conditional independence structure of PGM / posterior to derive conditional posterior densities.\n$P(\\vec{M}, H | \\vec{D}) \\propto \\left[ \\prod_{s=1}^N P(D_s|M_s)P(M_s|M_0, \\tau^2) \\right] \\times P(H)$\n\nGibbs: use $P(\\vec{M}, H|\\vec{D})$ instead to sample. Need to draw from conditional posteriors.\n1.  For $s=1,...,N$, Sample latent variables conditional on data and **hyperparameters**\n    $P(M_s|H, D_s) \\propto P(D_s|M_s) \\times P(M_s|M_0, \\tau^2)$\n    > N indep!\n    (conditional independence) (gaussian) (gaussian)\n    > product of 2 gaussians is gaussian -> easy to sample from, get a gaussian.\n2.  Sample hyperparameters from conditional on data and **latent variables**\n    $P(M_0, \\tau^2 | \\vec{M}, \\vec{D}) = P(M_0, \\tau^2|\\vec{M})$\n    > (conditional independence) (dropping $\\vec{D}$ from graph!)\n    (gaussian) (inv-$\\chi^2$)\n    $= P(M_0|\\tau^2, \\vec{M}) P(\\tau^2|\\vec{M})$\nReduces to the familiar posterior for unknown mean and variance of gaussian data (sheet 2 prob. 1, BDA 3.2-3.3)\n> for data (condition on D always).\n> input some guesses for $M_0, \\tau^2$ and condition on that, sample latent variables. turns out, that condition on H, M of SN1 is indep to M of SN2,3 etc.\n> so samples to two terms rather than N+2 terms. which depend on so can keep them. each s which is easy to sample from b/c gaussian\n> then cut through conditional relationships of graph.\n> so don't have to worry about data, only these N factors. but these look like gaussian. which only factors into a gaussian. & $1/x^2$ -> much easier to sample from.\n> w/ unknown $\\mu, \\sigma^2$?\n\n---\n(Page 140)\n\n**Full posterior**:\n[PGM is shown for the full posterior, as before.]\n$P(\\vec{M}, H|\\vec{D}) \\propto [\\prod P(D_s|M_s)P(M_s|M_0, \\tau^2)] \\times P(H)$\n1. [A similar PGM where $M_0, \\tau^2$ are shaded, indicating conditioning.]\n   $P(M_s|M_0, \\tau^2, \\vec{D}) \\propto P(D_s|M_s) P(M_s|M_0, \\tau^2)$\n   > $s$ indep.\n   > $P(M_s|M_0, \\tau^2, \\vec{D}) = P(M_s|M_0, \\tau^2, D_s)$\n   > gaussian $\\times$ N(M_s|M_0, $\\tau^2$)\n2. [A PGM where $M_s$ is shaded.]\n   > hyperprior\n   > $P(M_0, \\tau^2 | \\vec{M}, \\vec{D}) = P(M_0|\\tau^2, \\vec{M}) P(\\tau^2|\\vec{M})$\n   $P(M_0, \\tau^2|\\vec{M}) \\propto P(\\vec{M}|M_0, \\tau^2) \\alpha$ !\n   $= P(M_0|\\tau^2, \\vec{M})P(\\tau^2|\\vec{M})$ gaussian inv-$\\chi^2$\n   (similar to sheet 2 problem 1)\n   > \"think about this on your own\"\n**Hyperprior** $P(M_0, \\tau^2) \\propto 1$\n**Draw from**\n$\\tau^2|\\vec{M} \\sim \\text{Inv-}\\chi^2\\left(N-3, \\frac{(N-1)}{N-3}S^2\\right)$\n$M_0|\\tau^2; \\vec{M} \\sim N(\\bar{M}, \\tau^2/N)$\n$\\bar{M} = \\frac{1}{N}\\sum M_s \\quad S^2 = \\frac{1}{N-1}\\sum (M_s-\\bar{M})^2$\n\n(e.g.) 100 SN w/ distinct measurements\n> see slides\n> $D_s = \\hat{m}_s - \\hat{\\mu}_s$ data with some error\n[A histogram labeled 'obs abs mag $D_s$'. It is wide.]\n[A smaller histogram labeled 'the individual SN calc shallow abs mag' showing a narrow distribution.]\n> can make histogram of obs data\n> gibbs sampling sample $M_0, \\tau^2 | M_s$\n> in 10^2 dim. magnitude val. I don't see.\n[Two histograms from Gibbs sampling are shown. Left: '$\\mu_0$ marginal posterior estimate'. Right: '$\\tau$ marginal posterior estimate'.]\n[A scatter plot labeled 'scatter plot'. Points are scattered.]\n> given noisy estimates, by doing this hierarchical Bayes, can infer truth on population from data on spread (line-of-sight data spread tells you something about measurement error, diluting effect these parameters).\n> observed individual estimates have unseen spread from population dist and measurement noise.\n> partial pooling population parameters.\n\n---\n(Page 141)\n\n### HB Models, important and powerful feature: Partial Pooling, \"Shrinkage\" & \"Borrowing of Strength\" (slides)\n> each individual object $D_i$, individual estimate $\\hat{\\theta}_i$\n\n*   **Common sense procedure**:\n    O. Analyse each individual object's data $D_i$ separately and get each individual MLE estimate (with error).\n    O. Plug in all $\\hat{\\theta}_{MLE,i}$'s to estimate population hyperparameters.\n*   **PROBLEM**: Each individual $D_i$ estimate may be unbiased but collectively give a **biased** estimate of population (e.g. variance) because of errors.\n*   **SOLUTION**: Use HB to model & infer individuals & population **simultaneously** and get better estimates of **both**.\n\n> notes to self: (attempt to develop intuition)\n> HB uses posterior $\\rightarrow P(M_s|M_0, \\tau, D) \\propto P(D_s|M_s)P(M_s|M_0, \\tau^2)$\n> individual estimate, pop dist. acts as prior\n> so we went to infer $M_s$, alls and take peak as our best estimate of $M_s$?\n> individual estimate's just prior which is pop dist for observing data.\n> *but* HB model full posterior incorporates prior on $M_s$ so e.g. MAP estimate of $\\hat{M_s}$ will be very unlikely that this will sway our estimate of $M_s$ through its prior... balance between maximising prob of data but also probability of intrinsic value may be existing in the population in the first place.\n> how is this diff to MAP?\n> In part of HB get $P(M_s|D, H)$, so can infer these hyper-params instead of needing to have them ourselves (I think).\n\n---\n(Page 142)\n\n### Shrinkage Estimators (slides)\n> \"est not unbiased but their biases helps you get it away\"\n> closer to truth on avg than considering individual estimates for each individual in isolation.\n*   **Bias** estimator of individual towards the **population**\n*   Leads to overall **lower MSE** than individual unbiased estimators\n*   Allows \"**sharing of information**\" between individuals to improve overall estimation\n> example: slides (James-Stein explanation)\n\n### Shrinkage with Hierarchical Model\n\nReturn to our previous example:\n**Level 1**: pop. dist. of latent variable\n$M_s \\sim N(M_0, \\tau^2)$ \"pop. dist. / prior\"\n**Level 2**: measurement error process\n$D_s|M_s \\sim N(M_s, \\sigma_s^2)$ \"measurement likelihood\"\nIndividual MLE of SN s alone: $\\hat{M}_s = D_s$ (unbiased)\n> conditional on true value $M_s$, $\\hat{M}_s = D_s$\nPopulation dist. acts as prior\n$P(M_s|M_0, \\tau; D_s) \\propto P(D_s|M_s)P(M_s|M_0, \\tau^2)$\nand **pulls posterior estimate** of individual **closer to population mean estimate**.\n> to see this much we shrink the ests. of individual towards pop. mean estimate\n\n**Pull/shrinkage controlled by population variance** $\\tau^2$.\n> full posterior:\n> $P(M_s, M_0, \\tau|D) \\propto P(H)[\\prod P(D_i|M_s)P(M_s|M_0, \\tau^2)]$\n> just ignore terms not in $M_s$, then in simple gaussian example:\n> $P(M_s|M_0, \\tau, D_s) \\propto P(D_s|M_s)P(M_s|M_0, \\tau^2)$\n\n---\n(Page 143)\n\n### How does HB implement shrinking?\nexample (dataset of 8 SN)\n[A plot shows 8 vertical error bars (estimates for $M_s$) along an x-axis labeled 'SN'. The y-axis is labeled 'abs mag'. A horizontal line shows $\\hat{M}_s=D_s$.]\nIf you knew $M_0, \\tau$ of individual $M_s$. How does it depend on $\\tau$?\n[A plot shows several curves starting at individual estimates $\\hat{M}_s=D_s$ at $\\tau=0$ and converging to a common value $M_0$ as $\\tau$ increases. The y-axis is $E(M_s|M_0, \\tau, D)$. The curves are labeled \"conditional posterior means\".]\n> we can take $\\tau$ to zero; converges to $M_0$\n> $\\tau$ controls degree of shrinkage toward $M_0$ (increase $\\tau$, wider ranges are probable so more spread)\n> one curve for each SN. posterior mean of $M_s|M_0, \\tau$\nbut we don't know $M_0, \\tau$: $P(M_0, \\tau|D) = P(M_0|\\tau, D) P(\\tau|D)$\n> jointly marginalising out individual $M_s$\n[Two plots are shown below. Left: A plot of the posterior $p(M_0|\\tau,D)$ vs $M_0$, showing curves that sharpen as $\\tau$ increases. Right: A plot of the posterior $p(\\tau|D)$ vs $\\tau$, showing a peak at some $\\tau > 0$.]\n> posterior mean of $M_0|\\tau, D$\n> that curves shrink towards population mean with decreasing $\\tau$.\n> hierarchical bayes gives the posterior density $P(\\tau|D)$ -> way to estimate $\\tau$ (don't we require to have point estimates of D, M_0, $\\tau$? to estimate?)\n> [A final plot shows the conditional posterior mean $E(M_s|M_0, \\tau, D)$ vs $\\tau$. The on avg HB estimate is closer to truth than individual MLE.]\n> how can we work out best trade-off between shrinking towards same pop value vs allowing them to be independently estimated? at about $\\tau = 0.134$.\n\n---\n(Page 144)\n\n## Lecture 24\n_19.3.25_\n> recommend reading (Will's book!) MacKay Information theory, inference & learning algorithms ch. 28\n\nToday: **Bayesian Model selection**\n\n### Model Comparison & selection\n> nuances: \"just test if\" inference.\n> what's assumes model true & we just estimate parameters of the data ($P(\\theta|D)$).\n> 2nd level: what model generated the data? ($P(M|D)$)\n> 3rd level: \"what's the best model?\" Bayesian\n\n$\\rightarrow$ **No. of spectral lines** in a (noisy) spectrum?\n$\\rightarrow$ **Clustering / mixture models** - how many clumps?\n$\\rightarrow$ **Time series - curve fitting**\n- Is there a trend?\n- Complexity / order / degree of best model.\n- which G.P. kernel best explains the data?\n$\\rightarrow$ **Cosmology - standard (8-parameter) cosmological model** vs more exotic (more parameters) models?\n> is it standard model ($\\Lambda$CDM)? but people looking for evidence of more exotic variations of the model (trying and figure out dark energy) but then we need to introduce more parameters\n> (1) which extra parameters are warranted by data\n> (adding more params will always give better fit but how do we know if adding extra params is just fitting to noise, or actually warranted by data?) $\\rightarrow$ hope to answer through Bayesian model comparison\n\n---\n(Page 145)\n\n### Bayesian Model Comparison\n**Parameter estimation**: posterior on parameters\n$$ P(\\theta|D, M_1) = \\frac{P(D|\\theta, M_1)P(\\theta|M_1)}{P(D|M_1)} $$\n**Model selection**: posterior on models\n$$ P(D|M_1) = \\int P(D|\\theta, M_1)P(\\theta|M_1) d\\theta_1 $$\n> evidence or marginal likelihood\n> needs proper prior! can't just be improper\n\n**Posterior odds ratio**:\n> \"hypothesis test comparing two models\"\n$$ \\frac{P(M_1|D)}{P(M_2|D)} = \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{\\text{BF}} \\times \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\substack{\\text{Prior} \\\\ \\text{odds}}} $$\n> prob important for model selection vs scientists based on physical preference of prior probabilities.\n**Bayes Factor**$_{12} = \\frac{P(D|M_1)}{P(D|M_2)}$\n\n---\n(Page 146)\n\n> let's develop some intuition\nFor a given model with a single parameter, $\\omega$, consider the approximation\n$$ p(D) = \\int p(D|\\omega)p(\\omega)d\\omega \\approx p(D|\\omega_{MAP}) \\frac{\\Delta\\omega_{posterior}}{\\Delta\\omega_{prior}} $$\nWhere the posterior is assumed to be sharply peaked.\n[Image of two distributions on an x-axis labeled $\\omega$. A wide, flat 'prior' distribution $\\Delta\\omega_{prior}$ and a narrow, peaked 'posterior' distribution $\\Delta\\omega_{posterior}$ centered at $\\omega_{MAP}$.]\n> cotton approximation.\n> evidence is area of posterior.\n> $P(D) = \\int P(\\omega|D) P(D) d\\omega = P(D)$\n> $P(\\omega|D)P(D) = P(D|\\omega)P(\\omega)$\n> $P(D) = \\frac{P(D|\\omega)P(\\omega)}{P(\\omega|D)}$\n> $P(D) = P(D|\\omega_{MAP}) \\frac{P(\\omega)}{P(\\omega|D_{MAP})}$\n> $\\approx P(D|\\omega_{MAP}) \\frac{\\Delta\\omega_{prior}}{\\Delta\\omega_{posterior}}$\n> $\\approx P(D|\\omega_{MAP}) \\frac{\\text{width prior}}{\\text{width post}}$\n> no, this is inverted.\n\nTaking logarithms, we obtain\n$$ \\ln p(D) \\approx \\ln P(D|\\omega_{MAP}) + \\ln\\left(\\frac{\\Delta\\omega_{posterior}}{\\Delta\\omega_{prior}}\\right) $$\n> always negative\n> suppose we have a problem with priors\nGeneralize with M parameters, all assumed to have the same ratio $\\Delta\\omega_{posterior}/\\Delta\\omega_{prior}$, we get\n> related to fit of data under best-fit parameters\n> term penalises for complexity\n$$ \\ln p(D) \\approx \\ln P(D|\\omega_{MAP}) + M \\ln\\left(\\frac{\\Delta\\omega_{posterior}}{\\Delta\\omega_{prior}}\\right) $$\n> negative\n> **OCCAM FACTOR**\n> this term increases in complexity\n> (smaller models fit data better)\n> if model has many parameters, adding more params will always fit better data, but this model's evidence is not always improved, because we would be overfitting to noise, because of the second (end term) penalizing overfitting.\n> want to find sweet spot, enough params will fit targetting data.\n\n---\n(Page 147)\n\n### Matching data and model complexity\n[Image of a plot with x-axis 'D' (data) and y-axis 'P(D)'. Three bell-shaped curves are shown, labeled $M_1, M_2, M_3$. The curves represent $P(D|M_1), P(D|M_2), P(D|M_3)$ respectively. A vertical line at $D_0$ intersects the curves, indicating the evidence for each model given that data. $M_1$ is wide and short, $M_2$ is narrower and taller, $M_3$ is very narrow and short.]\n> more complex models can predict a greater variety of data sets\n> e.g. M1 can only predict limited amounts of data (e.g. linear model)\n> M2 can predict slightly more complex data sets (e.g. maybe a quadratic)\n> M3 e.g. if a neural network may be able to predict arbitrary functions\n> $P(D|M)$ is sets a distribution over potential data sets\n> the more complex data sets a model can predict, the more \"spread out\" it will be. It is normalised -> so any particular dataset it will have a low probability overall.\n> so simple models will have more limited range of data sets they can predict, but, because of that, overall $P(D)$ is higher.\n> whereas, complex models predict greater range but $P(D)$ low b/c of that normalisation factor (paying a price)\nso this gives a good way to weigh for eg models to dataset vs their overall complexity.\ne.g. for dataset $D_0$, $M_1$ is v. low probability. $M_3$ is similarly low probable but $M_2$ has best balance between complexity of model (more complex than $M_1$), but also not too complex. ($M_2$)\n\n---\n(Page 148)\n\n> recommend read ch 28 MacKay\n\n### Marginalising Joint Distribution of Parameters and Data\nhypothesis H\n($\\omega \\sim P(\\omega|H)$ & $D \\sim P(D|\\omega, H) \\rightarrow (D,\\omega) \\sim P(D, \\omega|H)$)\nI'm going to call the model, M\n($\\theta \\sim P(\\theta|M)$ & $D \\sim P(D|\\theta, M) \\rightarrow (D,\\theta) \\sim P(D, \\theta|M)$)\n\n[An image with multiple plots. Top left: Three curves $P(D|M_1), P(D|M_2), P(D|M_3)$ vs D, similar to the previous page. A horizontal dashed line represents $D_{obs}$.]\n[Top right: Three scatter plots showing samples from the joint distribution $P(D,\\theta|M)$ for models $M_1, M_2, M_3$.]\n[Bottom row: Three curves showing the marginal distribution $P(\\theta|M)$ for each model.]\n[Far right: Three curves showing the posterior distribution $P(\\theta|D,M)$ for each model.]\n> what likelihood is being used just for sampling distribution\n> draw $\\theta$ from prior, then given $\\theta$, draw D from sampling distribution $\\rightarrow$ this gives $\\theta$ and D pairs from the joint distribution. e.g. $P(D,\\theta|M_1)$.\n> To get joint data points they're samples from joint. Observed data set here is just a single vertical slice through (log dotted line).\n> set line \"graph mode\"\n> draw from prior $P(\\theta|M_1)$, draw from sampling dist $P(D|\\theta,M_1)$. gives pairs D, $\\theta$ joint!\n> [A small sketch showing height distribution.]\n\n---\n(Page 149)\n\n> used to be director of IoA\n### Interpreting the Bayes Factor / Evidence Ratio: The Jeffreys scale\n$$ \\Delta \\ln E = \\ln BF $$\n> these interpretations are up for dispute, not rigorous.\n| $\\Delta \\ln E$ | |\n|---|---|\n| $\\Delta \\ln E < 1$ | \"not worth more than a bare mention\" |\n| $1 < \\Delta \\ln E < 2.5$ | \"significant\" |\n| $2.5 < \\Delta \\ln E < 5$ | \"strong to very strong\" |\n| $5 < \\Delta \\ln E$ | \"decisive\" |\n\n### How to calculate Evidence $P(D) = \\int P(D|\\theta)P(\\theta)d\\theta$\n*   **Analytic** (really simple, nice problems)\n    > analytic integral\n*   **Laplace approximation**\n*   **Savage-Dickey Ratio** (Nested Models)\n    > (see later)\n*   **Monte-Carlo**: probably slow if likelihood is peaked\n    > potentially approx since only few samples have high likelihood\n    $\\theta_i \\sim P(\\theta)$, $P(D) \\approx \\frac{1}{m}\\sum_{i=1}^m P(D|\\theta_i)$\n    > approximate with MC integral of likelihood\n*   **Harmonic Mean Estimator** (unstable)\n    > large variance\n    $\\theta_i \\sim P(\\theta|D)$, $P(D) \\approx \\left[ \\frac{1}{m}\\sum_{i=1}^m \\frac{1}{P(D|\\theta_i)} \\right]^{-1}$\n    > MC avg of inverse likelihood over the posterior.\n*   **Nested Sampling**\n\n---\n(Page 150)\n\n### Calc. evidence via Laplace approx:\n(Recall: The Laplace Approximation)\nEvidence $P(D) = \\int P^*(\\theta|D)d\\theta$\nUnnormalised posterior $P^*(\\theta|D) = P(D|\\theta)P(\\theta)$\nFind MAP estimate: $\\theta_0 = \\mathrm{argmax} \\ln P^*(\\theta|D)$\nTaylor expansion:\n$\\ln P^*(\\theta|D) \\approx \\ln P^*(\\theta_0|D) - \\frac{1}{2}(\\theta-\\theta_0)^T A (\\theta-\\theta_0) + ...$\nHessian at mode: $A_{ij} = -\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\ln P^*(\\theta|D)|_{\\theta=\\theta_0}$\n$$ P^*(\\theta|D) \\approx P^*(\\theta_0|D) \\times \\exp(-\\frac{1}{2}(\\theta-\\theta_0)^T A (\\theta-\\theta_0)) $$\nNot always accurate:\ne.g.\n[Left image: A plot of $-ln P^*(\\theta|D)$, showing the true curve and a parabolic approximation. Right image: A plot of $P^*(\\theta|D)$ showing the true posterior and a Gaussian Laplace approximation.]\n> topside-down unnormalised posterior, laplace approx is a gaussian.\n*Evidence from Laplace Approximation*:\n$P(D) = \\int P^*(\\theta|D) d\\theta \\approx P^*(\\theta_0|D) \\times \\int |2\\pi A^{-1}|^{1/2} N(\\theta|\\theta_0, A^{-1})d\\theta$\n$P(D) \\approx P(D|\\theta_0) \\times P(\\theta_0) \\times \\det(A/2\\pi)^{-1/2}$\n$P(D) \\approx P(D|\\theta_0) \\times P(\\theta_0) \\times \\det(A/2\\pi)^{-1/2}$\n> having narrow posterior\n$A = -\\nabla\\nabla \\ln P(\\theta|D)$\n> unnormalised? need to be able to approximate posterior almost 2x to do this w/ only hessian from unnorm posterior\n\n---\n(Page 151)\n\n### Evidence implements Occam's Razor!\n$$ P(D) \\approx \\underbrace{P(D|\\theta_0)}_{\\substack{\\text{best fit} \\\\ \\text{likelihood}}} \\times \\underbrace{P(\\theta_0) \\times \\det(A/2\\pi)^{-1/2}}_{\\text{Occam factor}} $$\n> (? or just part of it?)\n$\\Sigma_{post} \\approx A^{-1}$ roughly\nhow can we see this? Suppose prior is Gaussian $P(\\theta)=N(\\theta|\\theta_{prior}, \\Sigma_{prior})$\n> assume prior centered near MAP to do this 'cheat yet?'\nOccam Factor:\n$$ \\propto |2\\pi \\Sigma_{prior}|^{-1/2} \\times |2\\pi \\Sigma_{post}|^{1/2} $$\n$$ \\propto \\frac{|\\Sigma_{post}|^{1/2}}{|\\Sigma_{prior}|^{1/2}} \\quad (\\text{ratio of posterior to prior width}) $$\n[Image of two distributions on the same axis. A wide, flat 'prior' and a narrow, peaked 'post'erior.]\n> from inference book: factor by which our model's hypothesis space collapses when the data arrive. magnitude of this factor is a measure of complexity of the model. depends on both the prior & the data! prior probability assigns to those params\n> more params -> multiply prior together -> more complex model, smaller evidence\n> [Two plots of $P(D|M)$ vs M, one for a simple model and one for a complex model.]\n> simple models concentrate probability over a limited no. of datasets.\n> complex models must make image of distributed 'best' probability of a given go but will be smaller in evidence of both. simple: 'ex' complex model can predict data, complex, will have much smaller prob of.\n\n---\n(Page 152)\n\n### Savage-Dickey Ratio\n> for nested models?\n> useful method that simple model is special case of a more complex model.\n\nSuppose the parameters are $\\phi, \\psi$ and complex model $M_1$ reduces to simpler model $M_0$ when $\\psi=0$ (Nested)\n> e.g. cosmological params\n> if $\\psi=0$ then we get simple params ($\\Lambda$CDM?)\nAnd the prior is separable:\n$P(\\phi, \\psi|M_1) = P(\\psi|M_1)P(\\phi|M_1)$\n> nuisance parameter (hyperparam?) -> see params in common model and the prior on $\\phi$ is the same (unknown)\n> e.g. for $\\Lambda$CDM, $w=-1, \\Omega_k=0$\n\nAnd the prior on $\\phi$ is the same for each model:\n> prior on nuisance parameters\n$P(\\phi|M_1) = P(\\phi|M_0)$\nThen the Bayes Factor reduces to:\n$$ B_{01} = \\frac{P(D|M_0)}{P(D|M_1)} = \\left. \\frac{P(\\psi|D, M_1)}{P(\\psi|M_1)} \\right|_{\\psi=0} $$\n> Show this:\n> posterior numerator of $P(D|M_0) = \\int P(D|\\phi, M_0)P(\\phi|M_0)d\\phi$. $P(D|M_1) = \\int P(D|\\phi, \\psi, M_1) \\dots$\n> $P(\\psi=0|D,M_1) = P(\\psi=0|\\phi, M_1)P(D|\\phi,M_1)/P(D|M_1)$\n> numerator becomes $P(D|M_0)$, denominator $P(D|M_1)$\n[Image showing several plots of distributions vs a parameter $\\theta$. Pairs of prior and posterior distributions are shown for models $M_1, M_2, M_3$. The overlap between prior and posterior for a specific value relates to the evidence.]\n> from internet, from book. how does this relate to model complexity?\n> For this data, evidence of model $M_2$ is greater than for $M_1$ because overall for $M_1$ is greater\n> M1 has high density for simple model but has zero density at the true values for the best fit for this data, with a complex model.\n\n---\n(Page 153)\n\n### NESTED SAMPLING\n> for evidence calculation but as a by-product you get posterior samples!\n*   An algorithm designed to compute Bayesian evidence (John Skilling, 2004)\n*   Can be better than MCMC for multi-modal distributions\n*   Get weighted samples from posterior for free\n*   Evolve an ensemble of \"live points\" successively sampling prior volume above a likelihood-level constraint\n*   Perform evidence integral over contours of equal likelihood\n*   Uses statistical estimate of prior mass within likelihood-level\n\n---\n(Page 154)\n\nWant to calculate multi-dim. evidence integral\n$$ Z = \\int L(\\theta)\\pi(\\theta)d\\theta $$\nDefine \"prior mass enclosed above likelihood $L^*$\"\n$$ X(L^*) = \\int_{L(\\theta)>L^*} \\pi(\\theta)d\\theta $$\nInverse: Likelihood at enclosed prior mass: $L(X)$\n1D evidence integral: $Z = \\int L(X) dX$\n> L is non-increasing value (from X=0) starts high, monotonic?\n[Image of two plots. Left: Contour plot in a 2D parameter space $\\Theta$, showing nested likelihood contours for $\\theta_1, \\theta_2, \\theta_3$. Right: A plot of Enclosing Likelihood L vs Enclosed Prior Mass X. The curve L(X) is monotonically decreasing. The area under the curve is the evidence integral, which is shown partitioned into vertical strips.]\nNested likelihood contours sort to enclosed prior mass X.\n> eg. 2D example take 3 random points from prior $\\theta_1, \\theta_2, \\theta_3$ and calc their likelihood, each will be associated with some likelihood contour. each contour is associated with some prior vol $X(\\theta)$ and also a prior volume $L(X)$.\n> So can associate with each point a likelihood contour and also a prior mass enclosed above that likelihood.\n> If we can map from points in param. space to $L(X)$, I can turn a previously multi-dim integral in param space to a 1-Dim integral in x from 0 to 1.\n\n---\n(Page 155)\n\n### Nested Sampling: How?\n> how do we achieve this?\n[NB: For the rest of this lecture, assume prior is uniform box in parameter space (w/ total integral 1)]\n1.  Begin by sampling $N_{live}$ points $\\{\\theta_i\\}$ from the prior ($X_0=1$), evaluate likelihood for each live point, initialise evidence $Z=0$.\n2.  Find the live point with smallest likelihood $L^*$.\n3.  Estimate compression factor\n    $t_i = X_i/X_{i-1}$.\n    > volume of estimated statistical $t_i \\in C_i, N_{live}$\n4.  Accumulate evidence as a 1D integral\n    $\\Delta Z = L^*(X_{i-1}-X_i) = L^*(1-t_i)X_i$.\n5.  Sample a new live point from prior constrained to be $L > L^*$, compute its likelihood value.\n    > (e.g. with MCMC or otherwise) easy to draw from constraint, reject anything not within it. sample until accept.\n6.  Repeat steps 2-5 until convergence (i.e. evidence Z stops changing within some tolerance).\n7.  Add final evidence estimate of remaining live points\n    $\\Delta Z = \\bar{L} \\times X_{end}$.\n8.  List of dead points $\\{\\theta_i\\}$ give weighted posterior samples\n    $w_i \\propto (1-t_i)X_i$.\n\n---\n(Page 156)\n\n> want to find Z, instead of dividing into cubes & summing over L for each cube, can we get more clever with smaller likelihood contour & project poster to some reality not about digressions in other regions?\n[Image of a 2D parameter space $(\\theta_1, \\theta_2)$ with a grid, illustrating naive integration.]\n> naive: sum over cubes and add them up. NOT in huge param spaces. this is very computationally expensive.\n\n[Image of the same space with nested likelihood contours.]\n> nested samples: number cubes similar likelihood & summing over these. to perform same integral by integrating over contours of equal likelihood & multiply by prior volume inside each contour value.\n[A diagram shows how nested contours in the $(\\theta_1, \\theta_2)$ space map to the L(X) plot, illustrating the transformation to a 1D integral.]\n> nested sampling on a 1D problem\n\ncompression in 1 iterate of nested sampling:\n[A diagram showing a line from 0 to 1 representing X. Several 'x's are marked on it, representing 'uniformly distributed live points'. The leftmost 'x' is circled and labeled 'remove worst'. A new 'x' is added, labeled 'draw replacement'. This represents one step of compression.]\n$t_i \\approx e^{-1/N_{live}}$\n\n---\n(Page 157)\n\n### Estimate Compression Factor\n> tricky bit -> can't actually see the contours of equal likelihood space -> so we use statistical argument to estimate what they are.\n*   Don't actually know $\\{X_i\\}$ exactly, but can use statistical argument (to estimate!).\n*   At each iteration, live points are **uniformly** distributed between $(0, X_{i-1})$.\n    > lowest likelihood points\n*   $X_i$ (lowest likelihood) in current iteration, is **highest X** of $N_{live}$ uniform random values.\n    > max of $N_{live}$ values drawn from uniform dist on $[0, X_{i-1}]$ is distributed as $X(N_{live})$. therefore its relative value...\n*   Therefore compression factor $t_i = X_i/X_{i-1}$ has **beta($N_{live}, 1$) distribution**, i.e.\n    $$ P(t_i) = N_{live} t_i^{N_{live}-1} $$\n    > highest root of these $N_{live}$ samples has beta dist\n*   Can approximate with mean, i.e.\n    $\\ln t_i \\approx \\langle \\ln t_i \\rangle = -1/N_{live}$\n    > $E[\\ln t_i] = \\int_0^1 N_{live} t_i^{N_{live}-1} \\ln t_i dt = \\dots = -1/N_{live}$\n    or $t_i \\approx e^{-1/N_{live}}$\n\n---\n(Page 158)\n\n### Nested sampling Demo:\nCartoon: **One nested sampling iterate ($N_{live}=4$)**\n[Image shows two circles representing parameter space. Left: \"iterate i-1\" with 4 points inside a contour. Right: \"iterate i\" with a smaller contour. One point from the previous step is labeled 'old/dead' and is outside the new contour. A new point has been added inside. The text \"record dead points\" points from the old point.]\n\nDemo: 2D gaussian, uniform prior on some range (actually cover gaussian).\n[Three plots are shown. Left: \"Sample 1000 live points\" showing a scatter plot of points. Middle: \"accumulate evidence\", a plot of cumulative evidence vs iteration, which rises and plateaus. Right: A plot of \"enclosed prior volume\" and \"likelihood level\" vs iteration.]\n> know analytic sol (??) see lecture\n> this is very accurate.\n> Z run see nested sampling approaches observed Z (?)\n> as no. of live points reduced & live points kept remaining, likelihood increases.\n> (if $g$ true, is=0)\n> ends up not accumulate much but accumulate most where points have similar values in middle (whereas at either end, either small or cores do not matter, so accumulation is shaped like that).\n> 'mc' slides 'begin with small no. of live samples, then have it go up, yes.\n\n---\n(Page 159)\n\n> actually we realised the most basic nested sampling algorithm, but there are more!\n> our way gets slow down w/ greater dimensionality. we are trying to sample from larger space.\n\n### Nested Sampling:\n(Other Algorithms)\n### State-of-the-Art Nested sampling\n\n*   **Smarter** ways to sample from likelihood-constrained prior e.g. MCMC, slice sampling, sequential Monte Carlo\n*   **Stochastically sample** a set of compression factors $\\{t_i\\}$ rather than assume a constant mean value $\\rightarrow$ gives a **measure of uncertainty** in evidence estimate\n*   Better **strategic allocation** of live points e.g. dynamic nested sampling\n*   **Codes**: Multinest, Ultranest, Polychord, dynesty\n\n> you're done!\n\n---\n(Page 160)\n\n### (Appendices Start)\n\n> useful properties of determinants:\n*   $\\det(AB) = \\det(A)\\det(B) (= \\det(B)\\det(A))$\n*   $\\det(A^T) = \\det(A)$\n*   $\\det(A^{-1}) = \\frac{1}{\\det(A)} = \\det(A)^{-1}$\n*   $n \\times n$ matrix A: $\\det(kA) = k^n \\det(A)$\n> e.g. $|2\\pi\\Sigma|^{1/2} = (2\\pi)^{n/2}|\\Sigma|^{1/2}$\n\n---\n(Page 161)\n\n(No content on page 162)\n\n---\n(Page 162)\n\n(No content on page 163)\n\n---\n(Page 163)\n\n(No content on page 164)\n\n---\n(Page 164)\n\n### MCMC Theory\n\nPREMISE:\nIf $\\pi(a)$ satisfies D.B.: $\\pi(a)T(b|a) = \\pi(b)T(a|b)$, then stationary distribution $\\pi_{stat}$ exists!\n> (ergodicity but ok)\nAnd additionally under the conditions 1) irreducible 2) aperiodic and 3) positive recurrent, we can guarantee that our MC will converge to the unique stationary distribution $\\pi_{stat}$.\n\nSHOW THAT OUR ALGOS ARE CONSTRUCTED S.T. DB SATISFIED:\nWe want to construct our algorithm s.t. DB is satisfied and $\\pi_{stat}(a) = \\pi_{post}(a)$, i.e. distribution we converge to is the target distribution (posterior).\n$\\therefore$ require $\\pi_{post}(a)T(b|a) = \\pi_{post}(b)T(a|b) \\quad (DB)$\n$T(b|a) = J(b|a)A(b|a)$\n> prob make the transition, prob propose the jump, prob accept the jump\n$\\implies$ we want to choose J, A s.t. they satisfy:\n$\\pi_{post}(a)J(b|a)A(b|a) = \\pi_{post}(b)J(a|b)A(a|b)$\n$$ \\frac{A(b|a)}{A(a|b)} = \\frac{\\pi_{post}(b)J(a|b)}{\\pi_{post}(a)J(b|a)} $$\nMETROPOLIS SATISFIES DB: $J(a|b) = J(b|a)$, symmetric proposal dist.\n$\\rightarrow$ suppose wlog $\\pi_{post}(b) > \\pi_{post}(a)$,\n$LHS = RHS \\implies \\frac{A(b|a)}{A(a|b)} = \\frac{\\pi_{post}(b)}{\\pi_{post}(a)}$ (should work, do this separately)\n$\\rightarrow \\pi_{post}(a)J(b|a) \\underbrace{1}_{A(b|a)} = \\pi_{post}(b)J(a|b)\\underbrace{\\frac{\\pi_{post}(a)}{\\pi_{post}(b)}}_{A(a|b)}$\n$\\rightarrow$ since $J(b|a) = J(a|b)$, RHS=LHS!\n\n---\n(Page 165)\n\ncont from prev. page\n### SHOW THAT METROPOLIS-HASTINGS SATISFIES DB: (more general)\nM-H: $J(a|b) \\neq J(b|a)$ necessarily, $A(b|a) = \\min\\left( \\frac{\\pi_{post}(b)J(a|b)}{\\pi_{post}(a)J(b|a)}, 1 \\right)$\nw.t.s. $\\pi_{post}(a)J(b|a)A(b|a) = \\pi_{post}(b)J(a|b)A(a|b)$\n$\\rightarrow$ since either $A(b|a)=1$ or $A(a|b)=1$, easy to show LHS=RHS.\n\n### GIBBS\nw.t.s. $\\pi(\\theta_1, \\phi_1)T(\\theta_2, \\phi_2|\\theta_1, \\phi_1) = \\pi(\\theta_2, \\phi_2)T(\\theta_1, \\phi_1|\\theta_2, \\phi_2)$\nsince for gibbs always accept, $T(\\theta_2, \\phi_2|\\theta_1, \\phi_1) = J(\\theta_2, \\phi_2|\\theta_1, \\phi_1) \\times 1$\nFor gibbs start at $\\theta_1, \\phi_1$, then propose move $\\theta$ then propose move $\\phi$\nso $J(\\theta_2, \\phi_2|\\theta_1, \\phi_1) = \\pi(\\theta_2|\\phi_1)\\pi(\\phi_2|\\theta_2)$\nand use fact that going $\\theta_1, \\phi_1 \\rightarrow \\theta_1, \\phi_2 \\rightarrow \\theta_2, \\phi_2$\nso LHS: $\\pi(\\theta_1, \\phi_1)\\pi(\\theta_2|\\phi_1)\\pi(\\phi_2|\\theta_2) = \\pi(\\theta_1, \\phi_1)\\frac{\\pi(\\theta_2, \\phi_1)}{\\pi(\\phi_1)}\\frac{\\pi(\\theta_2, \\phi_2)}{\\pi(\\theta_2)}$\nRHS: $\\pi(\\theta_2, \\phi_2)\\pi(\\theta_1|\\phi_2)\\pi(\\phi_1|\\theta_1)$\nbut since $\\theta_1$ is independent of $\\phi_2$ then update to $\\theta_2$, $\\phi_1$ is indep of $\\theta_2$. (see process on RHS)\n$\\pi(\\theta_1, \\phi_1)\\pi(\\theta_2) = \\pi(\\theta_1, \\theta_2)$\ndoing similarly for LHS $\\implies$ LHS=RHS\n$\\implies$ DB satisfied\n$\\implies$ stationary dist $\\pi_{stat} = \\pi$ exists.\n\n---\n(Page 166)",
      "timestamp": "2025-10-31T15:28:09.710433",
      "tokens": 15290,
      "cost": 0.131303125
    }
  ],
  "total_tokens": 390627,
  "total_cost": 1.0087862499999998
}