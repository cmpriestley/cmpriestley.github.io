{
  "name": "astrostats",
  "system_prompt": null,
  "created_at": "2025-10-29T10:22:45.138367",
  "messages": [
    {
      "role": "user",
      "content": "Please transcribe this entire PDF document to markdown format. Extract all text content, equations (using LaTeX notation where appropriate), headings, and structure. Preserve the document hierarchy and formatting. This PDF contains lecture notes on Astrostatistics. Make sure to transcribe ALL pages in the document.",
      "timestamp": "2025-10-29T10:26:41.288254",
      "tokens": 42883,
      "cost": 0.080306875
    },
    {
      "role": "assistant",
      "content": "This is a complete transcription of the provided PDF document containing lecture notes on Astrostatistics.\n\n***\n\n# Astrostatistics\n*modern statistical methods for astronomy*\n\n## Lecture 2\n_27.1.25_\n\n[Image: A data point with error bars] data $(x_i, y_i)$\n\n### ① Where did it come from?\n\nInvert question - if you knew \"true\" values $(\\eta_i, \\xi_i)$, how do you generate the data?\n$(\\eta_i, \\xi_i)$ are *latent variables*.\n$x_i = \\xi_i + \\epsilon_{xi}$, $E_{xi} \\sim N(0, \\sigma_x^2)$\n\n**Posit:**\n$x_i | \\xi_i \\sim N(\\xi_i, \\sigma_{x_i}^2)$  \n$y_i | \\eta_i \\sim N(\\eta_i, \\sigma_{y_i}^2)$\n> **MEASUREMENT ERROR MODEL**\n\n### ② Where do $\\eta_i, \\xi_i$ come from?\n\n**@ $\\eta_i$?**\n*intrinsic scatter* $\\epsilon_i \\sim N(0, \\sigma^2)$\n\n**Posit:** $\\eta_i = \\alpha + \\beta \\xi_i + \\epsilon_i$\n> **linear model**\n\n### ③ Where does $\\xi_i$ come from?\n\n**Posit:** $\\xi_i \\sim N(\\mu, \\tau^2)$\n> \"hyperparameters\"\n> **POPULATION DISTRIBUTION MODEL**\n\n---\n(Page 1)\n\n[Image of a graph plotting y vs x. It shows a dashed line representing the linear model $y = \\alpha + \\beta \\xi$. Around this line, there is a region showing the intrinsic scatter with width $\\sigma$. A point $(\\xi_i, \\eta_i)$ is on this line of intrinsic scatter, not on the main linear model line. From this point $(\\xi_i, \\eta_i)$, a vertical line extends to show a data point $(x_i, y_i)$, illustrating the measurement error. Below the x-axis, a Gaussian curve is drawn, centered at $\\mu$ with a width $\\tau$, representing the population distribution of $\\xi$.]\n\n### GENERATIVE MODEL\n\n$\\xi \\sim N(\\mu, \\tau^2)$ \n> Population Distribution\n\n$\\eta_i | \\xi_i \\sim N(\\alpha + \\beta \\xi_i, \\sigma^2)$\n> Regression Model\n\n$x_i | \\xi_i \\sim N(\\xi_i, \\sigma_{x_i}^2)$  \n$y_i | \\eta_i \\sim N(\\eta_i, \\sigma_{y_i}^2)$\n> Measurement Error\n\n**Knowns and Unknowns:**\n\n- $\\Psi = (\\mu, \\tau)$\n  > Population Distribution Independent Variable \"Hyperparameters\"\n- $\\theta = (\\alpha, \\beta, \\sigma^2)$\n  > Regression Parameters\n- $(\\xi_i, \\eta_i)$\n  > Latent (true) Variables ({(x,y) without measurement error})\n- $(x_i, y_i)$\n  > Observed Data\n- $(\\sigma_{x_i}, \\sigma_{y_i})$\n  > with measurement uncertainties\n\n---\n(Page 2)\n\n## Lecture 3\n_29.1.25_\n\n### FUNDAMENTALS AND NOTATION\n\n(F&B ch. 2-3, Ivezic ch. 3-5)\n(measurement we take, take values)\n\nData are realisations of **random variables** (outcomes of probabilistic experiments/observations)\n\nA **random variable** $X$ can take on values in some domain (measurable space)\ne.g. (discrete) $\\mathbb{Z}$, (continuous) $\\mathbb{R}$, (multivariate/vector combinations) $\\mathbb{R}^N$\n\n**Discrete case:**\n\nDefine the probability distribution over possible outcomes with a **probability mass function** (PMF)\n$$ \\mathrm{Pr}(X=k) = P_X(k) $$\n> prob r.v. takes some value k\ne.g. $\\mathrm{Pr}(0 \\le X \\le 2) = \\sum_{k=0}^2 P_X(k)$\nNormalisation: $\\sum_{k=0}^\\infty P_X(k) = 1$\n\n**Continuous case:**\n(normally don't work with discrete e.g. 10^6 photons -> continuous)\nDefine the **probability density function** (PDF)\n$$ \\mathrm{Pr}(x \\le X \\le x+dx) = P_X(x)dx $$\ne.g. $\\mathrm{Pr}(0 \\le X \\le 2) = \\int_0^2 P_X(x)dx$\nNormalisation: $\\int_{-\\infty}^\\infty P_X(x)dx = 1$\n\n---\n(Page 3)\n\nand the **cumulative distribution function** (CDF)\n$$ \\mathrm{Pr}(X \\le x_0) = \\int_{-\\infty}^{x_0} P_X(x)dx $$\n\n**Multivariate continuous case:**\n$\\vec{x} \\in \\mathbb{R}^N$, $S \\subset \\mathbb{R}^N$\n> member of, subset\n$$ \\mathrm{Pr}(\\vec{x} \\in S) = \\int_S P_{\\vec{x}}(\\vec{x}) d^N\\vec{x} $$\n> volume element in $\\mathbb{R}^N$ (small set values RV X takes values in)\n\nNormalisation: $\\int_{\\mathbb{R}^N} P_{\\vec{x}}(\\vec{x}) d\\vec{x} = 1$\n\nWhen there is no ambiguity, we may simplify notation,\ni.e. $P_X(x) \\rightarrow P(x)$.\n> random variable\n\nGiven a distribution $P(x)$ for RV $x$,\n$X \\sim P(x)$ means \"distributed as\", \"is drawn from\", \"draw $X$ from $P(x)$\" depending on context\n> e.g. in algorithm means this not \"approximately\" sign\n\nAstronomers' measurements are produced by physical processes that are inherently probabilistic (truly random processes thanks to **quantum mechanics**).\n\n---\n(Page 4)\n\n### Example 1: Photometry / Photon counting\n\n[Image of a star emitting photons towards a detector]\n> Chandra X-ray observatory\n\nrate = photons arriving / time = $r$\n\n$ \\mathrm{Pr}(k \\text{ photons in time } T) = \\frac{e^{-\\lambda}\\lambda^k}{k!} $ where $\\lambda = rT$\n$ P(k) = \\text{Poisson}(k|\\lambda) $\n> (also written) $k \\sim \\text{Poisson}(\\lambda)$\n\nfor $k \\rightarrow \\text{large}$, $P(k) \\approx N(k|\\lambda, \\lambda)$\n> Gaussian/Normal\n> a limit theorem\n\nDefine **Gaussian Random Variable**\n$X \\sim N(\\mu, \\sigma^2)$, $x \\in \\mathbb{R}$\nPDF: $P(x) = N(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(x-\\mu)^2/\\sigma^2}$\n\nGeneralise to **Multivariate Gaussian Vector**\n$\\vec{v} \\in \\mathbb{R}^N$ (e.g. $N=2, \\vec{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$)\nPDF: $P(\\vec{v}) = N(\\vec{v}|\\vec{\\mu}, \\Sigma) = |2\\pi\\Sigma|^{-\\frac{1}{2}} e^{-\\frac{1}{2}(\\vec{v}-\\vec{\\mu})^T \\Sigma^{-1} (\\vec{v}-\\vec{\\mu})}$\n$\\vec{\\mu} \\in \\mathbb{R}^N$, $\\Sigma$ is NxN symmetric pos. def. matrix.\n\n---\n(Page 5)\n\n### Example: Astrometry\n\n[Image illustrating astrometry: a star's light passing through the atmosphere (with turbulence) to a detector. This creates a Point Spread Function (PSF) on the detector, which is a 2D distribution. This 2D distribution is shown as a circular contour plot and its marginal distributions p(y) and p(x) are shown as 1D Gaussians along the axes.]\nPOINT SPREAD FN. (PSF)\n> (JOINT)\n> marginals\n\nJoint distribution $P(x,y) = N\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\middle| \\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix}, \\begin{pmatrix} \\sigma_x^2 & 0 \\\\ 0 & \\sigma_y^2 \\end{pmatrix} \\right)$\n(simplifying $\\sigma_x^2 = \\sigma_y^2$)\n$= N(x|\\mu_x, \\sigma_x^2) N(y|\\mu_y, \\sigma_y^2)$\n\nLaw of total probability (LTP):\n$P(x) = \\int_{-\\infty}^\\infty P(x,y)dy = N(x|\\mu_x, \\sigma_x^2)$\n$P(y) = \\int_{-\\infty}^\\infty P(x,y)dx = N(y|\\mu_y, \\sigma_y^2)$\n\nIn this case, $P(x,y) = P(x)P(y)$, x,y indep. RVs.\n\nIn general, this is not true:\n$P(x,y) = N\\left( \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\middle| \\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix}, \\begin{pmatrix} \\sigma_x^2 & \\rho\\sigma_x\\sigma_y \\\\ \\rho\\sigma_x\\sigma_y & \\sigma_y^2 \\end{pmatrix} \\right)$ $\\rho > 0$\n> (positive slant)\n\n[Image of a correlated 2D Gaussian distribution, shown as an elliptical contour, with its marginal distributions P(x) and P(y) along the axes.]\n$P(x,y) \\ne P(x)P(y)$\n> marginals same as above\n\n---\n(Page 6)\n\n### Conditional Probability\n> lower dim distributions obtained by integrating out other variables\n> marginals\n\n$P(x,y) = P(x|y)P(y) = P(y|x)P(x)$\n> joint -> conditionals\n\n$$ P(y|x) = \\frac{P(x,y)}{P(x)} = \\frac{P(x|y)P(y)}{P(x)} $$\n> **BAYES' THEOREM**\n\nNote that LTP: $P(x) = \\int P(x,y)dy = \\int P(x|y)P(y)dy$\n$P(y|x) = \\frac{P(x|y)P(y)}{\\int P(x|y)P(y)dy}$\n\nIn the bivariate case: Gaussian\n$P(y|x) = N\\left(y | \\mu_y + \\rho\\frac{\\sigma_y}{\\sigma_x}(x-\\mu_x), \\sigma_y^2(1-\\rho^2)\\right)$\n> if I know x, I can predict y\n> If it is bivariate gaussian\n> original variance in y decreases by factor (1-p^2)\n> variance of prediction error is smaller\n\n[Image showing a bivariate Gaussian distribution with its marginals. A vertical slice at $x=x_0$ is taken, and the resulting conditional distribution $P(y|x=x_0)$ is shown. This conditional distribution is a narrower Gaussian than the marginal $P(y)$. The means of these conditional distributions form the regression line.]\n\nObservations $x_0$ gives you information to predict y\n$p(y|x_0) \\ne p(y)$, $\\rho>0$.\n\n---\n(Page 7)\n\n## Lecture 4\n_31.1.25_\n\n### MOMENTS & SUMMARIES OF PROB. DISTRIBUTIONS\n$X \\sim P(x)$ -> X is RV drawn from P(x)\n\n**Expect values**\n$$ E[X] = \\int x P(x) dx $$\n$$ E[g(x)] = \\int g(x) P(x) dx $$\n\n**Variance**\n$$ \\mathrm{Var}(X) = E[(X - E(X))^2] $$\n$$ = E[X^2] - (E[X])^2 $$\n\n**Multivariate (x,y):**\n\n**Covariance**\n$$ \\mathrm{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])] $$\n$$ (=E[XY] - E[X]E[Y]?) $$\n\n**Multivariate Normal:**\n$$ \\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix}, \\begin{pmatrix} \\sigma_x^2 & \\sigma_x\\sigma_y\\rho \\\\ \\sigma_x\\sigma_y\\rho & \\sigma_y^2 \\end{pmatrix} \\right) \\quad -1 < \\rho < 1 $$\n> PROPERTIES: `Cov(X,X)=Var(X)`, `Cov(aX,bY)=abCov(X,Y)`, etc.\n\n\\* $X \\perp Y \\rightarrow \\mathrm{Cov}(X,Y) = 0$\n\\* general case $\\mathrm{Cov}(X,Y) = \\sigma_x\\sigma_y\\rho$\n> `Cov(x,y)=p` is for standard normals (σ=1).\n\n**Useful Properties:**\nCollection of random variables\n$\\{X_1, ..., X_N\\}, \\{Y_1, ..., Y_N\\}$\n> set of x & y measurements labelled 1 to N\n\n---\n(Page 8)\n\nDef. $S = \\sum_{i=1}^N a_i X_i$, $T = \\sum_{j=1}^M b_j Y_j$, $a_i, b_j \\in \\mathbb{R}$\n\n**Expectations are linear** $\\rightarrow E[S] = \\sum a_i E[X_i]$\n> expectation of sum = sum of expectation (linearity of integral)\n\n**Bilinearity of covariance:**\n$ \\mathrm{Cov}(S,T) = \\mathrm{Cov}(\\sum_{i=1}^N a_i X_i, \\sum_{j=1}^M b_j Y_j) = \\sum_i \\sum_j a_i b_j \\mathrm{Cov}(X_i, Y_j) $\n$ \\mathrm{Var}(S) = \\mathrm{Cov}(S,S) = \\sum_{i=1}^N a_i^2 \\mathrm{Var}(X_i) + \\sum_{i \\ne j} a_i a_j \\mathrm{Cov}(X_i, X_j) $\n... useful for summing multiple sources of error.\n> variance of a sum of RV is covariance of sum with itself.\n> `just two terms` `some errors correlated` `not correlated`\n> IMPORTANT! Variance of sum is sum of covariances!\n> `Var(Σ X_i) = ΣΣ Cov(X_i, X_j) = Σ Cov(X_i, X_j) + Σ Var(X_i)`\n\n### TRANSFORMATIONS OF RVs\n$X \\sim P(x)$\n$Y = \\phi(x)$ (invertible & differentiable function)\nwhat is $P(y)$?\n\n$x = \\phi^{-1}(y)$\n$p(y) = P(x) \\left| \\frac{dx}{dy} \\right| = P(\\phi^{-1}(y)) \\left| \\frac{d\\phi^{-1}(y)}{dy} \\right|$\n> Jacobian\n\n**Example:**\n$X \\sim U(0,1)$\n$Y = - \\ln X \\rightarrow P(y) = \\begin{cases} e^{-y}, & 0 \\le y < \\infty \\\\ 0, & \\text{o/w} \\end{cases}$\n$\\frac{dx}{dy} = -e^{-y}$\n> $y = \\text{Expon}(1)$\n\n[Image of the PDF for a Uniform(0,1) distribution and the resulting Exponential(1) distribution for Y.]\n\n---\n(Page 9)\n\n### Propagation of error\n> can think of this as transformation of RV but with variances\n\nMeasurement $x_0$ w/ error variance $\\sigma_x^2$, error bar $\\sigma_x$\n[Image of a non-linear function $\\phi(x)$. A narrow Gaussian $P(x)$ centered at $x_0$ with width $\\sigma_x$ is shown on the x-axis. This is mapped by $\\phi(x)$ to a skewed distribution $P(y)$ on the y-axis with width $\\sigma_y$.]\n\n$y = \\phi(x) = \\phi(x_0) + \\left.\\frac{d\\phi}{dx}\\right|_{x=x_0}(x-x_0) + ...$\n$\\mathrm{Var}(y) \\approx \\left| \\frac{d\\phi}{dx} \\right|_{x=x_0}^2 E[(x-x_0)^2]$\n$$ \\sigma_y^2 \\approx \\left| \\frac{d\\phi}{dx} \\right|_{x=x_0}^2 \\sigma_x^2 $$\n\n**Example:**\nAstronomers' flux $f, \\sigma_f$\nmagnitude $m = -2.5 \\log_{10}(f) + \\text{const.}$\n$$ \\sigma_m \\approx \\left| \\frac{2.5}{\\ln(10)} \\right| \\frac{\\sigma_f}{f} $$\n> `log_10(f) = ln(f)/ln(10)`\n> `2.5/ln(10)` approx 1. Good enough.\n\n... goodness of approximation depends on how linear transformation is 'in the neighborhood' where we have uncertainty.\nIt will break down if uncertainty large enough or trans. non-linear enough.\n\n**MULTIVARIATE:** $Y = g(X_1, ..., X_d) \\approx g(x^0) + \\sum_i \\frac{\\partial g}{\\partial x_i} (X_i - x_i^0)$ (linear taylor exp.)\n$\\rightarrow$ `use rule Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)`\n$ \\mathrm{Var}(Y) \\approx \\sum_i \\left(\\frac{\\partial g}{\\partial x_i}\\right)^2 \\mathrm{Var}(X_i) + \\sum_{i \\ne j} \\left(\\frac{\\partial g}{\\partial x_i}\\right)\\left(\\frac{\\partial g}{\\partial x_j}\\right) \\mathrm{Cov}(X_i, X_j) $\n> intrinsic correlation\n\n---\n(Page 10)\n\n**Multivariate case:**\n$X_{obs} = (X_1, ..., X_N)$, $Y = g(X_1, ..., X_N)$\n$$ \\sigma_Y^2 \\approx \\sum_{i=1}^N \\left.\\left(\\frac{\\partial g}{\\partial x_i}\\right)^2\\right|_{X_{obs}} \\mathrm{Var}(X_i) + \\sum_{i \\ne j} \\left.\\left(\\frac{\\partial g}{\\partial x_i}\\right)\\left(\\frac{\\partial g}{\\partial x_j}\\right)\\right|_{X_{obs}} \\mathrm{Cov}(X_i, X_j) $$\n> important term, allows you to pick out correlated errors\n\n### iid SEQUENCES OF RVs\n\n**iid = independent & identically distributed**\n(e.g. repeated measurement)\n> why care? e.g. astronomers do multiple observations, often model them as iid.\n\nSuppose we have a sequence of indexed RVs.\n$X_1, X_2, X_3, ..., X_N$ iid $P(X)$\nThis means RVs are independent! (`cov(X_i, X_j) = 0`)\nidentically distrib. $X_i \\sim P(X)$, $i \\ne j \\quad X_i \\perp X_j$\n\n**Joint distribution** $\\sim P(X_1, X_2, ..., X_N) = \\prod_{i=1}^N P(X_i)$\n\n**Not example:**\n$\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} \\mu \\\\ \\mu \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 & \\sigma^2\\rho \\\\ \\sigma^2\\rho & \\sigma^2 \\end{pmatrix} \\right) \\quad \\rho \\ne 0$\nMay $P(X_1) = \\int P(X_1, X_2) dX_2 = N(X_1|\\mu, \\sigma^2)$, $P(X_2) = N(X_2|\\mu, \\sigma^2)$\n$X_1, X_2$ identically distributed but not independent (or as last lecture. $P(X_2|X_1) \\ne P(X_2)$).\n$$ \\mathrm{Cov}(X_1, X_2) = \\rho \\ne 0 $$\n$$ \\sqrt{\\mathrm{Var}(X_1)}\\sqrt{\\mathrm{Var}(X_2)}} = \\rho \\ne 0 $$\n> show not independent: `p(x1,x2) != p(x1)p(x2)` or showing `ρ != 0` (equivalently cov != 0)\n\n---\n(Page 11)\n\n### Limit theorems for iid RVs\n(see book for technicalities)\n\n**① LAW OF LARGE NUMBERS (LLN)**\n$X_1, ..., X_N$ iid $P(X)$\n$\\mu = E[X_i] = \\int x P(x)dx$\n> {theoretical model, population mean}\nDef: Sample mean $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^N X_i$\nas $N \\rightarrow \\text{large}$, $\\bar{X}_N \\rightarrow \\mu$\n> i.e. sample mean gives good approx. to population mean when N large.\n\n**② CENTRAL LIMIT THEOREM (CLT)**\n[Image: a non-Gaussian population distribution `P` evolving into a Gaussian sampling distribution for `X̄_N` as `N -> ∞`.]\n$X_1, ..., X_N$ iid $P(X)$, $\\sigma^2 = \\mathrm{Var}(X_i)$ finite.\nAs $N \\rightarrow \\text{large}$, $\\frac{\\bar{X}_N - \\mu}{\\sigma/\\sqrt{N}} \\rightarrow N(0,1)$\n> Unit gaussian\ni.e. $\\bar{X}_N \\rightarrow N(\\mu, \\sigma^2/N)$, error in the mean = $\\sigma/\\sqrt{N}$\n> note: this is a general result, we haven't assumed `P(x)` is gaussian, just that it has finite σ^2.\n\n### STATISTICAL ESTIMATORS\ndata, RV\nProbability model $P_{\\theta}(X)$ or $P(X|\\theta)$\n> parameters\nSuppose data are $D=(X_1, ..., X_N)$, $X_1, ..., X_N$ iid $P_{\\theta}(X)$\n> data are iid drawn from probability distribution.\nEstimate $\\theta$ from $D$ by constructing an **estimator** $\\hat{\\theta}(D)$ that yields a \"good\" value for the **estimand** $\\theta$.\n> thing you are estimating\n\n---\n(Page 12)\n\n## Lecture 5\n_3.2.25_\n\n[Example context: CMB radiation from ~300,000 years after Big Bang. Fluctuations in plasma temp are map of fluctuations. A key property is the mean temperature. How might we find the mean of this distribution?]\n> \"point estimation\"\n\n### STATISTICAL ESTIMATORS CONT.\n\nConstruct a \"good\" estimator $\\hat{\\theta}(D) = \\hat{\\theta}(\\vec{X})$ for estimand $\\theta$.\n> `θ̂`: estimate, `θ`: estimand, true value\n\nProb model $P_{\\theta}(X)$ or $P(X|\\theta)$\n$D=(X_1, ..., X_N)$ iid $P(X|\\theta)$.\n$X_i \\sim P_{\\mu}(x)$, $\\mu = E_P(x)$\n\n[Image of a generic probability distribution with mean `μ` and standard deviation `σ`.]\n\ne.g. (CMB Example) estimate $\\mu$ from temp. measurement in each pixel $X_i, i=1, ..., N$\nEstimand $\\mu=E(X_i)$\nEstimator $\\hat{\\mu}=f(\\vec{X})$\n\n**Possible Estimators:** (how do we choose?)\n1. Sample mean $\\frac{1}{N}\\sum_i X_i$ > unbiased\n2. Take $K < N$, $\\frac{1}{K}\\sum_{i=1}^K X_i$ > truncated sample mean, unbiased, inconsistent\n3. $\\frac{1}{N-1}\\sum_i X_i$ > not unbiased but asymptotically unbiased\n4. Midrange $\\frac{1}{2}[\\max(\\vec{X}) + \\min(\\vec{X})]$\n5. Median\n6. Bin $\\rightarrow$ Histogram $\\rightarrow$ mode\n\n---\n(Page 13)\n\n7. Just report 3 Kelvin\n\nHow do you evaluate different possible **estimators** for a particular **estimand**?\n($\\mu \\rightarrow \\hat{\\mu}(\\vec{X})$)\n\n### Criteria for Estimators $\\hat{\\theta}$ for $\\theta$\n\n**UNBIASEDNESS**: $\\hat{\\theta}$ is **unbiased estimate** for $\\theta$ if $E_P[\\hat{\\theta}] = \\theta$\n> definition e.g. `E_p`\n\nbias: $b(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$\n> expectation is over repeated experiments sampling the full data distribution $P(\\vec{X}|\\theta)$\n\nImagine you did J experiments with fixed sample size N, $j=1, ..., J$\n$\\rightarrow \\vec{X}_j = (X_{1,j}, ..., X_{N,j})$ -> J datasets $\\vec{X}_1, ..., \\vec{X}_J$\n$\\hat{\\theta}_1 = \\hat{\\theta}(\\vec{X}_1), \\hat{\\theta}_2=\\hat{\\theta}(\\vec{X}_2), ... \\hat{\\theta}_J=\\hat{\\theta}(\\vec{X}_J)$\nUnbiased as $J \\rightarrow \\infty$, $\\frac{1}{J}\\sum_{j=1}^J \\hat{\\theta}_j = E[\\hat{\\theta}] = \\theta$\n\n[Image of a number line with the `true θ` marked, and several estimates `θ̂_i` scattered around it, suggesting their average will land on `true θ`.]\n\n---\n(Page 14)\n\nN.B. You only really did one experiment!\n\n**ASYMPTOTICALLY UNBIASED**: $E[\\hat{\\theta}(\\vec{X})] \\rightarrow \\theta$ as $N \\rightarrow \\infty$\n(e.g. ③ on previous page)\n\n**CONSISTENCY**: As you gather more data (sample size N $\\rightarrow$ large), $\\hat{\\theta}$ converges to $\\theta$.\n$$ \\forall \\epsilon > 0, \\mathrm{Pr}(|\\hat{\\theta}-\\theta| > \\epsilon) \\rightarrow 0 \\text{ as } N \\rightarrow \\infty $$\n> e.g. ② not consistent\n\n**EFFICIENCY**: Smallest **mean squared error**\n$ \\mathrm{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta}(\\vec{X}) - \\theta)^2] $\n(for unbiased $\\rightarrow$ smallest variance)\n\n$$ \\mathrm{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}] + E[\\hat{\\theta}] - \\theta)^2] = \\mathrm{Var}(\\hat{\\theta}) + \\mathrm{Bias}^2(\\hat{\\theta}) $$\n\n**BIAS-VARIANCE TRADEOFF**: Sometimes the most **efficient** estimator is biased and the unbiased estimator is not the most **efficient**.\n\n**MINIMUM VARIANCE UNBIASED ESTIMATORS (MVUE)**\n\n**Caveat**: Unbiased estimators can be wrong (obviously wrong).\n\n---\n(Page 15)\n\n**Ex.**\n$X_i \\stackrel{iid}{\\sim} U(0, \\theta)$, $i=1, ..., N$\nEstimate $\\theta$? Unbiased Estimator?\n\n[Image of a uniform distribution P(x) from 0 to θ.]\n\nSuppose we take sample mean:\n$\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$\n$E[\\bar{X}] = \\theta/2$\n$\\hat{\\theta}_u = 2\\bar{X}$ unbiased estimator\nsince $E[\\hat{\\theta}_u] = 2(\\theta/2) = \\theta$\n> `unbiased`\n\nSuppose instead\n$\\hat{\\theta}_m = \\max(X_1, ..., X_N) < \\theta$\n> `always biased!`\n\ngenerate samples $\\rightarrow$ RNG $\\vec{X} = (0.32, 0.46, 0.97, 2.77, 7.06)$\nunbiased estimate $\\hat{\\theta}_u = 4.63$ (unbiased but clearly wrong)\nbiased estimate $\\hat{\\theta}_m = 7.06$ (biased but much more sensible)\n\n**Unbiasedness** is not a property of any single experiment, only a property of averaging over all experiments that **did not happen**.\n\n---\n(Page 16)\n\n**Ex.** Estimator properties depend on data process $p(X|\\theta)$, $N=10$\n(choose a, b s.t. $M=(a+b)/2$, $\\mathrm{Var}(X) = (b-a)^2/12 = \\sigma^2$)\n\n| Estimator         | Gaussian $X_1, ..., X_N \\sim N(\\mu, \\sigma^2=1)$ | Uniform $X_1, ..., X_N \\sim U(a,b)$ |\n| ----------------- | ------------------------------------- | ------------------------------------ |\n| **Estimator Variance** | | |\n| Sample Mean $\\bar{X}$ | $\\sigma^2/N = 0.10$                   | $\\sigma^2/N = 0.10$                  |\n| Midrange $\\frac{\\min(X)+\\max(X)}{2}$ | $\\frac{\\pi^2\\sigma^2}{24\\ln N} = 0.18$ | $\\frac{6\\sigma^2}{(N+2)(N+1)} \\approx \\frac{6\\sigma^2}{N^2} = 0.06$ |\n|                   | midrange less efficient               | midrange more efficient              |\n\n> both sample mean & midrange are unbiased, but midrange `V` goes down as `1/N^2` whereas `V_mean` goes down as `1/N`.\n\n### LIKELIHOOD-BASED INFERENCE\n\nProbability model $P(\\vec{D}|\\theta)$ we call \"**sampling distribution**\"\nProbability distribution for possible/potential datasets $\\vec{D}$, for a given parameter value $\\theta$.\n\nWe observe $\\vec{D}_{obs}: P(\\vec{D}=\\vec{D}_{obs}|\\theta) = L(\\theta)$\n> **Likelihood Function**\n> Sampling dist: probability dist. over all possible outcomes of D for a given θ. (before it is observed). N.B. $\\int P(D|\\theta)dD = 1$\n\n---\n(Page 17)\n\n## Lecture 6\n_5.2.25_\n\n### LIKELIHOOD-BASED INFERENCE CONT.\n\nProbability model $D \\sim P(D|\\theta)$, $\\int P(D|\\theta)dD = 1$\n> **Sampling distribution**\n\n[Image showing three different sampling distributions $P(D|\\theta_1)$, $P(D|\\theta_2)$, $P(D|\\theta_3)$ as functions of D. A vertical line at $D_{obs}$ intersects these curves, providing the likelihood values for each $\\theta$.]\n\n**Likelihood**: $P(D=D_{obs}|\\theta) = L(\\theta)$\nN.B. $\\int L(\\theta)d\\theta$ not necessarily = 1\n\n[Image showing the likelihood function $L(\\theta)$ as a function of $\\theta$. The values at $\\theta_1, \\theta_2, \\theta_3$ correspond to the heights of the intersections in the previous graph. The maximum likelihood is at $\\theta_2$, so $\\hat{\\theta}_{MLE} = \\theta_2$.]\n\nSuppose we now observe data $D_{obs}$ (outcome or realisation of D), the **likelihood** is the sampling dist. evaluated at $D=D_{obs}$, viewed as a fn. of the parameters. i.e. $L(\\theta) = P(D=D_{obs}|\\theta)$.\n\n---\n(Page 18)\n\n**Notation**:\nOften we will elide the distinction between $\\vec{D}$ and $\\vec{D}_{obs}$\n$P(\\vec{D}|\\theta)$ understood to mean $P(\\vec{D}=\\vec{D}_{obs}|\\theta)$.\n\n**log likelihood** $l(\\theta) = \\ln L(\\theta)$.\n\n**iid case:**\n$X_i \\stackrel{iid}{\\sim} f(X|\\theta) \\quad i=1, ..., N, \\quad P(\\vec{X}|\\theta) = \\prod_{i=1}^N f(x_i|\\theta)$\n$L(\\theta) = P(\\vec{X}|\\theta) = \\prod_{i=1}^N f(x_i|\\theta)$ -> individual likelihood\n$l(\\theta) = \\sum_{i=1}^N \\ln f(x_i|\\theta)$ -> log likelihood\n\n### Fisher Information $\\leftrightarrow$ Uncertainty\n\nDEFINE:\n$L(\\theta) = P(x|\\theta)$\n**Score** $S = \\frac{\\partial}{\\partial\\theta}\\ln L(\\theta) = \\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta)$\n> `p(x|θ)`\n$E[S] = E[\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta)] = \\int [\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta)] P(x|\\theta) dx$\n$= \\int [\\frac{1}{P(x|\\theta)} \\frac{\\partial}{\\partial\\theta} P(x|\\theta)] P(x|\\theta) dx$\n$* = \\int \\frac{\\partial}{\\partial\\theta} P(x|\\theta) dx = \\frac{\\partial}{\\partial\\theta} \\int P(x|\\theta)dx = \\frac{\\partial}{\\partial\\theta}[1] = 0$\n\\* Under regularity conditions $\\frac{\\partial}{\\partial\\theta} \\leftrightarrow \\int dx$\n\n---\n(Page 19)\n\n**Fisher Information** = Variance of the **Score**\n$I(\\theta) = \\mathrm{Var}(S) = E[S^2] - (E[S])^2$\n$I(\\theta) = E[(\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta))^2] - (E[\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta)])^2$\n?$= -E[\\frac{\\partial^2}{\\partial\\theta^2}\\ln P(x|\\theta)]$\n\n$E[\\frac{\\partial^2}{\\partial\\theta^2}\\ln P(x|\\theta)] = E[\\frac{\\partial}{\\partial\\theta}[\\frac{1}{P(x|\\theta)}\\frac{\\partial}{\\partial\\theta}P(x|\\theta)]]$\n$= E[-\\frac{1}{P(x|\\theta)^2}(\\frac{\\partial}{\\partial\\theta}P(x|\\theta))^2 + \\frac{1}{P(x|\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}P(x|\\theta)]$\n$= E[-(\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta))^2 + \\frac{1}{P(x|\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}P(x|\\theta)]$\n$\\int [\\frac{1}{P(x|\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}P(x|\\theta)] P(x|\\theta) dx = \\frac{\\partial^2}{\\partial\\theta^2} \\int P(x|\\theta) dx = \\frac{\\partial^2}{\\partial\\theta^2}(1) = 0$\n$= -E[(\\frac{\\partial}{\\partial\\theta}\\ln P(x|\\theta))^2]$\n$= -I(\\theta)$\n\n$$ I(\\theta) = E[-\\frac{\\partial^2}{\\partial\\theta^2}(\\ln P(x|\\theta))] $$\n\n> from info theory: amount of information r.v. X carries about an unknown parameter $\\theta$ (upon which the probability of X depends)\n> Formally: variance of score or repeated value of observed information.\n> F.I. used to calculate covariance matrices associated with MLE.\n\n---\n(Page 20)\n\n### Cramer-Rao Lower Bound\n> \"inverse of fisher info is a lower bound on the variance of any unbiased estimate\"\n\nSuppose we have estimator $T(x)$ for $\\theta$,\n$E[T] = \\theta + b(\\theta)$.\n\nConsider $\\mathrm{Cov}(S, T(X)) = E[S(X)T(X)] - E[S]E[T]$\n> score\n> $E[S]=0$ assuming unbiased.\n$= E[T(X) \\frac{1}{P(X|\\theta)} \\frac{\\partial}{\\partial\\theta} P(X|\\theta)]$\n$= \\int [T(X) \\frac{1}{P(X|\\theta)} \\frac{\\partial}{\\partial\\theta} P(X|\\theta)] P(X|\\theta) dx$\n$* = \\int T(X) \\frac{\\partial}{\\partial\\theta} P(X|\\theta) dx = \\frac{\\partial}{\\partial\\theta} \\int T(X) P(X|\\theta) dx$\n> * under regularity conditions\n$= \\frac{\\partial}{\\partial\\theta} E[T(X)] = 1 + b'(\\theta)$\n\n$\\mathrm{Cov}(S,T) = \\sqrt{\\mathrm{Var}(S)} \\sqrt{\\mathrm{Var}(T)} \\mathrm{Corr}(S,T) \\leftarrow -1 \\le \\mathrm{Corr} \\le 1, |\\mathrm{Corr}| \\le 1$\n$|\\mathrm{Cov}(S,T)| \\le \\sqrt{\\mathrm{Var}(S)}\\sqrt{\\mathrm{Var}(T)}$\n$(1+b'(\\theta))^2 \\le \\mathrm{Var}(S)\\mathrm{Var}(T)$\n$\\le I(\\theta) \\mathrm{Var}(T)$\n\n$$ \\mathrm{Var}(T) \\ge \\frac{(1+b'(\\theta))^2}{I(\\theta)} $$\n> **CRLB** gives low bound on minimum poss variance for an estimator. There are so many estimators to choose. The one with the lowest variance is often preferred.\n\n$T$ unbiased $\\rightarrow b=0$\n$$ \\mathrm{Var}(T) \\ge I(\\theta)^{-1} $$\n> useful property\n\n---\n(Page 21)\n\n### CRLB Multivariate Case\n\n$\\vec{X} \\sim P(\\vec{X}|\\vec{\\theta})$\nand $\\vec{T}(\\vec{x})$ is unbiased estimator for $\\vec{\\theta}$,\n$E[\\vec{T}(\\vec{x})] = \\vec{\\theta}$, $E[T_j(\\vec{x})] = \\theta_j$\nThen $\\left[ \\mathrm{Cov}(\\vec{T}(\\vec{x})) \\ge I^{-1}(\\theta) \\right]$\nhas elements $\\mathrm{Cov}(T_j, T_k)$\n\nFisher Matrix $I(\\theta)$ has elements\n$I_{jk} = E[-\\frac{\\partial^2}{\\partial\\theta_j \\partial\\theta_k} \\ln P(\\vec{x}|\\vec{\\theta})]$\n> **Hessian**\n\nIn particular:\n$\\left[ \\mathrm{Var}(T_i) \\ge [I^{-1}(\\theta)]_{ii} \\right]$\n(diagonal terms)\n\n---\n(Page 22)\n\n### Max Likelihood\n\n$X_i \\stackrel{iid}{\\sim} f(x|\\theta) \\quad i=1, ..., N$\n$L(\\theta) = P(\\vec{x}|\\theta) = \\prod_{i=1}^N f(x_i|\\theta)$\n\n$\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{argmax}} \\, L(\\theta)$\n$= \\underset{\\theta}{\\mathrm{argmax}} \\, \\ln L(\\theta)$\n> (i think?) to find `θ_MLE`: `∂l/∂θ=0`. multiparameter `∂l/∂θ_1=0`, `∂l/∂θ_2=0` etc.\n\n**MLE Properties** (Model is true!)\n\\* **Consistent**: $\\hat{\\theta}_{MLE} \\rightarrow \\theta_{true}$ as $N \\rightarrow \\infty$\n\\* **Asymptotically unbiased**: $E[\\hat{\\theta}_{MLE}] \\rightarrow \\theta_{true}$ as $N \\rightarrow \\infty$\n\\* **Asymptotically Normal**: $(\\hat{\\theta}_{MLE} - \\theta_{true}) \\rightarrow N(0, I^{-1})$\n> Asymptotic normality: consistent estimators `θ̂` have dist 'around' true `θ` that approaches a normal dist (with a decreasing `σ` on `1/√N`)\n\n[Image showing that as N increases, the distribution of `θ̂_MLE` around the true `θ` becomes a narrower Gaussian, achieving the CRLB.]\n\n> why fisher info is useful: find covariance matrices associated with MLE\n> achieves CRLB as N->inf (efficient!). No other estimator is more efficient.\n\n---\n(Page 23)\n\n## Lecture 7\n_7.2.25_\n\n### Max Likelihood Properties (cont.)\n\n$X_i \\stackrel{iid}{\\sim} f(X|\\theta) \\quad i=1, ..., N$\n$L(\\theta) = P(\\vec{X}|\\theta) = \\prod_{i=1}^N f(X_i|\\theta)$\n\nMLE $\\rightarrow \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{argmax}} \\, L(\\theta) = \\underset{\\theta}{\\mathrm{argmax}} \\, \\ln L(\\theta)$\n\n\\* **Consistent**: $\\hat{\\theta}_{MLE} \\rightarrow \\theta_{true}$ as $N \\rightarrow \\infty$\n> $(\\forall \\epsilon > 0, \\mathrm{Pr}(|\\hat{\\theta}_{MLE} - \\theta_{true}| > \\epsilon) \\rightarrow 0 \\text{ as } N \\rightarrow \\infty)$\n\n\\* **Asymptotically Unbiased**: $E[\\hat{\\theta}_{MLE}] \\rightarrow \\theta_{true}$ as $N \\rightarrow \\infty$\n> (but not necessarily unbiased)\n\n\\* **Asymptotically Normal**: $(\\hat{\\theta}_{MLE} - \\theta_{true}) \\stackrel{d}{\\rightarrow} N(0, I^{-1})$\n\n$I(\\theta) = -E[\\frac{\\partial^2 l(\\theta)}{\\partial\\theta^2}] = -NE[\\frac{\\partial^2 \\ln f}{\\partial\\theta^2}]$\n> **Expected Fisher Information**\n$\\hat{I} = \\text{\"observed Fisher Info.\"} = -\\frac{\\partial^2 l}{\\partial\\theta^2}|_{\\theta=\\hat{\\theta}_{MLE}}$\n$\\approx I(\\theta)$ as $N \\rightarrow \\infty$\n\n\\* **Efficient**: Asymptotically achieves CRLB\n$\\mathrm{Var}(\\hat{\\theta}_{MLE}) \\rightarrow I^{-1}$ as $N \\rightarrow \\infty$\n> \"saturates\"\n\n---\n(Page 24)\n\n\\* **Functionally invariant**: $\\alpha = g(\\theta)$\n> \"equivariance property\", parameter trans.\n$\\hat{\\alpha}_{MLE} = g(\\hat{\\theta}_{MLE})$\n\n### Multiparameter Case MLE\n\n$L(\\vec{\\theta}) = p(\\vec{x}|\\vec{\\theta}) = \\prod_{i=1}^N p(x_i|\\vec{\\theta})$\n$\\hat{\\theta}_{MLE} = \\underset{\\vec{\\theta}}{\\mathrm{argmax}} \\, L(\\vec{\\theta})$\n\n\\* **Asymptotically Normal**:\n$\\vec{\\theta}_{MLE} - \\vec{\\theta}_{true} \\stackrel{d}{\\rightarrow} N(\\vec{0}, \\vec{I}^{-1})$\n> **Inverse Fisher Matrix**\n\n$I_{jk} = \\text{Exp. Fisher Info Matrix}$\n$= E[-\\frac{\\partial^2 l}{\\partial\\theta_j \\partial\\theta_k}] = -NE[\\frac{\\partial^2 \\ln f}{\\partial\\theta_j \\partial\\theta_k}]$\n> Hessian\nObserved F.I. = $\\hat{I}_{jk} = -\\frac{\\partial^2 l}{\\partial\\theta_j \\partial\\theta_k}|_{\\vec{\\theta}=\\hat{\\vec{\\theta}}_{MLE}}$\n$\\approx I_{jk}$ as $N \\rightarrow \\infty$\n\n\\* **Asymptotically Efficient**:\n$\\mathrm{Cov}(\\vec{\\theta}_{MLE}) \\rightarrow \\vec{I}^{-1}$\nIn particular, $\\mathrm{Var}(\\theta_{MLE, i}) \\rightarrow (\\vec{I}^{-1})_{ii}$ (diagonal)\n> usually we want to know variance of particular components\n\n---\n(Page 25)\n\n**Quick Example:** (evaluate Fisher Matrix)\n$X_i \\sim N(\\mu, \\sigma^2)$, $i=1, ..., N$\n$\\vec{\\theta} = (\\mu, \\sigma^2)$\n$L(\\vec{\\theta}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}(x_i-\\mu)^2/\\sigma^2}$\n$\\rightarrow l(\\vec{\\theta}) = \\sum_{i=1}^N -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}$\n\n1st deriv:\n$\\frac{\\partial l}{\\partial\\mu} = \\sum_{i=1}^N \\frac{(x_i-\\mu)}{\\sigma^2} = 0$\n$\\rightarrow \\hat{\\mu}_{MLE} = \\frac{1}{N}\\sum x_i = \\bar{X}",
      "timestamp": "2025-10-29T10:26:41.288602",
      "tokens": 10701,
      "cost": 0.080306875
    }
  ],
  "total_tokens": 53584,
  "total_cost": 0.16061375
}