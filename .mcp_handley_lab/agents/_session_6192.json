{
  "name": "_session_6192",
  "system_prompt": null,
  "created_at": "2025-10-31T16:54:18.328352",
  "messages": [
    {
      "role": "user",
      "content": "You are transcribing handwritten lecture notes from an astrostatistics course. Please transcribe pages 32-50 from the provided PDF.\n\n**TRANSCRIPTION STYLE RULES:**\n1. Page markers: End each page with `---` followed by `(Page N)` on next line\n2. Lecture headers: Use `## Lecture N` and `_date_` in italics below it\n3. Section headers: Use `###` for major topics\n4. Math notation: Use LaTeX in inline `$...$` or display `$$...$$` format\n5. Diagrams/Images: Describe in square brackets like `[Image of ...]` or `[Diagram showing ...]`\n6. Comments/marginal notes: Use `>` blockquote prefix\n7. Lists: Use `-` or numbered lists where appropriate\n8. Preserve all mathematical derivations and equations exactly\n\n**Example of desired output style:**\n```\n## Lecture 3\n_29.1.25_\n\n### FUNDAMENTALS AND NOTATION\n\nData are realisations of **random variables**\n$ \\mathrm{Pr}(X=k) = P_X(k) $\n> prob r.v. takes some value k\n\n---\n(Page 3)\n```\n\nPlease transcribe pages 32-50 completely and accurately, preserving all mathematical content, notes, and structure. Start with page 32 and continue through page 50.",
      "timestamp": "2025-10-31T16:55:51.169732",
      "tokens": 43125,
      "cost": 0.071258125
    },
    {
      "role": "assistant",
      "content": "### Supernova Absolute Magnitude Distribution: Selection Effects (board)\n> begin thought experiment to see what might go wrong, what might we need to account for?\n\nSame ex. but now\nAssume measurement error = 0 = $\\sigma_{m,s} = \\sigma_{\\mu s}$\n> for simplicity of argument\nPopulation variability $\\sigma_{int} \\rightarrow \\sigma$.\nOnly intrinsic population is source of variability ($\\sigma^2 = \\sigma_{int}^2$).\n\nSuppose distance is the same for an entire sample of SN\n$\\mu_s = \\mu$\n\npopulation dist\niid\n$M_s \\sim N(M_o, \\sigma^2)$\n> latent variable eq\n> $(M_s = M_o + \\mu_s)$\n\ncombine\n$\\rightarrow m_s \\sim N(M_o + \\mu, \\sigma^2)$ $s=1,...,N$\n\n[Diagram showing the effect of a survey limit on observing supernovae. The x-axis is apparent magnitude 'm', labelled 'dimmer' to the right. The y-axis is distance modulus 'μ', labelled 'fainter' upwards. A diagonal dashed line represents 'm = M_o + μ'. Two distances, μ1 and μ2, are shown. At each distance, there is a Gaussian distribution of observed magnitudes 'm_s' centered on 'm = M_o + μ_i'. A vertical dashed line at 'm_lim' represents the survey limit. For the closer distance μ1, the entire Gaussian is to the left of m_lim, so all SN are observed. For the farther distance μ2, the Gaussian is centered closer to m_lim, and the right tail of the distribution is cut off by the limit, meaning some fainter SN at that distance go unobserved. The observed SN population is shown as blue filled Gaussians, while unobserved ones are indicated by 'x's under the truncated part of the curve. The equation for the diagonal line is μ = m - M_o.]\n\nLabels on diagram:\n- Above μ2 Gaussian: `observed SN`\n- Right of μ2 Gaussian, past m_lim: `unobserved SN`\n- To the right of m_lim: `to faint to be observable`\n- Below x-axis: `m1 = M_o + μ1`, `m2 = M_o + μ2`\n- Vertical line: `M_lim` `survey limit`\n\n> (We stop)\n> Telescopes can only detect up to certain magnitude limit, can't see anything dimmer than this.\n> So for a set of SNs, say Ms are acting \"majoritively\" look similar but in the end we don't see the one above some limit, what do we do?\n\n---\n(Page 32)\n\nData: $\\{m_s\\} = \\vec{m}$\nfixed $\\mu$ for sample\n\nNaive Likelihood: $P(m_s|M_o, \\sigma^2) = N(m_s|\\mu+M_o, \\sigma^2)$\n$L(M_o, \\sigma^2) = \\prod_{s=1}^N P(m_s|M_o, \\sigma^2)$\n> naively apply naive likelihood\n\nIf $M_o + \\mu$ close to $M_{lim}$, MLE will be biased:\n$$ \\left\\{ \\begin{array}{ll} \\hat{M}_o & \\text{too bright} \\\\ \\hat{\\sigma} & \\text{too small} \\end{array} \\right. - \\left( \\begin{array}{l} \\text{we didn't see} \\\\ \\text{m,dim everywhere} \\end{array} \\right) $$\n> b/c tail of dist. is cut off.\n> different width, smaller than true width\n\n> how do we resolve this?\n### Accounting For selection Effects (sheet 1 & 2)\n\nLet $I_s = \\begin{cases} 1, & \\text{if SN observed} \\\\ 0, & \\text{if SN NOT observed} \\end{cases}$\n> indicator variable\n\nDefine Selection Function:\n> prob indicator given data\n$$ P(I_s|m_s) = \\begin{cases} 1, & m_s < M_{lim} \\\\ 0, & m_s \\ge M_{lim} \\end{cases} $$\n$$ = 1 - H(m_s - M_{lim}) $$\n> heaviside step fn\n\n[Graph of the selection function. The x-axis is m_s, the y-axis is P(I_s|m_s). The function is 1 for m_s < M_lim and drops to 0 at m_s = M_lim. A point is marked at M_lim on the x-axis.]\n\n---\n(Page 33)\n\n## Lecture 9\n_12.2.25_\n\n> (brief recap SN problem setup on board - useful for sheet 1)\n> mainly\n\n### SELECTION EFFECTS cont.\n\nrecap:\n$M_s = N(M_o, \\sigma^2)$\n$s=1,...,N$ (known)\n$\\mu_s = \\mu$ (known)\n> known distance\n> no measurement error\n\n[Diagram showing a Gaussian distribution P(M) centered at M. A vertical line at M_lim cuts off the right tail of the distribution. The observed samples are marked with 'x's under the curve to the left of M_lim.]\n\nFormulate likelihood to account for selection effects:\nLet $I_s = \\begin{cases} 1, & \\text{if SN observed} \\\\ 0, & \\text{if SN NOT observed} \\end{cases}$\n\nSelection function:\n$$ S(m_s) = P(I_s=1|m_s) = \\begin{cases} 1, & m_s < M_{lim} \\\\ 0, & m_s \\ge M_{lim} \\end{cases} $$\n\n[Graph of the selection function S(m_s) vs m. S(m_s) is 1 for m < M_lim and 0 for m >= M_lim. The x-axis is labeled m, y-axis S(m_s). A point is marked at M_lim on the x-axis.]\n> $1-H(m_s-M_{lim})$\n\n---\n(Page 34)\n\n### Observed Data Likelihood:\n$$ P(m_s|I_s=1, \\theta) = \\frac{P(I_s=1, m_s | \\theta)}{P(I_s=1|\\theta)} $$\n$$ = \\frac{P(I_s=1|m_s, \\theta)P(m_s|\\theta)}{\\int P(I_s=1|m_s, \\theta)P(m_s|\\theta)dm_s} $$\n$$ = \\frac{S(m_s)N(m_s|M_o+\\mu, \\sigma^2)}{\\int S(m_s)N(m_s|M_o+\\mu, \\sigma^2)dm_s} $$\n> cut off above M_lim\n$$ = \\frac{[1-H(m_s-M_{lim})]N(m_s|M_o+\\mu, \\sigma^2)}{\\int_{-\\infty}^{M_{lim}} N(m_s|M_o+\\mu, \\sigma^2)dm_s} $$\n> normalise\nGaussian CDF $\\rightarrow \\Phi$\n\n> \"look up on wikipedia\"\n(TRUNCATED) NORMAL\n$ = TN(m_s|M_o+\\mu, \\sigma^2, -\\infty, M_{lim}) $\n> $P(m_s|I_s=1, \\theta)$\nuntruncated mean & variance $\\uparrow$ lower truncation limit $\\uparrow$ upper truncation limit $\\uparrow$\n\n[Diagram of a truncated normal distribution. A Gaussian curve is shown, with the area to the right of M_lim cut off. The original mean M_o+μ is marked, which is different from the mean of the truncated distribution.]\n\nChallenge: what if $S(m_s) = P(I_s=1|m_s) = \\Phi\\left(\\frac{M_{lim}-m_s}{\\sigma_{lim}}\\right)$ ?\n> Not a sharp selection limit, but a soft boundary. e.g. detection of objects in sea of other 'clutter' objects.\n\n[Diagram showing a smooth, sigmoid-like selection function S(m_s) decreasing from 1 to 0 around M_lim. The curve is annotated with σ_lim, indicating the softness of the cutoff. The x-axis is m_s and y-axis is S(m_s).]\n> $\\sigma_{lim}$ is a parameter of the model\n\n---\n(Page 35)\n\n(slides)\n> Ex sheet 1 q 2: star formation in Perseus\n> 1) clustering dist of stars differ from field\n> 2) stars in dense cloud regions are hidden\n> 3) selection effect\n> 3) in densest regions, will be where largest stars form\n> -> pareto or power law distribution. $P(m) \\propto m^{-\\alpha}$ (for $m>m_o$)\n\n[Diagram showing a power-law distribution. A peaky 'naive likelihood' is drawn, contrasted with a broader 'true likelihood'. The x-axis is the power law exponent α.]\n> naive MLE -> 'biased', does not account for selection effects.\n\n### QUANTIFYING UNCERTAINTY USING BOOTSTRAP\n\nFrequentist interpretation:\nConsider variability of your estimator $g(\\vec{x})$ for $\\theta$ under (imaginary) repetitions of your experiment. (Random realisations of the potential data).\n\nHow does $g(\\vec{x})$ behave under the potential datasets you did not observe?\nunder $p(x|\\theta)$\ne.g. $Var[g(\\vec{x})] = E[g(\\vec{x}) - E[g(\\vec{x})]]$\n\nIf $g(\\vec{x})$ is approximately Gaussian distributed\n$\\implies 68\\%$ confidence interval $g(\\vec{z}) \\pm \\sqrt{Var(g(\\vec{z}))}$\n> standard confidence interval\n> $[g(\\vec{z})-\\sqrt{Var(g(\\vec{z}))}, g(\\vec{z})+\\sqrt{Var(g(\\vec{z}))}]$\n\n$(1-\\alpha)\\%$ confidence interval $[L(\\vec{x}), U(\\vec{x})]$ contains the true value $\\theta_{true}$ in at least $(1-\\alpha)\\%$ of the realisations.\n> assuming repeated experiments, resulting intervals that contain the parameter will approach 68%\n> NOT that -> parameter interval contains the true parameter (common misconception)\n\n---\n(Page 36)\n\n> get different interval for each experiment -> 68% of intervals contain θ. These realisations contain θ.\n\n$[L(\\vec{x}), U(\\vec{x})]$ is a random interval vs. $[L(\\vec{x}_{obs}), U(\\vec{x}_{obs})]$ evaluated on observed numerical values, only one dataset $\\vec{x}_{obs}$! (either contains $\\theta_{true}$ or doesn't)\n\n### Bootstrap:\n\nUse the observed dataset to simulate the variability of the unobserved (imaginary) data sets.\nBOOTSTRAP SAMPLE = sample with replacement from the observed dataset to the sample size\n\nEg. $X_1, ..., X_5 \\stackrel{iid}{\\sim} \\text{Poisson}(\\lambda)$\n$\\rightarrow P(X_i) = \\frac{\\lambda^{X_i} e^{-\\lambda}}{X_i!}$\n> could do w/ max. likelihood but suppose don't know dist.\n\nReal data (observed) $\\vec{X}_{obs}=(3,8,2,4,5)$.\nSuppose you want to estimate the skewness of $P(x)$ (asymmetry)\ni.e. skewness $= \\frac{E[(x-\\mu)^3]}{(\\sigma^2)^{3/2}} \\rightarrow \\substack{\\text{true mean} \\\\ \\mu=\\lambda \\\\ \\text{true variance} \\\\ \\sigma^2=\\lambda}$\n\nSample skewness $g(\\vec{x}) = \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i-\\bar{x})^3}{\\left(\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_i-\\bar{x})^2}\\right)^3}$\n\n---\n(Page 37)\n\nBootstrap B \"replicate\" datasets from observed dataset:\n> \"sample w/ replacement\" from original eg sample 10 times from 3,8,2,4,5 gives\n\n$\\vec{X}^{obs} = (3,8,2,4,5) \\quad \\hat{g}_{obs} = g(\\vec{x}^{obs}) = 0.6927$\n$\\vec{X}^{b=1} = (2,5,4,4,4) \\quad \\hat{g}_1 = g(\\vec{x}^{b=1}) = -0.8625$\n$\\vec{X}^{b=2} = (2,4,2,8,8) \\quad \\hat{g}_2 = g(\\vec{x}^{b=2}) = 0.2115$\n$\\vec{X}^{b=3} = (5,2,8,2,5) \\quad \\hat{g}_3 = g(\\vec{x}^{b=3}) = 0.3436$\n...\n$\\vec{X}^{b=B} = \\quad\\quad\\quad\\quad\\quad\\quad \\hat{g}_B =$\n\nCan now compute sample variance\n$\\hat{Var}(\\{\\hat{g}_1, ..., \\hat{g}_B\\}) \\rightarrow \\frac{1}{N-1}\\sum_i (x_i-\\bar{x})^2 (?)$\nStandard error = $\\sqrt{\\hat{Var}}$\n\n$\\hat{g} = 0.6927 \\pm 0.635 \\approx 68\\% \\text{ C.I.}$\n\n> (slides)\n> back to star formation paper example:\n> how do I get standard error on exponent?\n> draw bootstrap replicates and compute this against bootstrap\n> get model to each bootstrapped realisation\n> [Image of a histogram labeled 'true'] [Image of a histogram labeled 'naive'] plot max likelihood estimates for each as histograms -> naive (little bit of a different peak)\n\n---\n(Page 38)\n\n## Lecture 10\n_14.2.25_\n\n### REGRESSION (slides)\n\n* Fit a function $E[y|x] = f(x;\\theta)$ for the mean relation between $y$ and $x$.\n\n* Basic approaches\n    $\\rightarrow$ ordinary least squares (homoskedastic scatter)\n    $\\rightarrow$ generalized least squares (heteroskedastic, correlated scatter)\n    $\\rightarrow$ weighted least squares (minimum $\\chi^2$, known variance)\n    $\\rightarrow$ maximum likelihood\n\n* Real data problems require more complex modelling\n    $\\rightarrow$ regression dilution from covariate measurement errors\n\n[Diagram showing scattered data points with error bars, with a regression line fit through them. The x-axis is luminosity. The y-axis is not labeled.]\n> eg. each point has diff measurement error. regression dilution; namely apply OLS -> get biased Pluto-slope? (measurement error in x) (sheet 2)\n\n### Ordinary least squares (OLS):\n\nLinear model $y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ij} + \\epsilon_i$\n$\\downarrow$\n$Y = X\\beta + \\epsilon$\n$i=1,...,N$ objects, $E[\\epsilon_i]=0$, homoskedastic\n$Var[\\epsilon_i]=\\sigma^2$ (known).\n\n---\n(Page 39)\n\n$y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ij} + \\epsilon_i$\n> don't think this is too important to memorise? (slides)\n> error bars in y only, i.e. error in x is negligible\n> minimise sum squared of these distances\n\n> individual sum of squares\nMinimise w.r.t. $\\beta$ (solve for gradient=0):\n$$ RSS = \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^{k-1} \\beta_j x_{ij})^2 = (Y-X\\beta)^T(Y-X\\beta) $$\n> (*simple linear regression via wiki) eg. for $k=2$\n\n$\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T Y$\n> got except for?\n$Var(\\hat{\\beta}_{OLS}) = \\sigma^2(X^T X)^{-1}$\n> unbiased & has minimum variance under these assumptions\n$E(\\hat{\\beta}_{OLS}) = \\beta$ (unbiased) $\\rightarrow$ BLUE\n> best linear unbiased estimate\n\nEstimate unknown variance $\\sigma^2$\n> OLS estimate of variance $\\sigma^2$ is $\\hat{\\sigma}^2 = \\frac{RSS}{n-k}$\n> (for 2 parameter example)\n$\\hat{\\sigma}^2 = \\frac{1}{N-k}(Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta})$\n> k=2 eg. simple linear eg. 2 parameter (intercept, slope)\n\n(WEIGHTED LEAST SQUARES) - aka $\\chi^2$ minimisation:\n$X^2 = \\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\sum_j \\beta_j x_{ij})^2}{\\sigma_i^2 \\quad \\leftarrow \\text{known!}}$\n> $\\chi^2$ r.v. = sum of squared Gaussian r.v.\n> $\\downarrow$\n> $X^2 \\sim \\chi^2_{N-k}$ (if constraints)\n\nMinimise w.r.t. $\\beta$:\nIf Gaussian errors, at $\\beta=\\beta_{min}$, $X^2 \\sim \\chi^2_{N-k}$.\nModel check: $E(\\chi^2_{N-k}) = N-k \\rightarrow (\\text{if assumption is true (contrast)})$\n$\\frac{X^2}{N-k} \\approx 1$ (for large $N-k$)\n\n> A sketch: should have a $\\chi^2$ distribution with N-k degrees of freedom (if gaussian errors?)\n> reduced $\\chi^2$ statistic: a way to test model fit.\n\n---\n(Page 40)\n\nThese are special cases of generalized least squares.\nLinear model $y_i = \\beta_0 + \\sum_{j=1}^{k-1} \\beta_j x_{ij} + \\epsilon_i \\quad i=1,..,N$ objects\n$\\downarrow$\n$Y = X\\beta + \\epsilon$\n$E[\\epsilon_i]=0$, correlated errors\n$Var[\\epsilon] = Cov[\\epsilon, \\epsilon^T] = W$ (known)\n\nMinimize w.r.t. $\\beta$\n$\\hat{RSS} = (Y-X\\beta)^T W^{-1} (Y-X\\beta)$\n$\\hat{\\beta}_{GLS} = (X^T W^{-1} X)^{-1} X^T W^{-1} Y$\n$E[\\hat{\\beta}_{GLS}] = \\beta$ (unbiased)\n$Var[\\hat{\\beta}_{GLS}] = (X^T W X)^{-1}$\n\nThey can also be thought of as maximum likelihood (assuming Gaussian errors)\n$Y=X\\beta + \\epsilon$, $Y \\sim N(X\\beta, W)$\nMaximize w.r.t. $\\beta$\n$L(\\beta) = P(Y|\\beta, X) = N(Y|X\\beta, W)$\n> this week's \"linear regression\" talk on moodle covers this in more detail\n> (read slides)\n\n---\n(Page 41)\n\n> minimum $\\chi^2$ estimation gives values of $\\theta$ that make $\\chi^2(\\theta)$ as small as possible\n### $\\chi^2$ MINIMISATION (board)\n(vs max likelihood)\n> \"weighted least squares\" errors not iid (but still indep) -> weight according to true $\\sigma_i^2$ so that true dist doesn't get distorted by points w/ large $\\sigma_i$\n\n$(x_i, y_i)$ $i=1,...,N$\nCase: Variance unknown\n$y_i = f(x_ij, \\theta) + \\epsilon_i$, $Var(\\epsilon_i) = \\sigma_i^2$ known\n$= a+bx_i + \\epsilon_i$ (linear)\n\n$\\chi^2(\\theta) = \\sum_{i=1}^N \\frac{(y_i - f(x_i; \\theta))^2}{\\sigma_i^2}$\n$\\hat{\\theta} = \\underset{\\theta}{\\operatorname{argmin}} \\chi^2$\n\nRelation to maximum likelihood:\n$y_i = f(x_i;\\theta) + \\epsilon_i$, $\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma_i^2)$\n$P(y_i|x_i) = N(y_i|f(x_i;\\theta), \\sigma_i^2)$\n$L(\\theta) = \\prod_{i=1}^N P(y_i|x_i;\\theta) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{1}{2}(y_i - f(x_i;\\theta))^2/\\sigma_i^2}$\n$-2 \\ln L(\\theta) = \\sum_{i=1}^N \\ln(2\\pi\\sigma_i^2) + \\sum_{i=1}^N \\frac{(y_i-f(x_i;\\theta))^2}{\\sigma_i^2}$\n$= \\sum_{i=1}^N \\ln(2\\pi\\sigma_i^2) + \\chi^2(\\theta)$\n> $\\chi^2(\\theta)$ is constant\n> $(min \\chi^2(\\theta))$ is same as max likelihood\n\n$\\implies$ If $\\sigma_i$ known: $\\hat{\\theta}_{min \\chi^2} = \\hat{\\theta}_{MLE}$\n$\\implies$ If $\\sigma_i^2$ (or a component of it) unknown then NOT true!\n\n> (slides again, eg. ML is used in GLS).\n\n---\n(Page 42)\n\nCase: Variance component unknown: $\\leftarrow$ unknown\n$y_i = f(x_{ij}, \\theta) + \\epsilon_{int}^i + \\epsilon_m^i$ $\\quad \\epsilon_{int}^i \\sim N(0, \\sigma_{int}^2)$\n$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\epsilon_m^i \\sim N(0, \\sigma_{m,i}^2) \\leftarrow$ known\n\n$\\chi^2(\\theta, \\sigma_{int}^2) = \\sum_{i=1}^N \\frac{(y_i - f(x_{ij}, \\theta))^2}{\\sigma_{int}^2 + \\sigma_{m,i}^2}$\nProblem: $\\chi^2$ is minimised when $\\sigma_{int} \\rightarrow \\infty$ !!\n\nMax. likelihood:\n$P(y_i|x_i, \\theta, \\sigma_{int}^2) = N(y_i|f(x_{ij}\\theta), \\sigma_{int,i}^2 + \\sigma_{m,i}^2)$\n$L(\\theta, \\sigma_{int}^2) = \\prod_{i=1}^N [2\\pi(\\sigma_{int,i}^2+\\sigma_{m,i}^2)]^{-1/2} e^{-\\frac{1}{2}(y_i-f(x_{ij},\\theta))^2 / (\\sigma_{int}^2+\\sigma_{m,i}^2)}$\n$-2\\ln L(\\theta, \\sigma_{int}^2) = \\sum_{i=1}^N \\frac{(y_i - f(x_{ij}\\theta))^2}{\\sigma_{int}^2 + \\sigma_{m,i}^2} + \\ln[2\\pi(\\sigma_{int}^2+\\sigma_{m,i}^2)]$\n$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ (1) $\\quad\\quad\\quad\\quad$ (2) $\\uparrow$\nWhen $\\sigma_{int} \\rightarrow \\text{large}$, $\\downarrow$ but $\\uparrow$\n\nMax likelihood $\\rightarrow$ finite estimate of $\\sigma_{int}$!\n> (e.g. OLS assumes gaussian, independent errors with zero mean, constant variance, but derived from explicit modelling)\n\nLesson: $\\chi^2$ is an ad-hoc prescription. Likelihood is derived from explicit modelling assumptions.\n\n> slides: OLS leads to regression dilution when there are errors in x (really measurement errors) which causes slope to be shallower than true value aka regression dilution.\n> (OLS gives shallower slope if you have error on x data, regression dilution)\n\n---\n(Page 43)\n\n### Probabilistic Generative Modelling (slides)\n\n> ($P(D|\\alpha,\\theta)$)\n\n* Forward model comprises series of probabilistic steps describing conceptually how the observed data was generated from the parameters of interest.\n\n* Can introduce intermediate parameters / unobserved latent variables $\\alpha$ (e.g. true values corresponding to the observed data)\n> ... what data would be had if there had been no measurement error.\n\n* From forward model, derive the sampling distribution\ne.g. $P(D|\\theta) = \\int P(D|\\alpha)P(\\alpha|\\theta) d\\alpha$\n> integrate out to have returns of interest\n\n* Using observed data D, draw inference from likelihood function\n$L(\\theta) = P(D|\\theta)$\n\n* Or if Bayesian with prior $P(\\theta)$: sample posterior\n$P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$\n\n> params: $\\theta$ want, $\\alpha$ nuisance\n> $\\int P(D|\\theta) = \\int [ \\int P(D, \\alpha|\\theta)d\\alpha ] = \\int P(D|\\alpha, \\theta) P(\\alpha|\\theta) d\\alpha$\n> two ways:\n> $p(\\theta|D) = \\int p(\\theta, \\alpha|D) d\\alpha \\quad \\checkmark$\n> show as before from joint $P(D|\\theta) = \\int P(D|\\alpha,\\theta)P(\\alpha|\\theta)d\\alpha$\n\n---\n(Page 44)\n\n> (how do I come up with model for this complex situation where I have measurement error in my x and y data?)\n\n### Generative Model (slides) (same example from L2)\n\n- Population distribution $\\quad \\xi \\sim N(\\mu, \\tau^2)$\n- Regression Model $\\quad \\eta_i|\\xi_i \\sim N(\\alpha+\\beta\\xi_i, \\sigma^2)$\n- Measurement Error $\\quad [x_i, y_i]|\\xi_i, \\eta_i \\sim N([\\xi_i, \\eta_i], \\Sigma)$\n\nfor this example let $\\Sigma = \\begin{pmatrix} \\sigma_{x_i}^2 & 0 \\\\ 0 & \\sigma_{y_i}^2 \\end{pmatrix}$ (for simplicity)\n$x_i = \\xi_i + \\epsilon_{x,i} \\leftarrow \\sigma_{x,i}^2$\n$y_i = \\eta_i + \\epsilon_{y,i} \\leftarrow \\sigma_{y,i}^2$\n> probabilistic generative model for linear regression\n\n- Population dist. indep. variable $\\quad \\psi=(\\mu,\\tau)$\n- Regression Parameters $\\quad \\theta=(\\alpha,\\beta,\\sigma^2)$\n- Latent (true) variables (no meas. error) $\\quad (\\xi_i, \\eta_i)$\n- Observed data with measurement uncertainties $(\\sigma_{xi}, \\sigma_{yi})$ $\\quad (x_i, y_i) \\rightarrow \\Sigma = \\begin{pmatrix} \\sigma_{xi}^2 & 0 \\\\ 0 & \\sigma_{yi}^2 \\end{pmatrix}$\n\n> Now can\nGenerate observed data from latent variables. (see slides -> same as L3)\n> (now I've introduced all these new parameters how do I get rid of them?)\n### Formulating likelihood Function: Marginalising latent variables\n> (derive this sheet 2) (and on slides) \"marginalisation\"\n\n$P(x_i, y_i|\\theta, \\psi) = \\iint P(x_i, y_i, \\xi_i, \\eta_i | \\theta, \\psi) d\\xi_i d\\eta_i$\n> \"observed data likelihood\" $\\quad$ \"complete data likelihood\"\n$P(x_i, y_i|\\theta, \\psi) = \\iint \\underbrace{P(x_i,y_i|\\xi_i,\\eta_i)}_{\\text{measurement error}} \\underbrace{P(\\eta_i|\\xi_i,\\theta)}_{\\text{regression}} \\underbrace{P(\\xi_i|\\psi)}_{\\substack{\\text{population} \\\\ \\text{distribution of covariate}}} d\\xi_i d\\eta_i$\n\n---\n(Page 45)\n\n## Lecture 11\n_17.2.25_\n\n(slides)\n> obtain from previous, multiply marginal likelihoods to get likelihood for all data\nObserved data likelihood:\n$$ P(x,y|\\theta,\\psi) = \\prod_{i=1}^N P(x_i, y_i|\\theta,\\psi) \\quad \\text{(indep.)} $$\n\nIn frequentist statistics: distinction between data and parameters: parameters are fixed and unknown, but not \"random\". Only \"data\" are random realisations of random variables.\n\nWhat is the nature of the latent variables $\\xi_i, \\eta_i$?\n- They have distribution $(\\xi_i, \\eta_i) \\sim P(\\xi_i, \\eta_i|\\theta, \\psi) = P(\\eta_i|\\xi_i, \\theta) P(\\xi_i|\\psi)$\n- Often called \"nuisance parameters\" $\\rightarrow$ needed to complete the model but not the parameters of interest ($\\theta, \\psi$).\n- Are the latent variables \"data\" or \"parameters\"?\n> \"missing data\" $\\quad$ \"nuisance parameters\"\n\n### BAYESIAN VIEWPOINT\n\n* There is a symmetry between data D and parameters $\\theta$ - both are r.v.s described by probability distributions\n\n* Actually they are described by a joint probability\n$P(D, \\theta)$\n\n---\n(Page 46)\n\n(slides)\n* Data are r.v.s whose realisations are observed, parameters are r.v.s not observed\n\n* Goal is to infer the unobserved parameters from the observed data using the rules of probability\n$$ P(\\theta|D) = \\frac{P(D, \\theta)}{P(D)} \\quad \\text{CONDITIONAL PROBABILITY} $$\n$$ P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)} \\quad \\text{BAYES' THEOREM} $$\n\n* Probability interpreted as degree of belief / uncertainty in hypothesis\n> how sure are you in the values of that parameter?\n\n### Bayes' Theorem\n* Joint probability of data & parameters: $P(D, \\theta) = P(D|\\theta)P(\\theta) = P(\\theta|D)P(D)$\n* Probability of parameters given data:\n$$ \\underbrace{P(\\theta|D)}_{\\substack{\\text{posterior prob.}: \\\\ \\text{degree of belief}}} = \\frac{\\overbrace{P(D|\\theta)}^{\\substack{\\text{likelihood} \\\\ \\text{sampling distribution}}} \\overbrace{P(\\theta)}^{\\substack{\\text{prior prob.}: \\\\ \\text{degree of belief}}}}{\\underbrace{P(D)}_{\\substack{\\text{normalisation} \\\\ \\text{constant}}}} $$\n\n> example: bayesian inference of the dimensionality of space time. (see lecture)\n> observe light & detect w/ interferometer, signal from black hole merger (?)\n> idea: OK, use your tools to infer dimensionality of spacetime\n> dimensionality of spacetime is not a parameter that varies or changes. either one thing or another, but we don't know it. what is our degree of belief in the hypothesis.\n\n---\n(Page 47)\n\n### Simple Gaussian Example (slides)\nFrequentist Confidence vs. Bayesian Credible intervals\n\n1) Frequentist Confidence Interval\n$Y_1, ..., Y_N \\stackrel{iid}{\\sim} N(\\mu, 1)$ (sample mean)\nSampling dist. of statistic $\\bar{Y} \\sim N(\\mu, \\sigma^2/N)$\n$[\\bar{Y} - \\sigma_{\\bar{Y}}, \\bar{Y}+\\sigma_{\\bar{Y}}]$ is a 68% confidence interval\n* Under repeated experiments, 68% of the (random) confidence intervals constructed this way will contain (cover) $\\mu$\nThis does not mean that the probability is 68% that $\\mu$ lies within the interval ($y_{obs}$)\n> frequentist view: $\\mu$ is fixed number, it does not have prob dist. interval will either cover that number or it doesn't\n\n2) Bayesian credible Interval (board)\nassume flat prior: $P(\\mu) \\propto 1$\n> every value of $\\mu$ is equally likely\n> (not normalizable if interval is $-\\infty$ to $\\infty$, so say $\\propto 1$ (?)) \"improper prior\"\ncan derive posterior $p(\\mu|Y=y_{obs})$:\n$Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)$\n> unknown $\\uparrow$ $\\uparrow$ known\nLikelihood: $P(\\vec{y}|\\mu,\\sigma^2) = \\prod_{i=1}^N N(Y_i;\\mu,\\sigma^2)$\n$= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}(y_i-\\mu)^2/\\sigma^2}$\n$= (2\\pi\\sigma^2)^{-N/2} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i-\\mu)^2}$\n\n---\n(Page 48)\n\nLikelihood $P(\\vec{y}|\\mu, \\sigma^2)$ only depends on data $\\vec{y}$ through the sufficient statistics $\\bar{y}$ and $S^2$. Any two datasets $\\vec{y}$ and $\\vec{y}'$ with same sufficient statistics will give the same inference model.\nLikelihood principle: all info about parameters from data is located in likelihood function.\n$ \\sum (y_i-\\mu)^2 = \\sum (y_i - \\bar{y} + \\bar{y} - \\mu)^2 = \\sum [(y_i-\\bar{y})^2 + 2(y_i-\\bar{y})(\\bar{y}-\\mu) + (\\bar{y}-\\mu)^2] = \\sum(y_i-\\bar{y})^2 + N(\\bar{y}-\\mu)^2 $\n$P(\\vec{y}|\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} e^{\\frac{-(N-1)S^2}{2\\sigma^2}} e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2}$\ndef: where $\\left\\{ \\begin{array}{l} \\bar{y} = \\frac{1}{N}\\sum Y_i \\rightarrow \\text{simple mean} \\\\ S^2 = \\frac{1}{N-1}\\sum(y_i-\\bar{y})^2 \\end{array} \\right.$ sufficient statistics\n> \"sample variance\" (unbiased) estimator for $\\sigma^2$ is $S^2$.\n\nCase 1: $\\sigma^2=1$ is known, $P(\\mu)\\propto 1$\n> improper prior\nBayes' theorem $P(\\mu|\\vec{y}) \\propto P(\\vec{y}|\\mu) P(\\mu)$\n(ignore constant terms)\n$P(\\mu|\\vec{y}) \\propto e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2}$ (unnormalised posterior)\n> notice looks like part of a gaussian, only part gives variance is so this must define probability dist over $\\mu$.\n> integrate over $\\mu$\ncould:\n$P(\\mu|\\vec{y}) = A e^{-\\frac{N}{2\\sigma^2}(\\bar{y}-\\mu)^2} \\rightarrow \\int d\\mu \\rightarrow \\text{Find } A$\n> require $\\int P d\\mu = 1$\nor:\n$P(\\mu|\\vec{y}) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2/N)}} e^{-\\frac{1}{2\\sigma^2/N}(\\mu-\\bar{y})^2} = N(\\mu|\\bar{y}, \\sigma^2/N)$\n> normalised posterior\n> gaussian on $\\mu$ tells us what degree of belief we have in various values of $\\mu$\n> 68% prob $\\mu$ lies between $\\bar{y}\\pm\\sigma/\\sqrt{N}$\n\n[Diagram of a Gaussian posterior distribution for μ, centered at $\\bar{y}$. The area corresponding to 68% posterior probability is shaded, spanning $\\bar{y} \\pm \\sigma/\\sqrt{N}$.]\n> integrates within these limits gives 0.68\n> \"68% credible interval\"\n\n> see slide: frequentist vs. bayes for more detailed comparison\n> written overlays?\n\n---\n(Page 49)\n\nReading: Sivia Ch 1-3 Gelman \"Bayesian data analysis\"\nF & B 3.8\nIvezic 5\n\n## Lecture 12\n_19.2.25_\n\n(slides)\n\n### Frequentist vs. Bayes\n\n* Frequentists make statements about the data (or statistics or estimators = functions of the data) conditional on the parameter\n$P(D|\\theta)$ or $P(f(D)|\\theta)$\n\n* Often goal is to get a \"point estimate\" or confidence intervals with good properties under repeated experiments $\\rightarrow$ (\"long run\")\n\n* Arguments are based on datasets that could've happened but didn't e.g. null hypothesis testing\n\n* Bayesians make statements about the probability of parameters conditional on the dataset $D=D_{obs}$ that you actually observed:\n$P(\\theta|D=D_{obs})$\nThis requires an interpretation of the probability as quantifying a \"degree of belief\" in a hypothesis\n\n* Bayesian answer is the full posterior density $P(\\theta|D=D_{obs})$ quantifying the \"state of knowledge\" after seeing the data. Any numerical estimates are attempts to (imperfectly) summarise the posterior.\n\n---\n(Page 50)",
      "timestamp": "2025-10-31T16:55:51.170331",
      "tokens": 8861,
      "cost": 0.071258125
    }
  ],
  "total_tokens": 51986,
  "total_cost": 0.14251625
}